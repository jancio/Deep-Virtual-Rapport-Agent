BN0
	=> better than without BN, (lower val losses)
	slightly better than BN2


WARNING:tensorflow:From /home/ICT2000/jondras/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /home/ICT2000/jondras/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
masking_1 (Masking)          (None, 32, 12)            0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 32, 12)            48        
_________________________________________________________________
gru_1 (GRU)                  (None, 32, 32)            4320      
_________________________________________________________________
gru_2 (GRU)                  (None, 32, 32)            6240      
_________________________________________________________________
time_distributed_1 (TimeDist (None, 32, 1)             33        
=================================================================
Total params: 10,641
Trainable params: 10,617
Non-trainable params: 24
_________________________________________________________________
None
INFO:
	0fold_BN_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
WARNING:tensorflow:From /home/ICT2000/jondras/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Train on 159911 samples, validate on 28192 samples
Epoch 1/100
159911/159911 [==============================] - 124s 777us/step - loss: 0.2380 - val_loss: 0.2130
[last] - val_bacc: 0.7676 - val_f1: 0.6560 - val_precision: 0.8054 - val_recall: 0.5533 
[majority] - val_bacc: 0.7585 - val_f1: 0.6476 - val_precision: 0.8276 - val_recall: 0.5319 

Epoch 00001: val_loss improved from inf to 0.21301, saving model to ./checkpoints/0fold_BN_32-32u_32ws_12f/m_0001_0.2380_0.2130.hdf5
Epoch 2/100
159911/159911 [==============================] - 124s 775us/step - loss: 0.1924 - val_loss: 0.2090
[last] - val_bacc: 0.7899 - val_f1: 0.6840 - val_precision: 0.7939 - val_recall: 0.6009 
[majority] - val_bacc: 0.7809 - val_f1: 0.6770 - val_precision: 0.8133 - val_recall: 0.5798 

Epoch 00002: val_loss improved from 0.21301 to 0.20902, saving model to ./checkpoints/0fold_BN_32-32u_32ws_12f/m_0002_0.1924_0.2090.hdf5
Epoch 3/100
159911/159911 [==============================] - 126s 786us/step - loss: 0.1815 - val_loss: 0.2024
[last] - val_bacc: 0.8237 - val_f1: 0.7019 - val_precision: 0.7216 - val_recall: 0.6832 
[majority] - val_bacc: 0.8123 - val_f1: 0.7019 - val_precision: 0.7593 - val_recall: 0.6526 

Epoch 00003: val_loss improved from 0.20902 to 0.20236, saving model to ./checkpoints/0fold_BN_32-32u_32ws_12f/m_0003_0.1815_0.2024.hdf5
Epoch 4/100
159911/159911 [==============================] - 125s 783us/step - loss: 0.1736 - val_loss: 0.2172
[last] - val_bacc: 0.8124 - val_f1: 0.6756 - val_precision: 0.6852 - val_recall: 0.6663 
[majority] - val_bacc: 0.8065 - val_f1: 0.6844 - val_precision: 0.7280 - val_recall: 0.6458 

Epoch 00004: val_loss did not improve from 0.20236
Epoch 5/100
159911/159911 [==============================] - 123s 770us/step - loss: 0.1677 - val_loss: 0.2070
[last] - val_bacc: 0.8333 - val_f1: 0.7006 - val_precision: 0.6921 - val_recall: 0.7094 
[majority] - val_bacc: 0.8223 - val_f1: 0.7073 - val_precision: 0.7407 - val_recall: 0.6767 

Epoch 00005: val_loss did not improve from 0.20236
Epoch 6/100
159911/159911 [==============================] - 122s 764us/step - loss: 0.1619 - val_loss: 0.2118
[last] - val_bacc: 0.8066 - val_f1: 0.6837 - val_precision: 0.7257 - val_recall: 0.6464 
[majority] - val_bacc: 0.7997 - val_f1: 0.6881 - val_precision: 0.7646 - val_recall: 0.6256 

Epoch 00006: val_loss did not improve from 0.20236
Epoch 7/100
159911/159911 [==============================] - 124s 773us/step - loss: 0.1573 - val_loss: 0.2077
[last] - val_bacc: 0.8158 - val_f1: 0.6933 - val_precision: 0.7225 - val_recall: 0.6663 
[majority] - val_bacc: 0.8070 - val_f1: 0.6971 - val_precision: 0.7643 - val_recall: 0.6407 

Epoch 00007: val_loss did not improve from 0.20236
Epoch 8/100
159911/159911 [==============================] - 124s 774us/step - loss: 0.1525 - val_loss: 0.2175
[last] - val_bacc: 0.7688 - val_f1: 0.6534 - val_precision: 0.7887 - val_recall: 0.5578 
[majority] - val_bacc: 0.7657 - val_f1: 0.6567 - val_precision: 0.8198 - val_recall: 0.5477 

Epoch 00008: val_loss did not improve from 0.20236
Epoch 9/100
159911/159911 [==============================] - 124s 773us/step - loss: 0.1483 - val_loss: 0.2144
[last] - val_bacc: 0.8174 - val_f1: 0.6907 - val_precision: 0.7106 - val_recall: 0.6719 
[majority] - val_bacc: 0.8096 - val_f1: 0.7004 - val_precision: 0.7647 - val_recall: 0.6461 

Epoch 00009: val_loss did not improve from 0.20236
Epoch 10/100
159911/159911 [==============================] - 125s 780us/step - loss: 0.1451 - val_loss: 0.2279
[last] - val_bacc: 0.7849 - val_f1: 0.6597 - val_precision: 0.7335 - val_recall: 0.5994 
[majority] - val_bacc: 0.7836 - val_f1: 0.6709 - val_precision: 0.7773 - val_recall: 0.5902 

Epoch 00010: val_loss did not improve from 0.20236
Epoch 11/100
159911/159911 [==============================] - 124s 776us/step - loss: 0.1422 - val_loss: 0.2279
[last] - val_bacc: 0.8179 - val_f1: 0.6881 - val_precision: 0.7021 - val_recall: 0.6746 
[majority] - val_bacc: 0.8130 - val_f1: 0.6979 - val_precision: 0.7450 - val_recall: 0.6565 

Epoch 00011: val_loss did not improve from 0.20236
Epoch 12/100
159911/159911 [==============================] - 123s 771us/step - loss: 0.1397 - val_loss: 0.2307
[last] - val_bacc: 0.7937 - val_f1: 0.6667 - val_precision: 0.7211 - val_recall: 0.6199 
[majority] - val_bacc: 0.7933 - val_f1: 0.6802 - val_precision: 0.7657 - val_recall: 0.6119 

Epoch 00012: val_loss did not improve from 0.20236
Epoch 13/100
159911/159911 [==============================] - 122s 762us/step - loss: 0.1377 - val_loss: 0.2343
[last] - val_bacc: 0.7825 - val_f1: 0.6527 - val_precision: 0.7206 - val_recall: 0.5964 
[majority] - val_bacc: 0.7829 - val_f1: 0.6661 - val_precision: 0.7639 - val_recall: 0.5905 

Epoch 00013: val_loss did not improve from 0.20236
Epoch 00013: early stopping
	Min train loss: 0.13768354409495162 @epoch 12
	Min valid loss: 0.20235553577597276 @epoch 2

<Figure size 1000x600 with 1 Axes>

INFO:
	1fold_BN_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 160597 samples, validate on 28313 samples
Epoch 1/100
160597/160597 [==============================] - 124s 772us/step - loss: 0.2365 - val_loss: 0.1944
[last] - val_bacc: 0.7537 - val_f1: 0.6122 - val_precision: 0.7209 - val_recall: 0.5319 
[majority] - val_bacc: 0.7554 - val_f1: 0.6275 - val_precision: 0.7698 - val_recall: 0.5296 

Epoch 00001: val_loss improved from inf to 0.19436, saving model to ./checkpoints/1fold_BN_32-32u_32ws_12f/m_0001_0.2365_0.1944.hdf5
Epoch 2/100
160597/160597 [==============================] - 123s 763us/step - loss: 0.1900 - val_loss: 0.1864
[last] - val_bacc: 0.8002 - val_f1: 0.6536 - val_precision: 0.6703 - val_recall: 0.6377 
[majority] - val_bacc: 0.7905 - val_f1: 0.6606 - val_precision: 0.7222 - val_recall: 0.6088 

Epoch 00002: val_loss improved from 0.19436 to 0.18636, saving model to ./checkpoints/1fold_BN_32-32u_32ws_12f/m_0002_0.1900_0.1864.hdf5
Epoch 3/100
160597/160597 [==============================] - 124s 772us/step - loss: 0.1783 - val_loss: 0.1869
[last] - val_bacc: 0.8045 - val_f1: 0.6570 - val_precision: 0.6669 - val_recall: 0.6474 
[majority] - val_bacc: 0.7972 - val_f1: 0.6660 - val_precision: 0.7139 - val_recall: 0.6241 

Epoch 00003: val_loss did not improve from 0.18636
Epoch 4/100
160597/160597 [==============================] - 125s 780us/step - loss: 0.1702 - val_loss: 0.1870
[last] - val_bacc: 0.7825 - val_f1: 0.6452 - val_precision: 0.7055 - val_recall: 0.5945 
[majority] - val_bacc: 0.7734 - val_f1: 0.6489 - val_precision: 0.7559 - val_recall: 0.5685 

Epoch 00004: val_loss did not improve from 0.18636
Epoch 5/100
160597/160597 [==============================] - 123s 765us/step - loss: 0.1639 - val_loss: 0.1906
[last] - val_bacc: 0.7593 - val_f1: 0.6242 - val_precision: 0.7367 - val_recall: 0.5416 
[majority] - val_bacc: 0.7557 - val_f1: 0.6305 - val_precision: 0.7795 - val_recall: 0.5293 

Epoch 00005: val_loss did not improve from 0.18636
Epoch 6/100
160597/160597 [==============================] - 125s 777us/step - loss: 0.1579 - val_loss: 0.1893
[last] - val_bacc: 0.8006 - val_f1: 0.6533 - val_precision: 0.6685 - val_recall: 0.6387 
[majority] - val_bacc: 0.7942 - val_f1: 0.6578 - val_precision: 0.7009 - val_recall: 0.6198 

Epoch 00006: val_loss did not improve from 0.18636
Epoch 7/100
160597/160597 [==============================] - 126s 782us/step - loss: 0.1540 - val_loss: 0.1951
[last] - val_bacc: 0.8013 - val_f1: 0.6509 - val_precision: 0.6604 - val_recall: 0.6417 
[majority] - val_bacc: 0.7946 - val_f1: 0.6569 - val_precision: 0.6968 - val_recall: 0.6214 

Epoch 00007: val_loss did not improve from 0.18636
Epoch 8/100
160597/160597 [==============================] - 127s 788us/step - loss: 0.1504 - val_loss: 0.1901
[last] - val_bacc: 0.8061 - val_f1: 0.6641 - val_precision: 0.6805 - val_recall: 0.6484 
[majority] - val_bacc: 0.8031 - val_f1: 0.6762 - val_precision: 0.7231 - val_recall: 0.6351 

Epoch 00008: val_loss did not improve from 0.18636
Epoch 9/100
160597/160597 [==============================] - 125s 778us/step - loss: 0.1470 - val_loss: 0.1997
[last] - val_bacc: 0.7733 - val_f1: 0.6179 - val_precision: 0.6578 - val_recall: 0.5825 
[majority] - val_bacc: 0.7787 - val_f1: 0.6435 - val_precision: 0.7148 - val_recall: 0.5852 

Epoch 00009: val_loss did not improve from 0.18636
Epoch 10/100
160597/160597 [==============================] - 124s 774us/step - loss: 0.1438 - val_loss: 0.1944
[last] - val_bacc: 0.7987 - val_f1: 0.6458 - val_precision: 0.6544 - val_recall: 0.6374 
[majority] - val_bacc: 0.8036 - val_f1: 0.6670 - val_precision: 0.6960 - val_recall: 0.6404 

Epoch 00010: val_loss did not improve from 0.18636
Epoch 11/100
160597/160597 [==============================] - 124s 775us/step - loss: 0.1409 - val_loss: 0.2013
[last] - val_bacc: 0.7720 - val_f1: 0.6190 - val_precision: 0.6655 - val_recall: 0.5785 
[majority] - val_bacc: 0.7736 - val_f1: 0.6403 - val_precision: 0.7257 - val_recall: 0.5729 

Epoch 00011: val_loss did not improve from 0.18636
Epoch 12/100
160597/160597 [==============================] - 126s 782us/step - loss: 0.1386 - val_loss: 0.1975
[last] - val_bacc: 0.7752 - val_f1: 0.6265 - val_precision: 0.6764 - val_recall: 0.5835 
[majority] - val_bacc: 0.7710 - val_f1: 0.6351 - val_precision: 0.7198 - val_recall: 0.5682 

Epoch 00012: val_loss did not improve from 0.18636
Epoch 00012: early stopping
	Min train loss: 0.13856563269990874 @epoch 11
	Min valid loss: 0.1863575075409852 @epoch 1

<Figure size 1000x600 with 1 Axes>

INFO:
	2fold_BN_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 159222 samples, validate on 28070 samples
Epoch 1/100
159222/159222 [==============================] - 124s 776us/step - loss: 0.2352 - val_loss: 0.2362
[last] - val_bacc: 0.7694 - val_f1: 0.6473 - val_precision: 0.7507 - val_recall: 0.5689 
[majority] - val_bacc: 0.7693 - val_f1: 0.6596 - val_precision: 0.8007 - val_recall: 0.5609 

Epoch 00001: val_loss improved from inf to 0.23616, saving model to ./checkpoints/2fold_BN_32-32u_32ws_12f/m_0001_0.2352_0.2362.hdf5
Epoch 2/100
159222/159222 [==============================] - 122s 767us/step - loss: 0.1894 - val_loss: 0.2343
[last] - val_bacc: 0.7899 - val_f1: 0.6522 - val_precision: 0.6797 - val_recall: 0.6268 
[majority] - val_bacc: 0.7931 - val_f1: 0.6710 - val_precision: 0.7262 - val_recall: 0.6237 

Epoch 00002: val_loss improved from 0.23616 to 0.23433, saving model to ./checkpoints/2fold_BN_32-32u_32ws_12f/m_0002_0.1894_0.2343.hdf5
Epoch 3/100
159222/159222 [==============================] - 122s 769us/step - loss: 0.1777 - val_loss: 0.2370
[last] - val_bacc: 0.7709 - val_f1: 0.6420 - val_precision: 0.7240 - val_recall: 0.5767 
[majority] - val_bacc: 0.7718 - val_f1: 0.6527 - val_precision: 0.7589 - val_recall: 0.5725 

Epoch 00003: val_loss did not improve from 0.23433
Epoch 4/100
159222/159222 [==============================] - 122s 769us/step - loss: 0.1694 - val_loss: 0.2484
[last] - val_bacc: 0.7718 - val_f1: 0.6344 - val_precision: 0.6932 - val_recall: 0.5847 
[majority] - val_bacc: 0.7779 - val_f1: 0.6548 - val_precision: 0.7366 - val_recall: 0.5894 

Epoch 00004: val_loss did not improve from 0.23433
Epoch 5/100
159222/159222 [==============================] - 123s 773us/step - loss: 0.1627 - val_loss: 0.2463
[last] - val_bacc: 0.7880 - val_f1: 0.6491 - val_precision: 0.6770 - val_recall: 0.6234 
[majority] - val_bacc: 0.7889 - val_f1: 0.6626 - val_precision: 0.7155 - val_recall: 0.6169 

Epoch 00005: val_loss did not improve from 0.23433
Epoch 6/100
159222/159222 [==============================] - 122s 767us/step - loss: 0.1572 - val_loss: 0.2529
[last] - val_bacc: 0.7673 - val_f1: 0.6257 - val_precision: 0.6830 - val_recall: 0.5772 
[majority] - val_bacc: 0.7694 - val_f1: 0.6415 - val_precision: 0.7289 - val_recall: 0.5728 

Epoch 00006: val_loss did not improve from 0.23433
Epoch 7/100
159222/159222 [==============================] - 125s 785us/step - loss: 0.1517 - val_loss: 0.2480
[last] - val_bacc: 0.7903 - val_f1: 0.6607 - val_precision: 0.7044 - val_recall: 0.6221 
[majority] - val_bacc: 0.7877 - val_f1: 0.6676 - val_precision: 0.7374 - val_recall: 0.6099 

Epoch 00007: val_loss did not improve from 0.23433
Epoch 8/100
 91264/159222 [================>.............] - ETA: 49s - loss: 0.1481