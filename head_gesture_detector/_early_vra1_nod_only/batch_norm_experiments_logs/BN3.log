BN3


WARNING:tensorflow:From /home/ICT2000/jondras/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /home/ICT2000/jondras/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
masking_1 (Masking)          (None, 32, 12)            0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 32, 12)            48        
_________________________________________________________________
gru_1 (GRU)                  (None, 32, 32)            4320      
_________________________________________________________________
gru_2 (GRU)                  (None, 32, 32)            6240      
_________________________________________________________________
time_distributed_1 (TimeDist (None, 32, 1)             33        
_________________________________________________________________
batch_normalization_2 (Batch (None, 32, 1)             4         
_________________________________________________________________
time_distributed_2 (TimeDist (None, 32, 1)             0         
=================================================================
Total params: 10,645
Trainable params: 10,619
Non-trainable params: 26
_________________________________________________________________
None
INFO:
	0fold_BN3_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
WARNING:tensorflow:From /home/ICT2000/jondras/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Train on 159911 samples, validate on 28192 samples
Epoch 1/100
159911/159911 [==============================] - 125s 779us/step - loss: 0.4223 - val_loss: 0.2867
[last] - val_bacc: 0.7998 - val_f1: 0.6669 - val_precision: 0.7002 - val_recall: 0.6366 
[majority] - val_bacc: 0.7908 - val_f1: 0.6687 - val_precision: 0.7385 - val_recall: 0.6110 

Epoch 00001: val_loss improved from inf to 0.28671, saving model to ./checkpoints/0fold_BN3_32-32u_32ws_12f/m_0001_0.4223_0.2867.hdf5
Epoch 2/100
159911/159911 [==============================] - 122s 763us/step - loss: 0.2527 - val_loss: 0.2348
[last] - val_bacc: 0.7794 - val_f1: 0.6642 - val_precision: 0.7736 - val_recall: 0.5819 
[majority] - val_bacc: 0.7672 - val_f1: 0.6518 - val_precision: 0.7909 - val_recall: 0.5542 

Epoch 00002: val_loss improved from 0.28671 to 0.23477, saving model to ./checkpoints/0fold_BN3_32-32u_32ws_12f/m_0002_0.2527_0.2348.hdf5
Epoch 3/100
159911/159911 [==============================] - 122s 766us/step - loss: 0.2056 - val_loss: 0.2399
[last] - val_bacc: 0.8227 - val_f1: 0.6843 - val_precision: 0.6792 - val_recall: 0.6895 
[majority] - val_bacc: 0.8113 - val_f1: 0.6958 - val_precision: 0.7447 - val_recall: 0.6529 

Epoch 00003: val_loss did not improve from 0.23477
Epoch 4/100
159911/159911 [==============================] - 123s 770us/step - loss: 0.1848 - val_loss: 0.2166
[last] - val_bacc: 0.7727 - val_f1: 0.6595 - val_precision: 0.7904 - val_recall: 0.5658 
[majority] - val_bacc: 0.7701 - val_f1: 0.6625 - val_precision: 0.8170 - val_recall: 0.5572 

Epoch 00004: val_loss improved from 0.23477 to 0.21658, saving model to ./checkpoints/0fold_BN3_32-32u_32ws_12f/m_0004_0.1848_0.2166.hdf5
Epoch 5/100
159911/159911 [==============================] - 123s 769us/step - loss: 0.1731 - val_loss: 0.2136
[last] - val_bacc: 0.8168 - val_f1: 0.6814 - val_precision: 0.6877 - val_recall: 0.6752 
[majority] - val_bacc: 0.8080 - val_f1: 0.6921 - val_precision: 0.7457 - val_recall: 0.6458 

Epoch 00005: val_loss improved from 0.21658 to 0.21363, saving model to ./checkpoints/0fold_BN3_32-32u_32ws_12f/m_0005_0.1731_0.2136.hdf5
Epoch 6/100
159911/159911 [==============================] - 123s 769us/step - loss: 0.1648 - val_loss: 0.2136
[last] - val_bacc: 0.8029 - val_f1: 0.6901 - val_precision: 0.7581 - val_recall: 0.6333 
[majority] - val_bacc: 0.7932 - val_f1: 0.6827 - val_precision: 0.7745 - val_recall: 0.6104 

Epoch 00006: val_loss improved from 0.21363 to 0.21356, saving model to ./checkpoints/0fold_BN3_32-32u_32ws_12f/m_0006_0.1648_0.2136.hdf5
Epoch 7/100
159911/159911 [==============================] - 124s 776us/step - loss: 0.1602 - val_loss: 0.2129
[last] - val_bacc: 0.8204 - val_f1: 0.6940 - val_precision: 0.7107 - val_recall: 0.6782 
[majority] - val_bacc: 0.8126 - val_f1: 0.6968 - val_precision: 0.7431 - val_recall: 0.6559 

Epoch 00007: val_loss improved from 0.21356 to 0.21290, saving model to ./checkpoints/0fold_BN3_32-32u_32ws_12f/m_0007_0.1602_0.2129.hdf5
Epoch 8/100
159911/159911 [==============================] - 124s 774us/step - loss: 0.1544 - val_loss: 0.2129
[last] - val_bacc: 0.8061 - val_f1: 0.6873 - val_precision: 0.7381 - val_recall: 0.6431 
[majority] - val_bacc: 0.7965 - val_f1: 0.6852 - val_precision: 0.7687 - val_recall: 0.6181 

Epoch 00008: val_loss did not improve from 0.21290
Epoch 9/100
159911/159911 [==============================] - 125s 781us/step - loss: 0.1506 - val_loss: 0.2164
[last] - val_bacc: 0.8107 - val_f1: 0.6930 - val_precision: 0.7386 - val_recall: 0.6526 
[majority] - val_bacc: 0.8009 - val_f1: 0.6926 - val_precision: 0.7744 - val_recall: 0.6264 

Epoch 00009: val_loss did not improve from 0.21290
Epoch 10/100
159911/159911 [==============================] - 123s 767us/step - loss: 0.1472 - val_loss: 0.2171
[last] - val_bacc: 0.8071 - val_f1: 0.6904 - val_precision: 0.7437 - val_recall: 0.6443 
[majority] - val_bacc: 0.8009 - val_f1: 0.6948 - val_precision: 0.7814 - val_recall: 0.6256 

Epoch 00010: val_loss did not improve from 0.21290
Epoch 11/100
159911/159911 [==============================] - 123s 768us/step - loss: 0.1447 - val_loss: 0.2251
[last] - val_bacc: 0.8416 - val_f1: 0.7028 - val_precision: 0.6771 - val_recall: 0.7305 
[majority] - val_bacc: 0.8352 - val_f1: 0.7177 - val_precision: 0.7301 - val_recall: 0.7058 

Epoch 00011: val_loss did not improve from 0.21290
Epoch 12/100
159911/159911 [==============================] - 123s 772us/step - loss: 0.1412 - val_loss: 0.2264
[last] - val_bacc: 0.7947 - val_f1: 0.6783 - val_precision: 0.7537 - val_recall: 0.6166 
[majority] - val_bacc: 0.7872 - val_f1: 0.6793 - val_precision: 0.7900 - val_recall: 0.5958 

Epoch 00012: val_loss did not improve from 0.21290
Epoch 13/100
159911/159911 [==============================] - 122s 760us/step - loss: 0.1402 - val_loss: 0.2238
[last] - val_bacc: 0.7932 - val_f1: 0.6810 - val_precision: 0.7687 - val_recall: 0.6113 
[majority] - val_bacc: 0.7857 - val_f1: 0.6803 - val_precision: 0.8008 - val_recall: 0.5914 

Epoch 00013: val_loss did not improve from 0.21290
Epoch 14/100
159911/159911 [==============================] - 122s 765us/step - loss: 0.1374 - val_loss: 0.2327
[last] - val_bacc: 0.7850 - val_f1: 0.6650 - val_precision: 0.7509 - val_recall: 0.5967 
[majority] - val_bacc: 0.7790 - val_f1: 0.6679 - val_precision: 0.7893 - val_recall: 0.5789 

Epoch 00014: val_loss did not improve from 0.21290
Epoch 15/100
159911/159911 [==============================] - 124s 775us/step - loss: 0.1349 - val_loss: 0.2319
[last] - val_bacc: 0.8263 - val_f1: 0.6862 - val_precision: 0.6745 - val_recall: 0.6984 
[majority] - val_bacc: 0.8222 - val_f1: 0.7013 - val_precision: 0.7248 - val_recall: 0.6793 

Epoch 00015: val_loss did not improve from 0.21290
Epoch 16/100
159911/159911 [==============================] - 124s 774us/step - loss: 0.1340 - val_loss: 0.2322
[last] - val_bacc: 0.8044 - val_f1: 0.6800 - val_precision: 0.7225 - val_recall: 0.6422 
[majority] - val_bacc: 0.8012 - val_f1: 0.6892 - val_precision: 0.7621 - val_recall: 0.6291 

Epoch 00016: val_loss did not improve from 0.21290
Epoch 17/100

159911/159911 [==============================] - 123s 767us/step - loss: 0.1323 - val_loss: 0.2349
[last] - val_bacc: 0.8152 - val_f1: 0.6845 - val_precision: 0.7005 - val_recall: 0.6692 
[majority] - val_bacc: 0.8079 - val_f1: 0.6925 - val_precision: 0.7469 - val_recall: 0.6455 

Epoch 00017: val_loss did not improve from 0.21290
Epoch 00017: early stopping
	Min train loss: 0.13230455052291962 @epoch 16
	Min valid loss: 0.2128950028778577 @epoch 6

<Figure size 1000x600 with 1 Axes>

INFO:
	1fold_BN3_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 160597 samples, validate on 28313 samples
Epoch 1/100
160597/160597 [==============================] - 126s 785us/step - loss: 0.4232 - val_loss: 0.2966
[last] - val_bacc: 0.7920 - val_f1: 0.5977 - val_precision: 0.5569 - val_recall: 0.6450 
[majority] - val_bacc: 0.7921 - val_f1: 0.6248 - val_precision: 0.6196 - val_recall: 0.6301 

Epoch 00001: val_loss improved from inf to 0.29656, saving model to ./checkpoints/1fold_BN3_32-32u_32ws_12f/m_0001_0.4232_0.2966.hdf5
Epoch 2/100
160597/160597 [==============================] - 124s 772us/step - loss: 0.2538 - val_loss: 0.2269
[last] - val_bacc: 0.7952 - val_f1: 0.6367 - val_precision: 0.6411 - val_recall: 0.6324 
[majority] - val_bacc: 0.7869 - val_f1: 0.6484 - val_precision: 0.6987 - val_recall: 0.6048 

Epoch 00002: val_loss improved from 0.29656 to 0.22686, saving model to ./checkpoints/1fold_BN3_32-32u_32ws_12f/m_0002_0.2538_0.2269.hdf5
Epoch 3/100
160597/160597 [==============================] - 125s 776us/step - loss: 0.2041 - val_loss: 0.2002
[last] - val_bacc: 0.7822 - val_f1: 0.6377 - val_precision: 0.6841 - val_recall: 0.5971 
[majority] - val_bacc: 0.7729 - val_f1: 0.6430 - val_precision: 0.7377 - val_recall: 0.5699 

Epoch 00003: val_loss improved from 0.22686 to 0.20022, saving model to ./checkpoints/1fold_BN3_32-32u_32ws_12f/m_0003_0.2041_0.2002.hdf5
Epoch 4/100
160597/160597 [==============================] - 123s 765us/step - loss: 0.1821 - val_loss: 0.1952
[last] - val_bacc: 0.7777 - val_f1: 0.6341 - val_precision: 0.6896 - val_recall: 0.5868 
[majority] - val_bacc: 0.7739 - val_f1: 0.6444 - val_precision: 0.7381 - val_recall: 0.5719 

Epoch 00004: val_loss improved from 0.20022 to 0.19521, saving model to ./checkpoints/1fold_BN3_32-32u_32ws_12f/m_0004_0.1821_0.1952.hdf5
Epoch 5/100
160597/160597 [==============================] - 125s 780us/step - loss: 0.1698 - val_loss: 0.1855
[last] - val_bacc: 0.7874 - val_f1: 0.6508 - val_precision: 0.7039 - val_recall: 0.6051 
[majority] - val_bacc: 0.7810 - val_f1: 0.6521 - val_precision: 0.7328 - val_recall: 0.5875 

Epoch 00005: val_loss improved from 0.19521 to 0.18549, saving model to ./checkpoints/1fold_BN3_32-32u_32ws_12f/m_0005_0.1698_0.1855.hdf5
Epoch 6/100
160597/160597 [==============================] - 123s 763us/step - loss: 0.1625 - val_loss: 0.1871
[last] - val_bacc: 0.7992 - val_f1: 0.6522 - val_precision: 0.6696 - val_recall: 0.6357 
[majority] - val_bacc: 0.7890 - val_f1: 0.6571 - val_precision: 0.7169 - val_recall: 0.6065 

Epoch 00006: val_loss did not improve from 0.18549
Epoch 7/100
160597/160597 [==============================] - 124s 770us/step - loss: 0.1571 - val_loss: 0.1907
[last] - val_bacc: 0.7808 - val_f1: 0.6321 - val_precision: 0.6727 - val_recall: 0.5961 
[majority] - val_bacc: 0.7742 - val_f1: 0.6371 - val_precision: 0.7129 - val_recall: 0.5758 

Epoch 00007: val_loss did not improve from 0.18549
Epoch 8/100
160597/160597 [==============================] - 125s 778us/step - loss: 0.1524 - val_loss: 0.1918
[last] - val_bacc: 0.8141 - val_f1: 0.6537 - val_precision: 0.6342 - val_recall: 0.6743 
[majority] - val_bacc: 0.8005 - val_f1: 0.6625 - val_precision: 0.6932 - val_recall: 0.6344 

Epoch 00008: val_loss did not improve from 0.18549
Epoch 9/100
160597/160597 [==============================] - 124s 773us/step - loss: 0.1483 - val_loss: 0.1920
[last] - val_bacc: 0.8089 - val_f1: 0.6554 - val_precision: 0.6512 - val_recall: 0.6597 
[majority] - val_bacc: 0.8006 - val_f1: 0.6664 - val_precision: 0.7039 - val_recall: 0.6327 

Epoch 00009: val_loss did not improve from 0.18549
Epoch 10/100
160597/160597 [==============================] - 124s 773us/step - loss: 0.1451 - val_loss: 0.1984
[last] - val_bacc: 0.8318 - val_f1: 0.6647 - val_precision: 0.6206 - val_recall: 0.7156 
[majority] - val_bacc: 0.8209 - val_f1: 0.6753 - val_precision: 0.6688 - val_recall: 0.6820 

Epoch 00010: val_loss did not improve from 0.18549
Epoch 11/100
160597/160597 [==============================] - 122s 762us/step - loss: 0.1422 - val_loss: 0.1958
[last] - val_bacc: 0.7994 - val_f1: 0.6450 - val_precision: 0.6503 - val_recall: 0.6397 
[majority] - val_bacc: 0.7915 - val_f1: 0.6488 - val_precision: 0.6844 - val_recall: 0.6168 

Epoch 00011: val_loss did not improve from 0.18549
Epoch 12/100
160597/160597 [==============================] - 124s 771us/step - loss: 0.1390 - val_loss: 0.2066
[last] - val_bacc: 0.7999 - val_f1: 0.6315 - val_precision: 0.6161 - val_recall: 0.6477 
[majority] - val_bacc: 0.7919 - val_f1: 0.6470 - val_precision: 0.6778 - val_recall: 0.6188 

Epoch 00012: val_loss did not improve from 0.18549
Epoch 13/100
160597/160597 [==============================] - 124s 771us/step - loss: 0.1363 - val_loss: 0.2052
[last] - val_bacc: 0.7994 - val_f1: 0.6354 - val_precision: 0.6267 - val_recall: 0.6444 
[majority] - val_bacc: 0.7913 - val_f1: 0.6505 - val_precision: 0.6898 - val_recall: 0.6154 

Epoch 00013: val_loss did not improve from 0.18549
Epoch 14/100
160597/160597 [==============================] - 124s 773us/step - loss: 0.1344 - val_loss: 0.2063
[last] - val_bacc: 0.7864 - val_f1: 0.6322 - val_precision: 0.6549 - val_recall: 0.6111 
[majority] - val_bacc: 0.7850 - val_f1: 0.6489 - val_precision: 0.7072 - val_recall: 0.5995 

Epoch 00014: val_loss did not improve from 0.18549
Epoch 15/100
160597/160597 [==============================] - 125s 776us/step - loss: 0.1329 - val_loss: 0.2091
[last] - val_bacc: 0.7779 - val_f1: 0.6269 - val_precision: 0.6677 - val_recall: 0.5908 
[majority] - val_bacc: 0.7798 - val_f1: 0.6471 - val_precision: 0.7217 - val_recall: 0.5865 

Epoch 00015: val_loss did not improve from 0.18549
Epoch 00015: early stopping
	Min train loss: 0.13290317990101766 @epoch 14
	Min valid loss: 0.18549105022629553 @epoch 4

<Figure size 1000x600 with 1 Axes>

INFO:
	2fold_BN3_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 159222 samples, validate on 28070 samples
Epoch 1/100
159222/159222 [==============================] - 126s 790us/step - loss: 0.4260 - val_loss: 0.2802
[last] - val_bacc: 0.7921 - val_f1: 0.6860 - val_precision: 0.7812 - val_recall: 0.6115 
[majority] - val_bacc: 0.7873 - val_f1: 0.6891 - val_precision: 0.8168 - val_recall: 0.5959 

Epoch 00001: val_loss improved from inf to 0.28016, saving model to ./checkpoints/2fold_BN3_32-32u_32ws_12f/m_0001_0.4260_0.2802.hdf5
Epoch 2/100
159222/159222 [==============================] - 123s 770us/step - loss: 0.2531 - val_loss: 0.2476
[last] - val_bacc: 0.7879 - val_f1: 0.6578 - val_precision: 0.7045 - val_recall: 0.6169 
[majority] - val_bacc: 0.7836 - val_f1: 0.6732 - val_precision: 0.7758 - val_recall: 0.5946 

Epoch 00002: val_loss improved from 0.28016 to 0.24759, saving model to ./checkpoints/2fold_BN3_32-32u_32ws_12f/m_0002_0.2531_0.2476.hdf5
Epoch 3/100
159222/159222 [==============================] - 123s 771us/step - loss: 0.2014 - val_loss: 0.2393
[last] - val_bacc: 0.7706 - val_f1: 0.6461 - val_precision: 0.7400 - val_recall: 0.5733 
[majority] - val_bacc: 0.7681 - val_f1: 0.6536 - val_precision: 0.7830 - val_recall: 0.5609 

Epoch 00003: val_loss improved from 0.24759 to 0.23931, saving model to ./checkpoints/2fold_BN3_32-32u_32ws_12f/m_0003_0.2014_0.2393.hdf5
Epoch 4/100
159222/159222 [==============================] - 125s 786us/step - loss: 0.1799 - val_loss: 0.2409
[last] - val_bacc: 0.7944 - val_f1: 0.6578 - val_precision: 0.6807 - val_recall: 0.6364 
[majority] - val_bacc: 0.7929 - val_f1: 0.6761 - val_precision: 0.7437 - val_recall: 0.6198 

Epoch 00004: val_loss did not improve from 0.23931
Epoch 5/100
159222/159222 [==============================] - 122s 766us/step - loss: 0.1683 - val_loss: 0.2442
[last] - val_bacc: 0.7990 - val_f1: 0.6530 - val_precision: 0.6527 - val_recall: 0.6533 
[majority] - val_bacc: 0.7931 - val_f1: 0.6679 - val_precision: 0.7161 - val_recall: 0.6257 

Epoch 00005: val_loss did not improve from 0.23931
Epoch 6/100
159222/159222 [==============================] - 124s 779us/step - loss: 0.1606 - val_loss: 0.2574
[last] - val_bacc: 0.8109 - val_f1: 0.6435 - val_precision: 0.5983 - val_recall: 0.6961 
[majority] - val_bacc: 0.8124 - val_f1: 0.6742 - val_precision: 0.6707 - val_recall: 0.6777 

Epoch 00006: val_loss did not improve from 0.23931
Epoch 7/100
159222/159222 [==============================] - 122s 768us/step - loss: 0.1552 - val_loss: 0.2568
[last] - val_bacc: 0.7757 - val_f1: 0.6227 - val_precision: 0.6412 - val_recall: 0.6052 
[majority] - val_bacc: 0.7788 - val_f1: 0.6423 - val_precision: 0.6907 - val_recall: 0.6003 

Epoch 00007: val_loss did not improve from 0.23931
Epoch 8/100
159222/159222 [==============================] - 123s 770us/step - loss: 0.1513 - val_loss: 0.2680
[last] - val_bacc: 0.7583 - val_f1: 0.6023 - val_precision: 0.6425 - val_recall: 0.5668 
[majority] - val_bacc: 0.7651 - val_f1: 0.6298 - val_precision: 0.7073 - val_recall: 0.5676 

Epoch 00008: val_loss did not improve from 0.23931
Epoch 9/100
159222/159222 [==============================] - 123s 774us/step - loss: 0.1469 - val_loss: 0.2663
[last] - val_bacc: 0.7759 - val_f1: 0.6255 - val_precision: 0.6490 - val_recall: 0.6037 
[majority] - val_bacc: 0.7793 - val_f1: 0.6477 - val_precision: 0.7060 - val_recall: 0.5982 

Epoch 00009: val_loss did not improve from 0.23931
Epoch 10/100
159222/159222 [==============================] - 122s 769us/step - loss: 0.1436 - val_loss: 0.2737
[last] - val_bacc: 0.7731 - val_f1: 0.6150 - val_precision: 0.6273 - val_recall: 0.6032 
[majority] - val_bacc: 0.7781 - val_f1: 0.6425 - val_precision: 0.6939 - val_recall: 0.5982 

Epoch 00010: val_loss did not improve from 0.23931
Epoch 11/100
159222/159222 [==============================] - 124s 780us/step - loss: 0.1407 - val_loss: 0.2776
[last] - val_bacc: 0.7822 - val_f1: 0.6166 - val_precision: 0.6037 - val_recall: 0.6302 
[majority] - val_bacc: 0.7867 - val_f1: 0.6431 - val_precision: 0.6638 - val_recall: 0.6237 

Epoch 00011: val_loss did not improve from 0.23931
Epoch 12/100
159222/159222 [==============================] - 123s 774us/step - loss: 0.1382 - val_loss: 0.2796
[last] - val_bacc: 0.7650 - val_f1: 0.6153 - val_precision: 0.6580 - val_recall: 0.5777 
[majority] - val_bacc: 0.7690 - val_f1: 0.6384 - val_precision: 0.7196 - val_recall: 0.5736 

Epoch 00012: val_loss did not improve from 0.23931
Epoch 13/100
159222/159222 [==============================] - 123s 771us/step - loss: 0.1358 - val_loss: 0.2790
[last] - val_bacc: 0.7610 - val_f1: 0.6070 - val_precision: 0.6473 - val_recall: 0.5715 
[majority] - val_bacc: 0.7640 - val_f1: 0.6267 - val_precision: 0.7016 - val_recall: 0.5663 

Epoch 00013: val_loss did not improve from 0.23931
Epoch 00013: early stopping
	Min train loss: 0.13575601322433847 @epoch 12
	Min valid loss: 0.23931352315469911 @epoch 2

<Figure size 1000x600 with 1 Axes>

INFO:
	3fold_BN3_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 158947 samples, validate on 28022 samples
Epoch 1/100
158947/158947 [==============================] - 125s 788us/step - loss: 0.4252 - val_loss: 0.2750
[last] - val_bacc: 0.7928 - val_f1: 0.6943 - val_precision: 0.8031 - val_recall: 0.6114 
[majority] - val_bacc: 0.7869 - val_f1: 0.6892 - val_precision: 0.8150 - val_recall: 0.5970 

Epoch 00001: val_loss improved from inf to 0.27498, saving model to ./checkpoints/3fold_BN3_32-32u_32ws_12f/m_0001_0.4252_0.2750.hdf5
Epoch 2/100
158947/158947 [==============================] - 122s 770us/step - loss: 0.2535 - val_loss: 0.2483
[last] - val_bacc: 0.8295 - val_f1: 0.7448 - val_precision: 0.8149 - val_recall: 0.6858 
[majority] - val_bacc: 0.8124 - val_f1: 0.7247 - val_precision: 0.8201 - val_recall: 0.6492 

Epoch 00002: val_loss improved from 0.27498 to 0.24828, saving model to ./checkpoints/3fold_BN3_32-32u_32ws_12f/m_0002_0.2535_0.2483.hdf5
Epoch 3/100
158947/158947 [==============================] - 123s 773us/step - loss: 0.2054 - val_loss: 0.2326
[last] - val_bacc: 0.8282 - val_f1: 0.7399 - val_precision: 0.8042 - val_recall: 0.6850 
[majority] - val_bacc: 0.8089 - val_f1: 0.7196 - val_precision: 0.8178 - val_recall: 0.6424 

Epoch 00003: val_loss improved from 0.24828 to 0.23259, saving model to ./checkpoints/3fold_BN3_32-32u_32ws_12f/m_0003_0.2054_0.2326.hdf5
Epoch 4/100
158947/158947 [==============================] - 123s 776us/step - loss: 0.1841 - val_loss: 0.2219
[last] - val_bacc: 0.8015 - val_f1: 0.7114 - val_precision: 0.8236 - val_recall: 0.6260 
[majority] - val_bacc: 0.7825 - val_f1: 0.6886 - val_precision: 0.8381 - val_recall: 0.5843 

Epoch 00004: val_loss improved from 0.23259 to 0.22194, saving model to ./checkpoints/3fold_BN3_32-32u_32ws_12f/m_0004_0.1841_0.2219.hdf5
Epoch 5/100
158947/158947 [==============================] - 122s 769us/step - loss: 0.1712 - val_loss: 0.2176
[last] - val_bacc: 0.8645 - val_f1: 0.7724 - val_precision: 0.7784 - val_recall: 0.7665 
[majority] - val_bacc: 0.8398 - val_f1: 0.7501 - val_precision: 0.7934 - val_recall: 0.7114 

Epoch 00005: val_loss improved from 0.22194 to 0.21762, saving model to ./checkpoints/3fold_BN3_32-32u_32ws_12f/m_0005_0.1712_0.2176.hdf5
Epoch 6/100
158947/158947 [==============================] - 122s 767us/step - loss: 0.1628 - val_loss: 0.2193
[last] - val_bacc: 0.8272 - val_f1: 0.7404 - val_precision: 0.8100 - val_recall: 0.6819 
[majority] - val_bacc: 0.8024 - val_f1: 0.7128 - val_precision: 0.8245 - val_recall: 0.6277 

Epoch 00006: val_loss did not improve from 0.21762
Epoch 7/100
158947/158947 [==============================] - 122s 770us/step - loss: 0.1572 - val_loss: 0.2139
[last] - val_bacc: 0.8434 - val_f1: 0.7623 - val_precision: 0.8175 - val_recall: 0.7140 
[majority] - val_bacc: 0.8193 - val_f1: 0.7350 - val_precision: 0.8251 - val_recall: 0.6626 

Epoch 00007: val_loss improved from 0.21762 to 0.21393, saving model to ./checkpoints/3fold_BN3_32-32u_32ws_12f/m_0007_0.1572_0.2139.hdf5
Epoch 8/100
158947/158947 [==============================] - 123s 775us/step - loss: 0.1521 - val_loss: 0.2199
[last] - val_bacc: 0.8247 - val_f1: 0.7403 - val_precision: 0.8199 - val_recall: 0.6748 
[majority] - val_bacc: 0.8027 - val_f1: 0.7137 - val_precision: 0.8264 - val_recall: 0.6280 

Epoch 00008: val_loss did not improve from 0.21393
Epoch 9/100
158947/158947 [==============================] - 122s 766us/step - loss: 0.1480 - val_loss: 0.2247
[last] - val_bacc: 0.8255 - val_f1: 0.7395 - val_precision: 0.8136 - val_recall: 0.6777 
[majority] - val_bacc: 0.8078 - val_f1: 0.7198 - val_precision: 0.8240 - val_recall: 0.6390 

Epoch 00009: val_loss did not improve from 0.21393
Epoch 10/100
158947/158947 [==============================] - 122s 767us/step - loss: 0.1448 - val_loss: 0.2311
[last] - val_bacc: 0.8266 - val_f1: 0.7371 - val_precision: 0.8017 - val_recall: 0.6821 
[majority] - val_bacc: 0.8008 - val_f1: 0.7092 - val_precision: 0.8190 - val_recall: 0.6253 

Epoch 00010: val_loss did not improve from 0.21393
Epoch 11/100
158947/158947 [==============================] - 125s 784us/step - loss: 0.1416 - val_loss: 0.2311
[last] - val_bacc: 0.8208 - val_f1: 0.7305 - val_precision: 0.8035 - val_recall: 0.6697 
[majority] - val_bacc: 0.8023 - val_f1: 0.7093 - val_precision: 0.8120 - val_recall: 0.6297 

Epoch 00011: val_loss did not improve from 0.21393
Epoch 12/100
158947/158947 [==============================] - 120s 757us/step - loss: 0.1387 - val_loss: 0.2354
[last] - val_bacc: 0.8126 - val_f1: 0.7191 - val_precision: 0.7995 - val_recall: 0.6533 
[majority] - val_bacc: 0.7948 - val_f1: 0.7005 - val_precision: 0.8168 - val_recall: 0.6131 

Epoch 00012: val_loss did not improve from 0.21393
Epoch 13/100
158947/158947 [==============================] - 123s 772us/step - loss: 0.1358 - val_loss: 0.2402
[last] - val_bacc: 0.8130 - val_f1: 0.7204 - val_precision: 0.8025 - val_recall: 0.6536 
[majority] - val_bacc: 0.7981 - val_f1: 0.7058 - val_precision: 0.8202 - val_recall: 0.6195 

Epoch 00013: val_loss did not improve from 0.21393
Epoch 14/100
158947/158947 [==============================] - 123s 772us/step - loss: 0.1337 - val_loss: 0.2365
[last] - val_bacc: 0.8206 - val_f1: 0.7277 - val_precision: 0.7949 - val_recall: 0.6709 
[majority] - val_bacc: 0.8040 - val_f1: 0.7114 - val_precision: 0.8114 - val_recall: 0.6333 

Epoch 00014: val_loss did not improve from 0.21393
Epoch 15/100
158947/158947 [==============================] - 122s 770us/step - loss: 0.1324 - val_loss: 0.2352
[last] - val_bacc: 0.8282 - val_f1: 0.7355 - val_precision: 0.7904 - val_recall: 0.6877 
[majority] - val_bacc: 0.8104 - val_f1: 0.7197 - val_precision: 0.8115 - val_recall: 0.6465 

Epoch 00015: val_loss did not improve from 0.21393
Epoch 16/100
158947/158947 [==============================] - 123s 776us/step - loss: 0.1300 - val_loss: 0.2523
[last] - val_bacc: 0.8091 - val_f1: 0.7132 - val_precision: 0.7944 - val_recall: 0.6470 
[majority] - val_bacc: 0.7947 - val_f1: 0.6988 - val_precision: 0.8105 - val_recall: 0.6141 

Epoch 00016: val_loss did not improve from 0.21393
Epoch 17/100
158947/158947 [==============================] - 123s 774us/step - loss: 0.1293 - val_loss: 0.2439
[last] - val_bacc: 0.8167 - val_f1: 0.7225 - val_precision: 0.7939 - val_recall: 0.6628 
[majority] - val_bacc: 0.8007 - val_f1: 0.7069 - val_precision: 0.8110 - val_recall: 0.6265 

Epoch 00017: val_loss did not improve from 0.21393
Epoch 00017: early stopping
	Min train loss: 0.12934762981870507 @epoch 16
	Min valid loss: 0.2139297849019767 @epoch 6

<Figure size 1000x600 with 1 Axes>

INFO:
	4fold_BN3_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 157912 samples, validate on 27839 samples
Epoch 1/100
157912/157912 [==============================] - 124s 786us/step - loss: 0.4300 - val_loss: 0.2372
[last] - val_bacc: 0.7673 - val_f1: 0.6445 - val_precision: 0.7711 - val_recall: 0.5536 
[majority] - val_bacc: 0.7518 - val_f1: 0.6225 - val_precision: 0.7725 - val_recall: 0.5213 

Epoch 00001: val_loss improved from inf to 0.23721, saving model to ./checkpoints/4fold_BN3_32-32u_32ws_12f/m_0001_0.4300_0.2372.hdf5
Epoch 2/100
157912/157912 [==============================] - 123s 778us/step - loss: 0.2547 - val_loss: 0.2116
[last] - val_bacc: 0.7811 - val_f1: 0.6668 - val_precision: 0.7824 - val_recall: 0.5809 
[majority] - val_bacc: 0.7607 - val_f1: 0.6427 - val_precision: 0.8005 - val_recall: 0.5369 

Epoch 00002: val_loss improved from 0.23721 to 0.21164, saving model to ./checkpoints/4fold_BN3_32-32u_32ws_12f/m_0002_0.2547_0.2116.hdf5
Epoch 3/100
157912/157912 [==============================] - 123s 780us/step - loss: 0.2039 - val_loss: 0.1983
[last] - val_bacc: 0.8044 - val_f1: 0.6900 - val_precision: 0.7599 - val_recall: 0.6319 
[majority] - val_bacc: 0.7869 - val_f1: 0.6703 - val_precision: 0.7684 - val_recall: 0.5945 

Epoch 00003: val_loss improved from 0.21164 to 0.19825, saving model to ./checkpoints/4fold_BN3_32-32u_32ws_12f/m_0003_0.2039_0.1983.hdf5
Epoch 4/100
157912/157912 [==============================] - 123s 780us/step - loss: 0.1814 - val_loss: 0.1847
[last] - val_bacc: 0.8031 - val_f1: 0.6900 - val_precision: 0.7650 - val_recall: 0.6284 
[majority] - val_bacc: 0.7748 - val_f1: 0.6582 - val_precision: 0.7830 - val_recall: 0.5678 

Epoch 00004: val_loss improved from 0.19825 to 0.18475, saving model to ./checkpoints/4fold_BN3_32-32u_32ws_12f/m_0004_0.1814_0.1847.hdf5
Epoch 5/100
157912/157912 [==============================] - 124s 785us/step - loss: 0.1683 - val_loss: 0.1817
[last] - val_bacc: 0.7954 - val_f1: 0.6848 - val_precision: 0.7793 - val_recall: 0.6107 
[majority] - val_bacc: 0.7733 - val_f1: 0.6592 - val_precision: 0.7939 - val_recall: 0.5636 

Epoch 00005: val_loss improved from 0.18475 to 0.18169, saving model to ./checkpoints/4fold_BN3_32-32u_32ws_12f/m_0005_0.1683_0.1817.hdf5
Epoch 6/100
157912/157912 [==============================] - 122s 774us/step - loss: 0.1609 - val_loss: 0.1836
[last] - val_bacc: 0.7936 - val_f1: 0.6786 - val_precision: 0.7668 - val_recall: 0.6087 
[majority] - val_bacc: 0.7760 - val_f1: 0.6601 - val_precision: 0.7837 - val_recall: 0.5702 

Epoch 00006: val_loss did not improve from 0.18169
Epoch 7/100
157912/157912 [==============================] - 123s 781us/step - loss: 0.1548 - val_loss: 0.1822
[last] - val_bacc: 0.8137 - val_f1: 0.6904 - val_precision: 0.7293 - val_recall: 0.6555 
[majority] - val_bacc: 0.7886 - val_f1: 0.6691 - val_precision: 0.7573 - val_recall: 0.5993 

Epoch 00007: val_loss did not improve from 0.18169
Epoch 8/100
157912/157912 [==============================] - 122s 774us/step - loss: 0.1496 - val_loss: 0.1851
[last] - val_bacc: 0.7921 - val_f1: 0.6740 - val_precision: 0.7582 - val_recall: 0.6066 
[majority] - val_bacc: 0.7744 - val_f1: 0.6585 - val_precision: 0.7857 - val_recall: 0.5667 

Epoch 00008: val_loss did not improve from 0.18169
Epoch 9/100
157912/157912 [==============================] - 122s 774us/step - loss: 0.1460 - val_loss: 0.1899
[last] - val_bacc: 0.7888 - val_f1: 0.6632 - val_precision: 0.7377 - val_recall: 0.6024 
[majority] - val_bacc: 0.7759 - val_f1: 0.6557 - val_precision: 0.7688 - val_recall: 0.5716 

Epoch 00009: val_loss did not improve from 0.18169
Epoch 10/100
157912/157912 [==============================] - 123s 779us/step - loss: 0.1424 - val_loss: 0.1901
[last] - val_bacc: 0.7795 - val_f1: 0.6616 - val_precision: 0.7721 - val_recall: 0.5789 
[majority] - val_bacc: 0.7620 - val_f1: 0.6433 - val_precision: 0.7953 - val_recall: 0.5400 

Epoch 00010: val_loss did not improve from 0.18169
Epoch 11/100
157912/157912 [==============================] - 122s 771us/step - loss: 0.1398 - val_loss: 0.1918
[last] - val_bacc: 0.7985 - val_f1: 0.6620 - val_precision: 0.6999 - val_recall: 0.6281 
[majority] - val_bacc: 0.7860 - val_f1: 0.6624 - val_precision: 0.7463 - val_recall: 0.5955 

Epoch 00011: val_loss did not improve from 0.18169
Epoch 12/100
157912/157912 [==============================] - 122s 774us/step - loss: 0.1367 - val_loss: 0.1955
[last] - val_bacc: 0.7793 - val_f1: 0.6532 - val_precision: 0.7448 - val_recall: 0.5816 
[majority] - val_bacc: 0.7733 - val_f1: 0.6562 - val_precision: 0.7832 - val_recall: 0.5646 

Epoch 00012: val_loss did not improve from 0.18169
Epoch 13/100
157912/157912 [==============================] - 124s 782us/step - loss: 0.1346 - val_loss: 0.1956
[last] - val_bacc: 0.8054 - val_f1: 0.6728 - val_precision: 0.7073 - val_recall: 0.6416 
[majority] - val_bacc: 0.7869 - val_f1: 0.6642 - val_precision: 0.7487 - val_recall: 0.5969 

Epoch 00013: val_loss did not improve from 0.18169
Epoch 14/100
157912/157912 [==============================] - 121s 764us/step - loss: 0.1323 - val_loss: 0.1977
[last] - val_bacc: 0.7824 - val_f1: 0.6527 - val_precision: 0.7305 - val_recall: 0.5899 
[majority] - val_bacc: 0.7766 - val_f1: 0.6569 - val_precision: 0.7696 - val_recall: 0.5730 

Epoch 00014: val_loss did not improve from 0.18169
Epoch 15/100
157912/157912 [==============================] - 123s 782us/step - loss: 0.1307 - val_loss: 0.1954
[last] - val_bacc: 0.8061 - val_f1: 0.6783 - val_precision: 0.7203 - val_recall: 0.6409 
[majority] - val_bacc: 0.7872 - val_f1: 0.6663 - val_precision: 0.7539 - val_recall: 0.5969 

Epoch 00015: val_loss did not improve from 0.18169
Epoch 00015: early stopping
	Min train loss: 0.13073042082209205 @epoch 14
	Min valid loss: 0.18169155191810643 @epoch 4

<Figure size 1000x600 with 1 Axes>

INFO:
	5fold_BN3_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 157649 samples, validate on 27793 samples
Epoch 1/100
157649/157649 [==============================] - 128s 809us/step - loss: 0.4246 - val_loss: 0.2931
[last] - val_bacc: 0.7750 - val_f1: 0.6473 - val_precision: 0.7243 - val_recall: 0.5850 
[majority] - val_bacc: 0.7721 - val_f1: 0.6442 - val_precision: 0.7268 - val_recall: 0.5784 

Epoch 00001: val_loss improved from inf to 0.29312, saving model to ./checkpoints/5fold_BN3_32-32u_32ws_12f/m_0001_0.4246_0.2931.hdf5
Epoch 2/100
157649/157649 [==============================] - 125s 790us/step - loss: 0.2523 - val_loss: 0.2546
[last] - val_bacc: 0.7795 - val_f1: 0.6592 - val_precision: 0.7454 - val_recall: 0.5908 
[majority] - val_bacc: 0.7687 - val_f1: 0.6534 - val_precision: 0.7791 - val_recall: 0.5625 

Epoch 00002: val_loss improved from 0.29312 to 0.25456, saving model to ./checkpoints/5fold_BN3_32-32u_32ws_12f/m_0002_0.2523_0.2546.hdf5
Epoch 3/100
157649/157649 [==============================] - 125s 796us/step - loss: 0.2012 - val_loss: 0.2546
[last] - val_bacc: 0.7692 - val_f1: 0.6410 - val_precision: 0.7292 - val_recall: 0.5718 
[majority] - val_bacc: 0.7539 - val_f1: 0.6291 - val_precision: 0.7671 - val_recall: 0.5332 

Epoch 00003: val_loss did not improve from 0.25456
Epoch 4/100
157649/157649 [==============================] - 125s 791us/step - loss: 0.1776 - val_loss: 0.2485
[last] - val_bacc: 0.7637 - val_f1: 0.6339 - val_precision: 0.7305 - val_recall: 0.5599 
[majority] - val_bacc: 0.7556 - val_f1: 0.6317 - val_precision: 0.7672 - val_recall: 0.5369 

Epoch 00004: val_loss improved from 0.25456 to 0.24853, saving model to ./checkpoints/5fold_BN3_32-32u_32ws_12f/m_0004_0.1776_0.2485.hdf5
Epoch 5/100
157649/157649 [==============================] - 124s 784us/step - loss: 0.1663 - val_loss: 0.2528
[last] - val_bacc: 0.7641 - val_f1: 0.6368 - val_precision: 0.7397 - val_recall: 0.5591 
[majority] - val_bacc: 0.7566 - val_f1: 0.6380 - val_precision: 0.7883 - val_recall: 0.5358 

Epoch 00005: val_loss did not improve from 0.24853
Epoch 6/100
157649/157649 [==============================] - 125s 791us/step - loss: 0.1574 - val_loss: 0.2434
[last] - val_bacc: 0.7659 - val_f1: 0.6386 - val_precision: 0.7366 - val_recall: 0.5636 
[majority] - val_bacc: 0.7577 - val_f1: 0.6344 - val_precision: 0.7661 - val_recall: 0.5414 

Epoch 00006: val_loss improved from 0.24853 to 0.24343, saving model to ./checkpoints/5fold_BN3_32-32u_32ws_12f/m_0006_0.1574_0.2434.hdf5
Epoch 7/100
157649/157649 [==============================] - 124s 787us/step - loss: 0.1517 - val_loss: 0.2558
[last] - val_bacc: 0.7822 - val_f1: 0.6530 - val_precision: 0.7124 - val_recall: 0.6028 
[majority] - val_bacc: 0.7662 - val_f1: 0.6435 - val_precision: 0.7541 - val_recall: 0.5612 

Epoch 00007: val_loss did not improve from 0.24343
Epoch 8/100
157649/157649 [==============================] - 123s 782us/step - loss: 0.1469 - val_loss: 0.2570
[last] - val_bacc: 0.7568 - val_f1: 0.6305 - val_precision: 0.7553 - val_recall: 0.5411 
[majority] - val_bacc: 0.7491 - val_f1: 0.6262 - val_precision: 0.7856 - val_recall: 0.5205 

Epoch 00008: val_loss did not improve from 0.24343
Epoch 9/100
157649/157649 [==============================] - 122s 776us/step - loss: 0.1432 - val_loss: 0.2590
[last] - val_bacc: 0.7922 - val_f1: 0.6585 - val_precision: 0.6917 - val_recall: 0.6284 
[majority] - val_bacc: 0.7767 - val_f1: 0.6536 - val_precision: 0.7387 - val_recall: 0.5861 

Epoch 00009: val_loss did not improve from 0.24343
Epoch 10/100
157649/157649 [==============================] - 124s 789us/step - loss: 0.1393 - val_loss: 0.2660
[last] - val_bacc: 0.7471 - val_f1: 0.6142 - val_precision: 0.7453 - val_recall: 0.5223 
[majority] - val_bacc: 0.7428 - val_f1: 0.6113 - val_precision: 0.7606 - val_recall: 0.5110 

Epoch 00010: val_loss did not improve from 0.24343
Epoch 11/100
157649/157649 [==============================] - 116s 735us/step - loss: 0.1370 - val_loss: 0.2653
[last] - val_bacc: 0.7692 - val_f1: 0.6437 - val_precision: 0.7389 - val_recall: 0.5702 
[majority] - val_bacc: 0.7618 - val_f1: 0.6378 - val_precision: 0.7558 - val_recall: 0.5517 

Epoch 00011: val_loss did not improve from 0.24343
Epoch 12/100
157649/157649 [==============================] - 118s 747us/step - loss: 0.1344 - val_loss: 0.2718
[last] - val_bacc: 0.7564 - val_f1: 0.6232 - val_precision: 0.7279 - val_recall: 0.5448 
[majority] - val_bacc: 0.7554 - val_f1: 0.6276 - val_precision: 0.7517 - val_recall: 0.5387 

Epoch 00012: val_loss did not improve from 0.24343
Epoch 13/100
157649/157649 [==============================] - 124s 789us/step - loss: 0.1318 - val_loss: 0.2773
[last] - val_bacc: 0.7586 - val_f1: 0.6227 - val_precision: 0.7142 - val_recall: 0.5520 
[majority] - val_bacc: 0.7541 - val_f1: 0.6268 - val_precision: 0.7556 - val_recall: 0.5356 

Epoch 00013: val_loss did not improve from 0.24343
Epoch 14/100
157649/157649 [==============================] - 124s 785us/step - loss: 0.1304 - val_loss: 0.2836
[last] - val_bacc: 0.7497 - val_f1: 0.6163 - val_precision: 0.7383 - val_recall: 0.5290 
[majority] - val_bacc: 0.7459 - val_f1: 0.6183 - val_precision: 0.7711 - val_recall: 0.5160 

Epoch 00014: val_loss did not improve from 0.24343
Epoch 15/100
157649/157649 [==============================] - 126s 798us/step - loss: 0.1277 - val_loss: 0.2939
[last] - val_bacc: 0.7580 - val_f1: 0.6164 - val_precision: 0.6937 - val_recall: 0.5546 
[majority] - val_bacc: 0.7551 - val_f1: 0.6257 - val_precision: 0.7450 - val_recall: 0.5393 

Epoch 00015: val_loss did not improve from 0.24343
Epoch 16/100
157649/157649 [==============================] - 124s 788us/step - loss: 0.1265 - val_loss: 0.2816
[last] - val_bacc: 0.7687 - val_f1: 0.6358 - val_precision: 0.7126 - val_recall: 0.5739 
[majority] - val_bacc: 0.7667 - val_f1: 0.6427 - val_precision: 0.7486 - val_recall: 0.5631 

Epoch 00016: val_loss did not improve from 0.24343
Epoch 00016: early stopping
	Min train loss: 0.1265412977257721 @epoch 15
	Min valid loss: 0.24343339313595422 @epoch 5

<Figure size 1000x600 with 1 Axes>

INFO:
	6fold_BN3_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 160904 samples, validate on 28367 samples
Epoch 1/100
160904/160904 [==============================] - 128s 796us/step - loss: 0.4261 - val_loss: 0.2823
[last] - val_bacc: 0.8060 - val_f1: 0.7005 - val_precision: 0.7735 - val_recall: 0.6400 
[majority] - val_bacc: 0.7999 - val_f1: 0.6973 - val_precision: 0.7887 - val_recall: 0.6249 

Epoch 00001: val_loss improved from inf to 0.28229, saving model to ./checkpoints/6fold_BN3_32-32u_32ws_12f/m_0001_0.4261_0.2823.hdf5
Epoch 2/100
160904/160904 [==============================] - 123s 766us/step - loss: 0.2535 - val_loss: 0.2316
[last] - val_bacc: 0.8109 - val_f1: 0.7050 - val_precision: 0.7685 - val_recall: 0.6511 
[majority] - val_bacc: 0.7998 - val_f1: 0.6960 - val_precision: 0.7850 - val_recall: 0.6251 

Epoch 00002: val_loss improved from 0.28229 to 0.23159, saving model to ./checkpoints/6fold_BN3_32-32u_32ws_12f/m_0002_0.2535_0.2316.hdf5
Epoch 3/100
160904/160904 [==============================] - 125s 779us/step - loss: 0.2050 - val_loss: 0.2133
[last] - val_bacc: 0.7728 - val_f1: 0.6625 - val_precision: 0.7957 - val_recall: 0.5674 
[majority] - val_bacc: 0.7701 - val_f1: 0.6642 - val_precision: 0.8187 - val_recall: 0.5588 

Epoch 00003: val_loss improved from 0.23159 to 0.21325, saving model to ./checkpoints/6fold_BN3_32-32u_32ws_12f/m_0003_0.2050_0.2133.hdf5
Epoch 4/100
160904/160904 [==============================] - 120s 745us/step - loss: 0.1846 - val_loss: 0.2040
[last] - val_bacc: 0.8203 - val_f1: 0.7137 - val_precision: 0.7606 - val_recall: 0.6723 
[majority] - val_bacc: 0.8069 - val_f1: 0.7075 - val_precision: 0.7929 - val_recall: 0.6387 

Epoch 00004: val_loss improved from 0.21325 to 0.20402, saving model to ./checkpoints/6fold_BN3_32-32u_32ws_12f/m_0004_0.1846_0.2040.hdf5
Epoch 5/100
160904/160904 [==============================] - 125s 778us/step - loss: 0.1729 - val_loss: 0.2068
[last] - val_bacc: 0.7787 - val_f1: 0.6676 - val_precision: 0.7841 - val_recall: 0.5813 
[majority] - val_bacc: 0.7693 - val_f1: 0.6621 - val_precision: 0.8145 - val_recall: 0.5577 

Epoch 00005: val_loss did not improve from 0.20402
Epoch 6/100
160904/160904 [==============================] - 123s 765us/step - loss: 0.1658 - val_loss: 0.2047
[last] - val_bacc: 0.8027 - val_f1: 0.6946 - val_precision: 0.7678 - val_recall: 0.6341 
[majority] - val_bacc: 0.7907 - val_f1: 0.6874 - val_precision: 0.7966 - val_recall: 0.6046 

Epoch 00006: val_loss did not improve from 0.20402
Epoch 7/100
160904/160904 [==============================] - 124s 773us/step - loss: 0.1600 - val_loss: 0.2091
[last] - val_bacc: 0.7788 - val_f1: 0.6681 - val_precision: 0.7855 - val_recall: 0.5813 
[majority] - val_bacc: 0.7703 - val_f1: 0.6620 - val_precision: 0.8085 - val_recall: 0.5604 

Epoch 00007: val_loss did not improve from 0.20402
Epoch 8/100
160904/160904 [==============================] - 123s 766us/step - loss: 0.1558 - val_loss: 0.2103
[last] - val_bacc: 0.7808 - val_f1: 0.6696 - val_precision: 0.7802 - val_recall: 0.5864 
[majority] - val_bacc: 0.7741 - val_f1: 0.6669 - val_precision: 0.8064 - val_recall: 0.5685 

Epoch 00008: val_loss did not improve from 0.20402
Epoch 9/100
160904/160904 [==============================] - 124s 769us/step - loss: 0.1505 - val_loss: 0.2082
[last] - val_bacc: 0.8254 - val_f1: 0.7132 - val_precision: 0.7419 - val_recall: 0.6866 
[majority] - val_bacc: 0.8129 - val_f1: 0.7118 - val_precision: 0.7823 - val_recall: 0.6530 

Epoch 00009: val_loss did not improve from 0.20402
Epoch 10/100
160904/160904 [==============================] - 123s 764us/step - loss: 0.1470 - val_loss: 0.2127
[last] - val_bacc: 0.7821 - val_f1: 0.6727 - val_precision: 0.7852 - val_recall: 0.5883 
[majority] - val_bacc: 0.7744 - val_f1: 0.6691 - val_precision: 0.8135 - val_recall: 0.5683 

Epoch 00010: val_loss did not improve from 0.20402
Epoch 11/100
160904/160904 [==============================] - 121s 752us/step - loss: 0.1434 - val_loss: 0.2209
[last] - val_bacc: 0.7920 - val_f1: 0.6785 - val_precision: 0.7597 - val_recall: 0.6129 
[majority] - val_bacc: 0.7855 - val_f1: 0.6794 - val_precision: 0.7929 - val_recall: 0.5943 

Epoch 00011: val_loss did not improve from 0.20402
Epoch 12/100
160904/160904 [==============================] - 125s 779us/step - loss: 0.1412 - val_loss: 0.2208
[last] - val_bacc: 0.7866 - val_f1: 0.6705 - val_precision: 0.7559 - val_recall: 0.6024 
[majority] - val_bacc: 0.7776 - val_f1: 0.6677 - val_precision: 0.7899 - val_recall: 0.5783 

Epoch 00012: val_loss did not improve from 0.20402
Epoch 13/100
160904/160904 [==============================] - 125s 779us/step - loss: 0.1373 - val_loss: 0.2208
[last] - val_bacc: 0.7927 - val_f1: 0.6807 - val_precision: 0.7640 - val_recall: 0.6138 
[majority] - val_bacc: 0.7807 - val_f1: 0.6755 - val_precision: 0.8030 - val_recall: 0.5829 

Epoch 00013: val_loss did not improve from 0.20402
Epoch 14/100
160904/160904 [==============================] - 124s 769us/step - loss: 0.1356 - val_loss: 0.2263
[last] - val_bacc: 0.7838 - val_f1: 0.6692 - val_precision: 0.7644 - val_recall: 0.5951 
[majority] - val_bacc: 0.7817 - val_f1: 0.6759 - val_precision: 0.7998 - val_recall: 0.5853 

Epoch 00014: val_loss did not improve from 0.20402
Epoch 00014: early stopping
	Min train loss: 0.13559950169733184 @epoch 13
	Min valid loss: 0.2040193659723988 @epoch 3
