BN2


WARNING:tensorflow:From /home/ICT2000/jondras/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /home/ICT2000/jondras/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
masking_1 (Masking)          (None, 32, 12)            0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 32, 12)            48        
_________________________________________________________________
gru_1 (GRU)                  (None, 32, 32)            4320      
_________________________________________________________________
batch_normalization_2 (Batch (None, 32, 32)            128       
_________________________________________________________________
gru_2 (GRU)                  (None, 32, 32)            6240      
_________________________________________________________________
batch_normalization_3 (Batch (None, 32, 32)            128       
_________________________________________________________________
time_distributed_1 (TimeDist (None, 32, 1)             33        
=================================================================
Total params: 10,897
Trainable params: 10,745
Non-trainable params: 152
_________________________________________________________________
None
INFO:
	0fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
WARNING:tensorflow:From /home/ICT2000/jondras/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Train on 159911 samples, validate on 28192 samples
Epoch 1/100
159911/159911 [==============================] - 134s 841us/step - loss: 0.2452 - val_loss: 0.2192
[last] - val_bacc: 0.8190 - val_f1: 0.6649 - val_precision: 0.6414 - val_recall: 0.6903 
[majority] - val_bacc: 0.8102 - val_f1: 0.6851 - val_precision: 0.7178 - val_recall: 0.6553 

Epoch 00001: val_loss improved from inf to 0.21923, saving model to ./checkpoints/0fold_BN2_32-32u_32ws_12f/m_0001_0.2452_0.2192.hdf5
Epoch 2/100
159911/159911 [==============================] - 131s 816us/step - loss: 0.1883 - val_loss: 0.2074
[last] - val_bacc: 0.7760 - val_f1: 0.6623 - val_precision: 0.7836 - val_recall: 0.5736 
[majority] - val_bacc: 0.7724 - val_f1: 0.6608 - val_precision: 0.7969 - val_recall: 0.5643 

Epoch 00002: val_loss improved from 0.21923 to 0.20737, saving model to ./checkpoints/0fold_BN2_32-32u_32ws_12f/m_0002_0.1883_0.2074.hdf5
Epoch 3/100
159911/159911 [==============================] - 132s 825us/step - loss: 0.1773 - val_loss: 0.2046
[last] - val_bacc: 0.8179 - val_f1: 0.6910 - val_precision: 0.7098 - val_recall: 0.6731 
[majority] - val_bacc: 0.8089 - val_f1: 0.6951 - val_precision: 0.7509 - val_recall: 0.6470 

Epoch 00003: val_loss improved from 0.20737 to 0.20456, saving model to ./checkpoints/0fold_BN2_32-32u_32ws_12f/m_0003_0.1773_0.2046.hdf5
Epoch 4/100
159911/159911 [==============================] - 132s 826us/step - loss: 0.1685 - val_loss: 0.2175
[last] - val_bacc: 0.7757 - val_f1: 0.6552 - val_precision: 0.7592 - val_recall: 0.5762 
[majority] - val_bacc: 0.7742 - val_f1: 0.6600 - val_precision: 0.7844 - val_recall: 0.5697 

Epoch 00004: val_loss did not improve from 0.20456
Epoch 5/100
159911/159911 [==============================] - 133s 835us/step - loss: 0.1619 - val_loss: 0.2098
[last] - val_bacc: 0.8213 - val_f1: 0.6857 - val_precision: 0.6864 - val_recall: 0.6850 
[majority] - val_bacc: 0.8134 - val_f1: 0.6931 - val_precision: 0.7296 - val_recall: 0.6600 

Epoch 00005: val_loss did not improve from 0.20456
Epoch 6/100
159911/159911 [==============================] - 131s 819us/step - loss: 0.1554 - val_loss: 0.2189
[last] - val_bacc: 0.7941 - val_f1: 0.6735 - val_precision: 0.7406 - val_recall: 0.6175 
[majority] - val_bacc: 0.7911 - val_f1: 0.6800 - val_precision: 0.7742 - val_recall: 0.6062 

Epoch 00006: val_loss did not improve from 0.20456
Epoch 7/100
159911/159911 [==============================] - 130s 812us/step - loss: 0.1523 - val_loss: 0.2196
[last] - val_bacc: 0.7946 - val_f1: 0.6691 - val_precision: 0.7252 - val_recall: 0.6211 
[majority] - val_bacc: 0.7966 - val_f1: 0.6823 - val_precision: 0.7585 - val_recall: 0.6199 

Epoch 00007: val_loss did not improve from 0.20456
Epoch 8/100
159911/159911 [==============================] - 132s 823us/step - loss: 0.1478 - val_loss: 0.2242
[last] - val_bacc: 0.7979 - val_f1: 0.6703 - val_precision: 0.7168 - val_recall: 0.6294 
[majority] - val_bacc: 0.7917 - val_f1: 0.6785 - val_precision: 0.7665 - val_recall: 0.6086 

Epoch 00008: val_loss did not improve from 0.20456
Epoch 9/100
159911/159911 [==============================] - 131s 820us/step - loss: 0.1444 - val_loss: 0.2261
[last] - val_bacc: 0.8097 - val_f1: 0.6793 - val_precision: 0.7030 - val_recall: 0.6571 
[majority] - val_bacc: 0.8061 - val_f1: 0.6890 - val_precision: 0.7431 - val_recall: 0.6422 

Epoch 00009: val_loss did not improve from 0.20456
Epoch 10/100
159911/159911 [==============================] - 133s 834us/step - loss: 0.1409 - val_loss: 0.2301
[last] - val_bacc: 0.8202 - val_f1: 0.6824 - val_precision: 0.6810 - val_recall: 0.6838 
[majority] - val_bacc: 0.8162 - val_f1: 0.6992 - val_precision: 0.7377 - val_recall: 0.6645 

Epoch 00010: val_loss did not improve from 0.20456
Epoch 11/100
159911/159911 [==============================] - 133s 830us/step - loss: 0.1388 - val_loss: 0.2361
[last] - val_bacc: 0.8302 - val_f1: 0.6924 - val_precision: 0.6798 - val_recall: 0.7055 
[majority] - val_bacc: 0.8245 - val_f1: 0.7035 - val_precision: 0.7238 - val_recall: 0.6844 

Epoch 00011: val_loss did not improve from 0.20456
Epoch 12/100
159911/159911 [==============================] - 132s 823us/step - loss: 0.1364 - val_loss: 0.2334
[last] - val_bacc: 0.7983 - val_f1: 0.6844 - val_precision: 0.7585 - val_recall: 0.6235 
[majority] - val_bacc: 0.7940 - val_f1: 0.6880 - val_precision: 0.7887 - val_recall: 0.6101 

Epoch 00012: val_loss did not improve from 0.20456
Epoch 13/100
159911/159911 [==============================] - 130s 815us/step - loss: 0.1344 - val_loss: 0.2382
[last] - val_bacc: 0.7900 - val_f1: 0.6584 - val_precision: 0.7097 - val_recall: 0.6140 
[majority] - val_bacc: 0.7911 - val_f1: 0.6729 - val_precision: 0.7510 - val_recall: 0.6095 

Epoch 00013: val_loss did not improve from 0.20456
Epoch 00013: early stopping
	Min train loss: 0.13440616836388045 @epoch 12
	Min valid loss: 0.20455592425611974 @epoch 2

<Figure size 1000x600 with 1 Axes>

INFO:
	1fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 160597 samples, validate on 28313 samples
Epoch 1/100
160597/160597 [==============================] - 134s 837us/step - loss: 0.2431 - val_loss: 0.1863
[last] - val_bacc: 0.7738 - val_f1: 0.6335 - val_precision: 0.7030 - val_recall: 0.5765 
[majority] - val_bacc: 0.7729 - val_f1: 0.6459 - val_precision: 0.7476 - val_recall: 0.5685 

Epoch 00001: val_loss improved from inf to 0.18625, saving model to ./checkpoints/1fold_BN2_32-32u_32ws_12f/m_0001_0.2431_0.1863.hdf5
Epoch 2/100
160597/160597 [==============================] - 131s 817us/step - loss: 0.1854 - val_loss: 0.1881
[last] - val_bacc: 0.7992 - val_f1: 0.6553 - val_precision: 0.6781 - val_recall: 0.6341 
[majority] - val_bacc: 0.7955 - val_f1: 0.6639 - val_precision: 0.7140 - val_recall: 0.6204 

Epoch 00002: val_loss did not improve from 0.18625
Epoch 3/100
160597/160597 [==============================] - 131s 816us/step - loss: 0.1732 - val_loss: 0.1852
[last] - val_bacc: 0.7763 - val_f1: 0.6410 - val_precision: 0.7167 - val_recall: 0.5798 
[majority] - val_bacc: 0.7831 - val_f1: 0.6606 - val_precision: 0.7512 - val_recall: 0.5895 

Epoch 00003: val_loss improved from 0.18625 to 0.18523, saving model to ./checkpoints/1fold_BN2_32-32u_32ws_12f/m_0003_0.1732_0.1852.hdf5
Epoch 4/100
160597/160597 [==============================] - 133s 825us/step - loss: 0.1647 - val_loss: 0.1849
[last] - val_bacc: 0.7953 - val_f1: 0.6534 - val_precision: 0.6849 - val_recall: 0.6248 
[majority] - val_bacc: 0.7909 - val_f1: 0.6655 - val_precision: 0.7352 - val_recall: 0.6078 

Epoch 00004: val_loss improved from 0.18523 to 0.18495, saving model to ./checkpoints/1fold_BN2_32-32u_32ws_12f/m_0004_0.1647_0.1849.hdf5
Epoch 5/100
160597/160597 [==============================] - 131s 816us/step - loss: 0.1583 - val_loss: 0.1850
[last] - val_bacc: 0.7957 - val_f1: 0.6549 - val_precision: 0.6878 - val_recall: 0.6251 
[majority] - val_bacc: 0.7909 - val_f1: 0.6622 - val_precision: 0.7254 - val_recall: 0.6091 

Epoch 00005: val_loss did not improve from 0.18495
Epoch 6/100
160597/160597 [==============================] - 131s 817us/step - loss: 0.1515 - val_loss: 0.1878
[last] - val_bacc: 0.8096 - val_f1: 0.6709 - val_precision: 0.6882 - val_recall: 0.6544 
[majority] - val_bacc: 0.8071 - val_f1: 0.6821 - val_precision: 0.7261 - val_recall: 0.6430 

Epoch 00006: val_loss did not improve from 0.18495
Epoch 7/100
160597/160597 [==============================] - 132s 820us/step - loss: 0.1471 - val_loss: 0.2039
[last] - val_bacc: 0.8031 - val_f1: 0.6385 - val_precision: 0.6248 - val_recall: 0.6527 
[majority] - val_bacc: 0.7979 - val_f1: 0.6547 - val_precision: 0.6802 - val_recall: 0.6311 

Epoch 00007: val_loss did not improve from 0.18495
Epoch 8/100
160597/160597 [==============================] - 133s 827us/step - loss: 0.1430 - val_loss: 0.1953
[last] - val_bacc: 0.8039 - val_f1: 0.6547 - val_precision: 0.6625 - val_recall: 0.6470 
[majority] - val_bacc: 0.7968 - val_f1: 0.6638 - val_precision: 0.7090 - val_recall: 0.6241 

Epoch 00008: val_loss did not improve from 0.18495
Epoch 9/100
160597/160597 [==============================] - 132s 823us/step - loss: 0.1392 - val_loss: 0.2049
[last] - val_bacc: 0.8121 - val_f1: 0.6461 - val_precision: 0.6213 - val_recall: 0.6730 
[majority] - val_bacc: 0.8081 - val_f1: 0.6654 - val_precision: 0.6783 - val_recall: 0.6530 

Epoch 00009: val_loss did not improve from 0.18495
Epoch 10/100
160597/160597 [==============================] - 130s 809us/step - loss: 0.1361 - val_loss: 0.2038
[last] - val_bacc: 0.7795 - val_f1: 0.6314 - val_precision: 0.6753 - val_recall: 0.5928 
[majority] - val_bacc: 0.7769 - val_f1: 0.6403 - val_precision: 0.7118 - val_recall: 0.5818 

Epoch 00010: val_loss did not improve from 0.18495
Epoch 11/100
160597/160597 [==============================] - 133s 829us/step - loss: 0.1328 - val_loss: 0.2232
[last] - val_bacc: 0.8012 - val_f1: 0.6224 - val_precision: 0.5920 - val_recall: 0.6560 
[majority] - val_bacc: 0.7997 - val_f1: 0.6480 - val_precision: 0.6572 - val_recall: 0.6391 

Epoch 00011: val_loss did not improve from 0.18495
Epoch 12/100
160597/160597 [==============================] - 133s 830us/step - loss: 0.1310 - val_loss: 0.2124
[last] - val_bacc: 0.7858 - val_f1: 0.6209 - val_precision: 0.6268 - val_recall: 0.6151 
[majority] - val_bacc: 0.7877 - val_f1: 0.6458 - val_precision: 0.6885 - val_recall: 0.6081 

Epoch 00012: val_loss did not improve from 0.18495
Epoch 13/100
160597/160597 [==============================] - 133s 826us/step - loss: 0.1282 - val_loss: 0.2141
[last] - val_bacc: 0.8031 - val_f1: 0.6321 - val_precision: 0.6098 - val_recall: 0.6560 
[majority] - val_bacc: 0.7997 - val_f1: 0.6549 - val_precision: 0.6753 - val_recall: 0.6357 

Epoch 00013: val_loss did not improve from 0.18495
Epoch 14/100
160597/160597 [==============================] - 133s 827us/step - loss: 0.1260 - val_loss: 0.2127
[last] - val_bacc: 0.7806 - val_f1: 0.6281 - val_precision: 0.6620 - val_recall: 0.5975 
[majority] - val_bacc: 0.7750 - val_f1: 0.6339 - val_precision: 0.6996 - val_recall: 0.5795 

Epoch 00014: val_loss did not improve from 0.18495
Epoch 00014: early stopping
	Min train loss: 0.125964832538096 @epoch 13
	Min valid loss: 0.18494603254012318 @epoch 3

<Figure size 1000x600 with 1 Axes>

INFO:
	2fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 159222 samples, validate on 28070 samples
Epoch 1/100
159222/159222 [==============================] - 133s 835us/step - loss: 0.2429 - val_loss: 0.2292
[last] - val_bacc: 0.7799 - val_f1: 0.6587 - val_precision: 0.7411 - val_recall: 0.5928 
[majority] - val_bacc: 0.7801 - val_f1: 0.6658 - val_precision: 0.7658 - val_recall: 0.5889 

Epoch 00001: val_loss improved from inf to 0.22920, saving model to ./checkpoints/2fold_BN2_32-32u_32ws_12f/m_0001_0.2429_0.2292.hdf5
Epoch 2/100
159222/159222 [==============================] - 131s 820us/step - loss: 0.1849 - val_loss: 0.2405
[last] - val_bacc: 0.7774 - val_f1: 0.6469 - val_precision: 0.7114 - val_recall: 0.5930 
[majority] - val_bacc: 0.7807 - val_f1: 0.6611 - val_precision: 0.7459 - val_recall: 0.5936 

Epoch 00002: val_loss did not improve from 0.22920
Epoch 3/100
159222/159222 [==============================] - 131s 825us/step - loss: 0.1726 - val_loss: 0.2456
[last] - val_bacc: 0.7803 - val_f1: 0.6370 - val_precision: 0.6681 - val_recall: 0.6086 
[majority] - val_bacc: 0.7834 - val_f1: 0.6596 - val_precision: 0.7287 - val_recall: 0.6024 

Epoch 00003: val_loss did not improve from 0.22920
Epoch 4/100
159222/159222 [==============================] - 133s 837us/step - loss: 0.1648 - val_loss: 0.2646
[last] - val_bacc: 0.7378 - val_f1: 0.5773 - val_precision: 0.6477 - val_recall: 0.5206 
[majority] - val_bacc: 0.7460 - val_f1: 0.6028 - val_precision: 0.7034 - val_recall: 0.5274 

Epoch 00004: val_loss did not improve from 0.22920
Epoch 5/100
159222/159222 [==============================] - 130s 818us/step - loss: 0.1571 - val_loss: 0.2605
[last] - val_bacc: 0.7931 - val_f1: 0.6508 - val_precision: 0.6651 - val_recall: 0.6372 
[majority] - val_bacc: 0.7957 - val_f1: 0.6720 - val_precision: 0.7195 - val_recall: 0.6304 

Epoch 00005: val_loss did not improve from 0.22920
Epoch 6/100
159222/159222 [==============================] - 131s 823us/step - loss: 0.1528 - val_loss: 0.2569
[last] - val_bacc: 0.7579 - val_f1: 0.6122 - val_precision: 0.6780 - val_recall: 0.5580 
[majority] - val_bacc: 0.7597 - val_f1: 0.6280 - val_precision: 0.7278 - val_recall: 0.5523 

Epoch 00006: val_loss did not improve from 0.22920
Epoch 7/100
159222/159222 [==============================] - 131s 820us/step - loss: 0.1469 - val_loss: 0.2778
[last] - val_bacc: 0.7710 - val_f1: 0.6381 - val_precision: 0.7094 - val_recall: 0.5798 
[majority] - val_bacc: 0.7719 - val_f1: 0.6511 - val_precision: 0.7523 - val_recall: 0.5738 

Epoch 00007: val_loss did not improve from 0.22920
Epoch 8/100
159222/159222 [==============================] - 132s 829us/step - loss: 0.1438 - val_loss: 0.2774
[last] - val_bacc: 0.7883 - val_f1: 0.6343 - val_precision: 0.6336 - val_recall: 0.6351 
[majority] - val_bacc: 0.7894 - val_f1: 0.6541 - val_precision: 0.6874 - val_recall: 0.6239 

Epoch 00008: val_loss did not improve from 0.22920
Epoch 9/100
159222/159222 [==============================] - 131s 822us/step - loss: 0.1399 - val_loss: 0.2820
[last] - val_bacc: 0.7720 - val_f1: 0.6333 - val_precision: 0.6888 - val_recall: 0.5860 
[majority] - val_bacc: 0.7754 - val_f1: 0.6487 - val_precision: 0.7267 - val_recall: 0.5858 

Epoch 00009: val_loss did not improve from 0.22920
Epoch 10/100
159222/159222 [==============================] - 132s 829us/step - loss: 0.1372 - val_loss: 0.2750
[last] - val_bacc: 0.7654 - val_f1: 0.6338 - val_precision: 0.7204 - val_recall: 0.5658 
[majority] - val_bacc: 0.7686 - val_f1: 0.6436 - val_precision: 0.7408 - val_recall: 0.5689 

Epoch 00010: val_loss did not improve from 0.22920
Epoch 11/100
159222/159222 [==============================] - 132s 829us/step - loss: 0.1338 - val_loss: 0.2918
[last] - val_bacc: 0.7464 - val_f1: 0.6060 - val_precision: 0.7141 - val_recall: 0.5263 
[majority] - val_bacc: 0.7500 - val_f1: 0.6174 - val_precision: 0.7398 - val_recall: 0.5297 

Epoch 00011: val_loss did not improve from 0.22920
Epoch 00011: early stopping
	Min train loss: 0.1337891058004224 @epoch 10
	Min valid loss: 0.22920459333553744 @epoch 0

<Figure size 1000x600 with 1 Axes>

INFO:
	3fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 158947 samples, validate on 28022 samples
Epoch 1/100
158947/158947 [==============================] - 132s 832us/step - loss: 0.2462 - val_loss: 0.2235
[last] - val_bacc: 0.8123 - val_f1: 0.7056 - val_precision: 0.7565 - val_recall: 0.6611 
[majority] - val_bacc: 0.8051 - val_f1: 0.7007 - val_precision: 0.7694 - val_recall: 0.6433 

Epoch 00001: val_loss improved from inf to 0.22353, saving model to ./checkpoints/3fold_BN2_32-32u_32ws_12f/m_0001_0.2462_0.2235.hdf5
Epoch 2/100
158947/158947 [==============================] - 130s 819us/step - loss: 0.1877 - val_loss: 0.2178
[last] - val_bacc: 0.8125 - val_f1: 0.7172 - val_precision: 0.7934 - val_recall: 0.6543 
[majority] - val_bacc: 0.8074 - val_f1: 0.7149 - val_precision: 0.8082 - val_recall: 0.6409 

Epoch 00002: val_loss improved from 0.22353 to 0.21779, saving model to ./checkpoints/3fold_BN2_32-32u_32ws_12f/m_0002_0.1877_0.2178.hdf5
Epoch 3/100
158947/158947 [==============================] - 133s 834us/step - loss: 0.1768 - val_loss: 0.2169
[last] - val_bacc: 0.8125 - val_f1: 0.7191 - val_precision: 0.8004 - val_recall: 0.6529 
[majority] - val_bacc: 0.8025 - val_f1: 0.7116 - val_precision: 0.8198 - val_recall: 0.6287 

Epoch 00003: val_loss improved from 0.21779 to 0.21690, saving model to ./checkpoints/3fold_BN2_32-32u_32ws_12f/m_0003_0.1768_0.2169.hdf5
Epoch 4/100
158947/158947 [==============================] - 132s 829us/step - loss: 0.1678 - val_loss: 0.2258
[last] - val_bacc: 0.8031 - val_f1: 0.7161 - val_precision: 0.8333 - val_recall: 0.6277 
[majority] - val_bacc: 0.7954 - val_f1: 0.7070 - val_precision: 0.8390 - val_recall: 0.6109 

Epoch 00004: val_loss did not improve from 0.21690
Epoch 5/100
158947/158947 [==============================] - 130s 817us/step - loss: 0.1613 - val_loss: 0.2281
[last] - val_bacc: 0.8036 - val_f1: 0.7107 - val_precision: 0.8107 - val_recall: 0.6326 
[majority] - val_bacc: 0.7940 - val_f1: 0.6998 - val_precision: 0.8185 - val_recall: 0.6112 

Epoch 00005: val_loss did not improve from 0.21690
Epoch 6/100
158947/158947 [==============================] - 131s 826us/step - loss: 0.1565 - val_loss: 0.2298
[last] - val_bacc: 0.7925 - val_f1: 0.6977 - val_precision: 0.8180 - val_recall: 0.6082 
[majority] - val_bacc: 0.7921 - val_f1: 0.7002 - val_precision: 0.8304 - val_recall: 0.6053 

Epoch 00006: val_loss did not improve from 0.21690
Epoch 7/100
158947/158947 [==============================] - 132s 829us/step - loss: 0.1512 - val_loss: 0.2433
[last] - val_bacc: 0.7579 - val_f1: 0.6474 - val_precision: 0.8161 - val_recall: 0.5366 
[majority] - val_bacc: 0.7569 - val_f1: 0.6506 - val_precision: 0.8385 - val_recall: 0.5314 

Epoch 00007: val_loss did not improve from 0.21690
Epoch 8/100
158947/158947 [==============================] - 132s 830us/step - loss: 0.1468 - val_loss: 0.2323
[last] - val_bacc: 0.7988 - val_f1: 0.7014 - val_precision: 0.8001 - val_recall: 0.6243 
[majority] - val_bacc: 0.7937 - val_f1: 0.6990 - val_precision: 0.8168 - val_recall: 0.6109 

Epoch 00008: val_loss did not improve from 0.21690
Epoch 9/100
158947/158947 [==============================] - 131s 824us/step - loss: 0.1432 - val_loss: 0.2371
[last] - val_bacc: 0.8172 - val_f1: 0.7181 - val_precision: 0.7773 - val_recall: 0.6672 
[majority] - val_bacc: 0.8111 - val_f1: 0.7176 - val_precision: 0.8011 - val_recall: 0.6499 

Epoch 00009: val_loss did not improve from 0.21690
Epoch 10/100
158947/158947 [==============================] - 131s 826us/step - loss: 0.1388 - val_loss: 0.2392
[last] - val_bacc: 0.8233 - val_f1: 0.7222 - val_precision: 0.7672 - val_recall: 0.6821 
[majority] - val_bacc: 0.8113 - val_f1: 0.7179 - val_precision: 0.8014 - val_recall: 0.6502 

Epoch 00010: val_loss did not improve from 0.21690
Epoch 11/100
158947/158947 [==============================] - 133s 834us/step - loss: 0.1357 - val_loss: 0.2436
[last] - val_bacc: 0.8056 - val_f1: 0.7048 - val_precision: 0.7811 - val_recall: 0.6421 
[majority] - val_bacc: 0.7985 - val_f1: 0.7023 - val_precision: 0.8050 - val_recall: 0.6229 

Epoch 00011: val_loss did not improve from 0.21690
Epoch 12/100
158947/158947 [==============================] - 130s 815us/step - loss: 0.1330 - val_loss: 0.2571
[last] - val_bacc: 0.7914 - val_f1: 0.6925 - val_precision: 0.8038 - val_recall: 0.6082 
[majority] - val_bacc: 0.7872 - val_f1: 0.6908 - val_precision: 0.8196 - val_recall: 0.5970 

Epoch 00012: val_loss did not improve from 0.21690
Epoch 13/100
158947/158947 [==============================] - 131s 822us/step - loss: 0.1308 - val_loss: 0.2559
[last] - val_bacc: 0.7974 - val_f1: 0.6971 - val_precision: 0.7911 - val_recall: 0.6231 
[majority] - val_bacc: 0.7901 - val_f1: 0.6898 - val_precision: 0.8004 - val_recall: 0.6060 

Epoch 00013: val_loss did not improve from 0.21690
Epoch 00013: early stopping
	Min train loss: 0.130847728644489 @epoch 12
	Min valid loss: 0.21689659586245671 @epoch 2

<Figure size 1000x600 with 1 Axes>

INFO:
	4fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 157912 samples, validate on 27839 samples
Epoch 1/100
157912/157912 [==============================] - 132s 833us/step - loss: 0.2427 - val_loss: 0.1861
[last] - val_bacc: 0.7778 - val_f1: 0.6489 - val_precision: 0.7372 - val_recall: 0.5795 
[majority] - val_bacc: 0.7662 - val_f1: 0.6367 - val_precision: 0.7485 - val_recall: 0.5539 

Epoch 00001: val_loss improved from inf to 0.18608, saving model to ./checkpoints/4fold_BN2_32-32u_32ws_12f/m_0001_0.2427_0.1861.hdf5
Epoch 2/100
157912/157912 [==============================] - 130s 821us/step - loss: 0.1823 - val_loss: 0.1789
[last] - val_bacc: 0.7938 - val_f1: 0.6743 - val_precision: 0.7527 - val_recall: 0.6107 
[majority] - val_bacc: 0.7801 - val_f1: 0.6610 - val_precision: 0.7673 - val_recall: 0.5806 

Epoch 00002: val_loss improved from 0.18608 to 0.17892, saving model to ./checkpoints/4fold_BN2_32-32u_32ws_12f/m_0002_0.1823_0.1789.hdf5
Epoch 3/100
157912/157912 [==============================] - 130s 826us/step - loss: 0.1716 - val_loss: 0.1761
[last] - val_bacc: 0.8037 - val_f1: 0.6815 - val_precision: 0.7371 - val_recall: 0.6336 
[majority] - val_bacc: 0.7857 - val_f1: 0.6661 - val_precision: 0.7598 - val_recall: 0.5931 

Epoch 00003: val_loss improved from 0.17892 to 0.17607, saving model to ./checkpoints/4fold_BN2_32-32u_32ws_12f/m_0003_0.1716_0.1761.hdf5
Epoch 4/100
157912/157912 [==============================] - 130s 822us/step - loss: 0.1630 - val_loss: 0.1793
[last] - val_bacc: 0.8134 - val_f1: 0.6916 - val_precision: 0.7332 - val_recall: 0.6544 
[majority] - val_bacc: 0.7909 - val_f1: 0.6682 - val_precision: 0.7448 - val_recall: 0.6059 

Epoch 00004: val_loss did not improve from 0.17607
Epoch 5/100
157912/157912 [==============================] - 129s 814us/step - loss: 0.1565 - val_loss: 0.1822
[last] - val_bacc: 0.7943 - val_f1: 0.6780 - val_precision: 0.7618 - val_recall: 0.6107 
[majority] - val_bacc: 0.7737 - val_f1: 0.6531 - val_precision: 0.7699 - val_recall: 0.5671 

Epoch 00005: val_loss did not improve from 0.17607
Epoch 6/100
157912/157912 [==============================] - 129s 818us/step - loss: 0.1510 - val_loss: 0.1914
[last] - val_bacc: 0.7888 - val_f1: 0.6662 - val_precision: 0.7471 - val_recall: 0.6010 
[majority] - val_bacc: 0.7718 - val_f1: 0.6485 - val_precision: 0.7628 - val_recall: 0.5640 

Epoch 00006: val_loss did not improve from 0.17607
Epoch 7/100
157912/157912 [==============================] - 131s 827us/step - loss: 0.1454 - val_loss: 0.1961
[last] - val_bacc: 0.8066 - val_f1: 0.6701 - val_precision: 0.6964 - val_recall: 0.6458 
[majority] - val_bacc: 0.7874 - val_f1: 0.6587 - val_precision: 0.7291 - val_recall: 0.6007 

Epoch 00007: val_loss did not improve from 0.17607
Epoch 8/100
157912/157912 [==============================] - 131s 830us/step - loss: 0.1418 - val_loss: 0.1939
[last] - val_bacc: 0.8065 - val_f1: 0.6779 - val_precision: 0.7177 - val_recall: 0.6423 
[majority] - val_bacc: 0.7894 - val_f1: 0.6637 - val_precision: 0.7369 - val_recall: 0.6038 

Epoch 00008: val_loss did not improve from 0.17607
Epoch 9/100
157912/157912 [==============================] - 128s 813us/step - loss: 0.1375 - val_loss: 0.2012
[last] - val_bacc: 0.7893 - val_f1: 0.6498 - val_precision: 0.6960 - val_recall: 0.6094 
[majority] - val_bacc: 0.7725 - val_f1: 0.6380 - val_precision: 0.7240 - val_recall: 0.5702 

Epoch 00009: val_loss did not improve from 0.17607
Epoch 10/100
157912/157912 [==============================] - 129s 816us/step - loss: 0.1343 - val_loss: 0.2048
[last] - val_bacc: 0.8253 - val_f1: 0.6808 - val_precision: 0.6724 - val_recall: 0.6894 
[majority] - val_bacc: 0.8059 - val_f1: 0.6710 - val_precision: 0.7008 - val_recall: 0.6437 

Epoch 00010: val_loss did not improve from 0.17607
Epoch 11/100
157912/157912 [==============================] - 131s 828us/step - loss: 0.1317 - val_loss: 0.2047
[last] - val_bacc: 0.7940 - val_f1: 0.6640 - val_precision: 0.7208 - val_recall: 0.6156 
[majority] - val_bacc: 0.7808 - val_f1: 0.6541 - val_precision: 0.7417 - val_recall: 0.5851 

Epoch 00011: val_loss did not improve from 0.17607
Epoch 12/100
157912/157912 [==============================] - 130s 825us/step - loss: 0.1294 - val_loss: 0.2184
[last] - val_bacc: 0.8061 - val_f1: 0.6577 - val_precision: 0.6656 - val_recall: 0.6499 
[majority] - val_bacc: 0.7959 - val_f1: 0.6592 - val_precision: 0.7005 - val_recall: 0.6225 

Epoch 00012: val_loss did not improve from 0.17607
Epoch 13/100
157912/157912 [==============================] - 131s 829us/step - loss: 0.1261 - val_loss: 0.2160
[last] - val_bacc: 0.8180 - val_f1: 0.6649 - val_precision: 0.6525 - val_recall: 0.6776 
[majority] - val_bacc: 0.8100 - val_f1: 0.6707 - val_precision: 0.6878 - val_recall: 0.6544 

Epoch 00013: val_loss did not improve from 0.17607
Epoch 00013: early stopping
	Min train loss: 0.12607656438225423 @epoch 12
	Min valid loss: 0.17606622204684333 @epoch 2

<Figure size 1000x600 with 1 Axes>

INFO:
	5fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 157649 samples, validate on 27793 samples
Epoch 1/100
157649/157649 [==============================] - 133s 841us/step - loss: 0.2417 - val_loss: 0.2384
[last] - val_bacc: 0.7458 - val_f1: 0.6199 - val_precision: 0.7799 - val_recall: 0.5144 
[majority] - val_bacc: 0.7449 - val_f1: 0.6217 - val_precision: 0.7943 - val_recall: 0.5107 

Epoch 00001: val_loss improved from inf to 0.23843, saving model to ./checkpoints/5fold_BN2_32-32u_32ws_12f/m_0001_0.2417_0.2384.hdf5
Epoch 2/100
157649/157649 [==============================] - 130s 824us/step - loss: 0.1829 - val_loss: 0.2361
[last] - val_bacc: 0.7493 - val_f1: 0.6251 - val_precision: 0.7792 - val_recall: 0.5218 
[majority] - val_bacc: 0.7515 - val_f1: 0.6320 - val_precision: 0.7957 - val_recall: 0.5242 

Epoch 00002: val_loss improved from 0.23843 to 0.23607, saving model to ./checkpoints/5fold_BN2_32-32u_32ws_12f/m_0002_0.1829_0.2361.hdf5
Epoch 3/100
157649/157649 [==============================] - 130s 827us/step - loss: 0.1700 - val_loss: 0.2409
[last] - val_bacc: 0.7850 - val_f1: 0.6601 - val_precision: 0.7246 - val_recall: 0.6062 
[majority] - val_bacc: 0.7742 - val_f1: 0.6526 - val_precision: 0.7472 - val_recall: 0.5792 

Epoch 00003: val_loss did not improve from 0.23607
Epoch 4/100
157649/157649 [==============================] - 131s 829us/step - loss: 0.1603 - val_loss: 0.2482
[last] - val_bacc: 0.7972 - val_f1: 0.6731 - val_precision: 0.7178 - val_recall: 0.6337 
[majority] - val_bacc: 0.7826 - val_f1: 0.6624 - val_precision: 0.7423 - val_recall: 0.5980 

Epoch 00004: val_loss did not improve from 0.23607
Epoch 5/100
157649/157649 [==============================] - 131s 832us/step - loss: 0.1525 - val_loss: 0.2653
[last] - val_bacc: 0.7402 - val_f1: 0.6095 - val_precision: 0.7713 - val_recall: 0.5038 
[majority] - val_bacc: 0.7399 - val_f1: 0.6126 - val_precision: 0.7885 - val_recall: 0.5009 

Epoch 00005: val_loss did not improve from 0.23607
Epoch 6/100
157649/157649 [==============================] - 130s 828us/step - loss: 0.1470 - val_loss: 0.2693
[last] - val_bacc: 0.7680 - val_f1: 0.6453 - val_precision: 0.7515 - val_recall: 0.5655 
[majority] - val_bacc: 0.7597 - val_f1: 0.6367 - val_precision: 0.7639 - val_recall: 0.5459 

Epoch 00006: val_loss did not improve from 0.23607
Epoch 7/100
157649/157649 [==============================] - 129s 818us/step - loss: 0.1417 - val_loss: 0.2688
[last] - val_bacc: 0.7698 - val_f1: 0.6487 - val_precision: 0.7549 - val_recall: 0.5686 
[majority] - val_bacc: 0.7635 - val_f1: 0.6423 - val_precision: 0.7645 - val_recall: 0.5538 

Epoch 00007: val_loss did not improve from 0.23607
Epoch 8/100
157649/157649 [==============================] - 131s 831us/step - loss: 0.1378 - val_loss: 0.2821
[last] - val_bacc: 0.7651 - val_f1: 0.6339 - val_precision: 0.7233 - val_recall: 0.5641 
[majority] - val_bacc: 0.7552 - val_f1: 0.6302 - val_precision: 0.7634 - val_recall: 0.5366 

Epoch 00008: val_loss did not improve from 0.23607
Epoch 9/100
157649/157649 [==============================] - 130s 828us/step - loss: 0.1339 - val_loss: 0.2883
[last] - val_bacc: 0.7583 - val_f1: 0.6339 - val_precision: 0.7603 - val_recall: 0.5435 
[majority] - val_bacc: 0.7500 - val_f1: 0.6239 - val_precision: 0.7693 - val_recall: 0.5247 

Epoch 00009: val_loss did not improve from 0.23607
Epoch 10/100
157649/157649 [==============================] - 128s 809us/step - loss: 0.1308 - val_loss: 0.2892
[last] - val_bacc: 0.7644 - val_f1: 0.6325 - val_precision: 0.7215 - val_recall: 0.5631 
[majority] - val_bacc: 0.7585 - val_f1: 0.6320 - val_precision: 0.7515 - val_recall: 0.5454 

Epoch 00010: val_loss did not improve from 0.23607
Epoch 11/100
157649/157649 [==============================] - 130s 827us/step - loss: 0.1281 - val_loss: 0.2950
[last] - val_bacc: 0.7654 - val_f1: 0.6331 - val_precision: 0.7188 - val_recall: 0.5657 
[majority] - val_bacc: 0.7631 - val_f1: 0.6368 - val_precision: 0.7447 - val_recall: 0.5562 

Epoch 00011: val_loss did not improve from 0.23607
Epoch 12/100
157649/157649 [==============================] - 131s 830us/step - loss: 0.1254 - val_loss: 0.2954
[last] - val_bacc: 0.7715 - val_f1: 0.6349 - val_precision: 0.6970 - val_recall: 0.5829 
[majority] - val_bacc: 0.7645 - val_f1: 0.6341 - val_precision: 0.7268 - val_recall: 0.5623 

Epoch 00012: val_loss did not improve from 0.23607
Epoch 00012: early stopping
	Min train loss: 0.12538120215242182 @epoch 11
	Min valid loss: 0.23607241696913103 @epoch 1

<Figure size 1000x600 with 1 Axes>

INFO:
	6fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 160904 samples, validate on 28367 samples
Epoch 1/100
160904/160904 [==============================] - 135s 842us/step - loss: 0.2474 - val_loss: 0.2070
[last] - val_bacc: 0.7835 - val_f1: 0.6718 - val_precision: 0.7753 - val_recall: 0.5926 
[majority] - val_bacc: 0.7800 - val_f1: 0.6706 - val_precision: 0.7884 - val_recall: 0.5834 

Epoch 00001: val_loss improved from inf to 0.20698, saving model to ./checkpoints/6fold_BN2_32-32u_32ws_12f/m_0001_0.2474_0.2070.hdf5
Epoch 2/100
160904/160904 [==============================] - 131s 814us/step - loss: 0.1890 - val_loss: 0.2039
[last] - val_bacc: 0.7945 - val_f1: 0.6779 - val_precision: 0.7470 - val_recall: 0.6205 
[majority] - val_bacc: 0.7863 - val_f1: 0.6783 - val_precision: 0.7852 - val_recall: 0.5970 

Epoch 00002: val_loss improved from 0.20698 to 0.20385, saving model to ./checkpoints/6fold_BN2_32-32u_32ws_12f/m_0002_0.1890_0.2039.hdf5
Epoch 3/100
160904/160904 [==============================] - 133s 824us/step - loss: 0.1770 - val_loss: 0.1993
[last] - val_bacc: 0.7968 - val_f1: 0.6883 - val_precision: 0.7718 - val_recall: 0.6211 
[majority] - val_bacc: 0.7913 - val_f1: 0.6871 - val_precision: 0.7931 - val_recall: 0.6062 

Epoch 00003: val_loss improved from 0.20385 to 0.19929, saving model to ./checkpoints/6fold_BN2_32-32u_32ws_12f/m_0003_0.1770_0.1993.hdf5
Epoch 4/100
160904/160904 [==============================] - 134s 834us/step - loss: 0.1689 - val_loss: 0.2058
[last] - val_bacc: 0.8025 - val_f1: 0.6940 - val_precision: 0.7667 - val_recall: 0.6338 
[majority] - val_bacc: 0.7966 - val_f1: 0.6975 - val_precision: 0.8043 - val_recall: 0.6157 

Epoch 00004: val_loss did not improve from 0.19929
Epoch 5/100
160904/160904 [==============================] - 134s 836us/step - loss: 0.1617 - val_loss: 0.2039
[last] - val_bacc: 0.8239 - val_f1: 0.7168 - val_precision: 0.7573 - val_recall: 0.6804 
[majority] - val_bacc: 0.8112 - val_f1: 0.7096 - val_precision: 0.7819 - val_recall: 0.6495 

Epoch 00005: val_loss did not improve from 0.19929
Epoch 6/100
160904/160904 [==============================] - 135s 837us/step - loss: 0.1554 - val_loss: 0.2106
[last] - val_bacc: 0.7833 - val_f1: 0.6688 - val_precision: 0.7656 - val_recall: 0.5937 
[majority] - val_bacc: 0.7841 - val_f1: 0.6789 - val_precision: 0.7985 - val_recall: 0.5905 

Epoch 00006: val_loss did not improve from 0.19929
Epoch 7/100
160904/160904 [==============================] - 134s 833us/step - loss: 0.1502 - val_loss: 0.2163
[last] - val_bacc: 0.7762 - val_f1: 0.6588 - val_precision: 0.7634 - val_recall: 0.5794 
[majority] - val_bacc: 0.7762 - val_f1: 0.6689 - val_precision: 0.8020 - val_recall: 0.5737 

Epoch 00007: val_loss did not improve from 0.19929
Epoch 8/100
160904/160904 [==============================] - 133s 827us/step - loss: 0.1462 - val_loss: 0.2193
[last] - val_bacc: 0.8103 - val_f1: 0.7000 - val_precision: 0.7553 - val_recall: 0.6522 
[majority] - val_bacc: 0.8076 - val_f1: 0.7069 - val_precision: 0.7877 - val_recall: 0.6411 

Epoch 00008: val_loss did not improve from 0.19929
Epoch 9/100
160904/160904 [==============================] - 131s 816us/step - loss: 0.1427 - val_loss: 0.2136
[last] - val_bacc: 0.8023 - val_f1: 0.6906 - val_precision: 0.7567 - val_recall: 0.6352 
[majority] - val_bacc: 0.7942 - val_f1: 0.6904 - val_precision: 0.7906 - val_recall: 0.6127 

Epoch 00009: val_loss did not improve from 0.19929
Epoch 10/100
160904/160904 [==============================] - 132s 817us/step - loss: 0.1396 - val_loss: 0.2132
[last] - val_bacc: 0.8363 - val_f1: 0.7193 - val_precision: 0.7259 - val_recall: 0.7129 
[majority] - val_bacc: 0.8341 - val_f1: 0.7334 - val_precision: 0.7710 - val_recall: 0.6993 

Epoch 00010: val_loss did not improve from 0.19929
Epoch 11/100
160904/160904 [==============================] - 134s 830us/step - loss: 0.1360 - val_loss: 0.2146
[last] - val_bacc: 0.8143 - val_f1: 0.7008 - val_precision: 0.7435 - val_recall: 0.6628 
[majority] - val_bacc: 0.8131 - val_f1: 0.7098 - val_precision: 0.7752 - val_recall: 0.6547 

Epoch 00011: val_loss did not improve from 0.19929
Epoch 12/100
160904/160904 [==============================] - 131s 816us/step - loss: 0.1339 - val_loss: 0.2234
[last] - val_bacc: 0.7950 - val_f1: 0.6828 - val_precision: 0.7610 - val_recall: 0.6192 
[majority] - val_bacc: 0.7956 - val_f1: 0.6947 - val_precision: 0.7994 - val_recall: 0.6143 

Epoch 00012: val_loss did not improve from 0.19929
Epoch 13/100
160904/160904 [==============================] - 133s 827us/step - loss: 0.1313 - val_loss: 0.2206
[last] - val_bacc: 0.8086 - val_f1: 0.6928 - val_precision: 0.7398 - val_recall: 0.6514 
[majority] - val_bacc: 0.8056 - val_f1: 0.7036 - val_precision: 0.7854 - val_recall: 0.6373 

Epoch 00013: val_loss did not improve from 0.19929
Epoch 00013: early stopping
	Min train loss: 0.13128053821172875 @epoch 12
	Min valid loss: 0.19929071084055192 @epoch 2

<Figure size 1000x600 with 1 Axes>

INFO:
	7fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 157915 samples, validate on 27840 samples
Epoch 1/100
157915/157915 [==============================] - 134s 851us/step - loss: 0.2485 - val_loss: 0.2184
[last] - val_bacc: 0.7587 - val_f1: 0.6411 - val_precision: 0.7898 - val_recall: 0.5395 
[majority] - val_bacc: 0.7552 - val_f1: 0.6423 - val_precision: 0.8186 - val_recall: 0.5285 

Epoch 00001: val_loss improved from inf to 0.21837, saving model to ./checkpoints/7fold_BN2_32-32u_32ws_12f/m_0001_0.2485_0.2184.hdf5
Epoch 2/100
157915/157915 [==============================] - 129s 820us/step - loss: 0.1876 - val_loss: 0.2176
[last] - val_bacc: 0.7750 - val_f1: 0.6584 - val_precision: 0.7663 - val_recall: 0.5771 
[majority] - val_bacc: 0.7657 - val_f1: 0.6532 - val_precision: 0.7976 - val_recall: 0.5530 

Epoch 00002: val_loss improved from 0.21837 to 0.21764, saving model to ./checkpoints/7fold_BN2_32-32u_32ws_12f/m_0002_0.1876_0.2176.hdf5
Epoch 3/100
157915/157915 [==============================] - 130s 821us/step - loss: 0.1771 - val_loss: 0.2060
[last] - val_bacc: 0.8223 - val_f1: 0.6961 - val_precision: 0.7030 - val_recall: 0.6893 
[majority] - val_bacc: 0.8151 - val_f1: 0.7056 - val_precision: 0.7535 - val_recall: 0.6634 

Epoch 00003: val_loss improved from 0.21764 to 0.20605, saving model to ./checkpoints/7fold_BN2_32-32u_32ws_12f/m_0003_0.1771_0.2060.hdf5
Epoch 4/100
157915/157915 [==============================] - 132s 836us/step - loss: 0.1680 - val_loss: 0.2136
[last] - val_bacc: 0.7934 - val_f1: 0.6784 - val_precision: 0.7518 - val_recall: 0.6181 
[majority] - val_bacc: 0.7840 - val_f1: 0.6739 - val_precision: 0.7790 - val_recall: 0.5938 

Epoch 00004: val_loss did not improve from 0.20605
Epoch 5/100
157915/157915 [==============================] - 128s 812us/step - loss: 0.1607 - val_loss: 0.2145
[last] - val_bacc: 0.8286 - val_f1: 0.7066 - val_precision: 0.7126 - val_recall: 0.7007 
[majority] - val_bacc: 0.8187 - val_f1: 0.7111 - val_precision: 0.7571 - val_recall: 0.6704 

Epoch 00005: val_loss did not improve from 0.20605
Epoch 6/100
157915/157915 [==============================] - 129s 817us/step - loss: 0.1558 - val_loss: 0.2140
[last] - val_bacc: 0.8213 - val_f1: 0.7059 - val_precision: 0.7331 - val_recall: 0.6807 
[majority] - val_bacc: 0.8129 - val_f1: 0.7091 - val_precision: 0.7721 - val_recall: 0.6556 

Epoch 00006: val_loss did not improve from 0.20605
Epoch 7/100
157915/157915 [==============================] - 130s 824us/step - loss: 0.1503 - val_loss: 0.2250
[last] - val_bacc: 0.8063 - val_f1: 0.6831 - val_precision: 0.7171 - val_recall: 0.6521 
[majority] - val_bacc: 0.7965 - val_f1: 0.6879 - val_precision: 0.7700 - val_recall: 0.6216 

Epoch 00007: val_loss did not improve from 0.20605
Epoch 8/100
157915/157915 [==============================] - 129s 817us/step - loss: 0.1469 - val_loss: 0.2183
[last] - val_bacc: 0.8059 - val_f1: 0.6947 - val_precision: 0.7541 - val_recall: 0.6440 
[majority] - val_bacc: 0.7995 - val_f1: 0.6945 - val_precision: 0.7796 - val_recall: 0.6262 

Epoch 00008: val_loss did not improve from 0.20605
Epoch 9/100
157915/157915 [==============================] - 131s 829us/step - loss: 0.1432 - val_loss: 0.2253
[last] - val_bacc: 0.8175 - val_f1: 0.6980 - val_precision: 0.7230 - val_recall: 0.6748 
[majority] - val_bacc: 0.8121 - val_f1: 0.7088 - val_precision: 0.7745 - val_recall: 0.6534 

Epoch 00009: val_loss did not improve from 0.20605
Epoch 10/100
157915/157915 [==============================] - 129s 818us/step - loss: 0.1393 - val_loss: 0.2339
[last] - val_bacc: 0.7870 - val_f1: 0.6676 - val_precision: 0.7427 - val_recall: 0.6062 
[majority] - val_bacc: 0.7801 - val_f1: 0.6679 - val_precision: 0.7760 - val_recall: 0.5862 

Epoch 00010: val_loss did not improve from 0.20605
Epoch 11/100
157915/157915 [==============================] - 130s 824us/step - loss: 0.1366 - val_loss: 0.2286
[last] - val_bacc: 0.8051 - val_f1: 0.6933 - val_precision: 0.7525 - val_recall: 0.6426 
[majority] - val_bacc: 0.8009 - val_f1: 0.6938 - val_precision: 0.7712 - val_recall: 0.6305 

Epoch 00011: val_loss did not improve from 0.20605
Epoch 12/100
157915/157915 [==============================] - 131s 831us/step - loss: 0.1340 - val_loss: 0.2398
[last] - val_bacc: 0.7861 - val_f1: 0.6721 - val_precision: 0.7621 - val_recall: 0.6011 
[majority] - val_bacc: 0.7807 - val_f1: 0.6726 - val_precision: 0.7907 - val_recall: 0.5852 

Epoch 00012: val_loss did not improve from 0.20605
Epoch 13/100
157915/157915 [==============================] - 129s 819us/step - loss: 0.1319 - val_loss: 0.2413
[last] - val_bacc: 0.7944 - val_f1: 0.6776 - val_precision: 0.7446 - val_recall: 0.6216 
[majority] - val_bacc: 0.7939 - val_f1: 0.6863 - val_precision: 0.7762 - val_recall: 0.6151 

Epoch 00013: val_loss did not improve from 0.20605
Epoch 00013: early stopping
	Min train loss: 0.13193437662048163 @epoch 12
	Min valid loss: 0.2060485394393233 @epoch 2

<Figure size 1000x600 with 1 Axes>

INFO:
	8fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 158196 samples, validate on 27889 samples
Epoch 1/100
158196/158196 [==============================] - 132s 835us/step - loss: 0.2466 - val_loss: 0.2374
[last] - val_bacc: 0.7977 - val_f1: 0.6902 - val_precision: 0.7622 - val_recall: 0.6307 
[majority] - val_bacc: 0.7942 - val_f1: 0.6927 - val_precision: 0.7873 - val_recall: 0.6184 

Epoch 00001: val_loss improved from inf to 0.23741, saving model to ./checkpoints/8fold_BN2_32-32u_32ws_12f/m_0001_0.2466_0.2374.hdf5
Epoch 2/100
158196/158196 [==============================] - 130s 819us/step - loss: 0.1850 - val_loss: 0.2299
[last] - val_bacc: 0.8196 - val_f1: 0.7093 - val_precision: 0.7382 - val_recall: 0.6825 
[majority] - val_bacc: 0.8109 - val_f1: 0.7099 - val_precision: 0.7730 - val_recall: 0.6564 

Epoch 00002: val_loss improved from 0.23741 to 0.22988, saving model to ./checkpoints/8fold_BN2_32-32u_32ws_12f/m_0002_0.1850_0.2299.hdf5
Epoch 3/100
158196/158196 [==============================] - 130s 820us/step - loss: 0.1737 - val_loss: 0.2246
[last] - val_bacc: 0.8148 - val_f1: 0.7109 - val_precision: 0.7608 - val_recall: 0.6672 
[majority] - val_bacc: 0.8092 - val_f1: 0.7137 - val_precision: 0.7929 - val_recall: 0.6488 

Epoch 00003: val_loss improved from 0.22988 to 0.22456, saving model to ./checkpoints/8fold_BN2_32-32u_32ws_12f/m_0003_0.1737_0.2246.hdf5
Epoch 4/100
158196/158196 [==============================] - 132s 834us/step - loss: 0.1653 - val_loss: 0.2287
[last] - val_bacc: 0.8058 - val_f1: 0.7153 - val_precision: 0.8148 - val_recall: 0.6375 
[majority] - val_bacc: 0.7970 - val_f1: 0.7095 - val_precision: 0.8377 - val_recall: 0.6154 

Epoch 00004: val_loss did not improve from 0.22456
Epoch 5/100
158196/158196 [==============================] - 129s 817us/step - loss: 0.1579 - val_loss: 0.2459
[last] - val_bacc: 0.8058 - val_f1: 0.7068 - val_precision: 0.7840 - val_recall: 0.6434 
[majority] - val_bacc: 0.8039 - val_f1: 0.7101 - val_precision: 0.8044 - val_recall: 0.6356 

Epoch 00005: val_loss did not improve from 0.22456
Epoch 6/100
158196/158196 [==============================] - 129s 816us/step - loss: 0.1531 - val_loss: 0.2392
[last] - val_bacc: 0.8094 - val_f1: 0.7130 - val_precision: 0.7897 - val_recall: 0.6498 
[majority] - val_bacc: 0.8084 - val_f1: 0.7199 - val_precision: 0.8191 - val_recall: 0.6422 

Epoch 00006: val_loss did not improve from 0.22456
Epoch 7/100
158196/158196 [==============================] - 130s 825us/step - loss: 0.1483 - val_loss: 0.2428
[last] - val_bacc: 0.7977 - val_f1: 0.7022 - val_precision: 0.8054 - val_recall: 0.6224 
[majority] - val_bacc: 0.7913 - val_f1: 0.6993 - val_precision: 0.8281 - val_recall: 0.6052 

Epoch 00007: val_loss did not improve from 0.22456
Epoch 8/100
158196/158196 [==============================] - 129s 818us/step - loss: 0.1449 - val_loss: 0.2432
[last] - val_bacc: 0.8179 - val_f1: 0.7197 - val_precision: 0.7770 - val_recall: 0.6703 
[majority] - val_bacc: 0.8068 - val_f1: 0.7118 - val_precision: 0.7972 - val_recall: 0.6429 

Epoch 00008: val_loss did not improve from 0.22456
Epoch 9/100
158196/158196 [==============================] - 129s 813us/step - loss: 0.1411 - val_loss: 0.2522
[last] - val_bacc: 0.7969 - val_f1: 0.7027 - val_precision: 0.8114 - val_recall: 0.6196 
[majority] - val_bacc: 0.7900 - val_f1: 0.6983 - val_precision: 0.8314 - val_recall: 0.6019 

Epoch 00009: val_loss did not improve from 0.22456
Epoch 10/100
158196/158196 [==============================] - 129s 816us/step - loss: 0.1376 - val_loss: 0.2571
[last] - val_bacc: 0.7990 - val_f1: 0.7081 - val_precision: 0.8214 - val_recall: 0.6222 
[majority] - val_bacc: 0.7908 - val_f1: 0.7016 - val_precision: 0.8405 - val_recall: 0.6022 

Epoch 00010: val_loss did not improve from 0.22456
Epoch 11/100
158196/158196 [==============================] - 128s 810us/step - loss: 0.1347 - val_loss: 0.2577
[last] - val_bacc: 0.7862 - val_f1: 0.6950 - val_precision: 0.8400 - val_recall: 0.5927 
[majority] - val_bacc: 0.7815 - val_f1: 0.6895 - val_precision: 0.8453 - val_recall: 0.5821 

Epoch 00011: val_loss did not improve from 0.22456
Epoch 12/100
158196/158196 [==============================] - 130s 824us/step - loss: 0.1322 - val_loss: 0.2563
[last] - val_bacc: 0.8117 - val_f1: 0.7206 - val_precision: 0.8059 - val_recall: 0.6517 
[majority] - val_bacc: 0.8021 - val_f1: 0.7146 - val_precision: 0.8303 - val_recall: 0.6272 

Epoch 00012: val_loss did not improve from 0.22456
Epoch 13/100
158196/158196 [==============================] - 131s 827us/step - loss: 0.1307 - val_loss: 0.2548
[last] - val_bacc: 0.8044 - val_f1: 0.7104 - val_precision: 0.8032 - val_recall: 0.6368 
[majority] - val_bacc: 0.8003 - val_f1: 0.7121 - val_precision: 0.8302 - val_recall: 0.6234 

Epoch 00013: val_loss did not improve from 0.22456
Epoch 00013: early stopping
	Min train loss: 0.13066955246030343 @epoch 12
	Min valid loss: 0.22456198915586204 @epoch 2

<Figure size 1000x600 with 1 Axes>

INFO:
	9fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 157298 samples, validate on 27731 samples
Epoch 1/100
157298/157298 [==============================] - 133s 845us/step - loss: 0.2424 - val_loss: 0.2105
[last] - val_bacc: 0.7984 - val_f1: 0.6512 - val_precision: 0.6625 - val_recall: 0.6402 
[majority] - val_bacc: 0.7939 - val_f1: 0.6585 - val_precision: 0.6970 - val_recall: 0.6240 

Epoch 00001: val_loss improved from inf to 0.21051, saving model to ./checkpoints/9fold_BN2_32-32u_32ws_12f/m_0001_0.2424_0.2105.hdf5
Epoch 2/100
157298/157298 [==============================] - 129s 823us/step - loss: 0.1837 - val_loss: 0.2118
[last] - val_bacc: 0.7995 - val_f1: 0.6569 - val_precision: 0.6745 - val_recall: 0.6402 
[majority] - val_bacc: 0.7919 - val_f1: 0.6567 - val_precision: 0.6988 - val_recall: 0.6194 

Epoch 00002: val_loss did not improve from 0.21051
Epoch 3/100
157298/157298 [==============================] - 129s 820us/step - loss: 0.1718 - val_loss: 0.2210
[last] - val_bacc: 0.7629 - val_f1: 0.6204 - val_precision: 0.6986 - val_recall: 0.5579 
[majority] - val_bacc: 0.7568 - val_f1: 0.6212 - val_precision: 0.7310 - val_recall: 0.5401 

Epoch 00003: val_loss did not improve from 0.21051
Epoch 4/100
157298/157298 [==============================] - 129s 820us/step - loss: 0.1635 - val_loss: 0.2284
[last] - val_bacc: 0.7642 - val_f1: 0.6124 - val_precision: 0.6668 - val_recall: 0.5661 
[majority] - val_bacc: 0.7571 - val_f1: 0.6122 - val_precision: 0.6968 - val_recall: 0.5459 

Epoch 00004: val_loss did not improve from 0.21051
Epoch 5/100
157298/157298 [==============================] - 130s 828us/step - loss: 0.1561 - val_loss: 0.2310
[last] - val_bacc: 0.7851 - val_f1: 0.6284 - val_precision: 0.6409 - val_recall: 0.6164 
[majority] - val_bacc: 0.7795 - val_f1: 0.6333 - val_precision: 0.6733 - val_recall: 0.5977 

Epoch 00005: val_loss did not improve from 0.21051
Epoch 6/100
157298/157298 [==============================] - 127s 804us/step - loss: 0.1507 - val_loss: 0.2280
[last] - val_bacc: 0.7931 - val_f1: 0.6399 - val_precision: 0.6480 - val_recall: 0.6320 
[majority] - val_bacc: 0.7843 - val_f1: 0.6378 - val_precision: 0.6698 - val_recall: 0.6087 

Epoch 00006: val_loss did not improve from 0.21051
Epoch 7/100
157298/157298 [==============================] - 129s 822us/step - loss: 0.1458 - val_loss: 0.2447
[last] - val_bacc: 0.7736 - val_f1: 0.6125 - val_precision: 0.6332 - val_recall: 0.5931 
[majority] - val_bacc: 0.7695 - val_f1: 0.6222 - val_precision: 0.6770 - val_recall: 0.5756 

Epoch 00007: val_loss did not improve from 0.21051
Epoch 8/100
157298/157298 [==============================] - 132s 839us/step - loss: 0.1414 - val_loss: 0.2496
[last] - val_bacc: 0.7859 - val_f1: 0.6246 - val_precision: 0.6283 - val_recall: 0.6209 
[majority] - val_bacc: 0.7847 - val_f1: 0.6359 - val_precision: 0.6630 - val_recall: 0.6108 

Epoch 00008: val_loss did not improve from 0.21051
Epoch 9/100
157298/157298 [==============================] - 128s 813us/step - loss: 0.1384 - val_loss: 0.2517
[last] - val_bacc: 0.7554 - val_f1: 0.5956 - val_precision: 0.6486 - val_recall: 0.5505 
[majority] - val_bacc: 0.7560 - val_f1: 0.6075 - val_precision: 0.6858 - val_recall: 0.5453 

Epoch 00009: val_loss did not improve from 0.21051
Epoch 10/100
157298/157298 [==============================] - 129s 820us/step - loss: 0.1353 - val_loss: 0.2558
[last] - val_bacc: 0.7644 - val_f1: 0.6005 - val_precision: 0.6299 - val_recall: 0.5738 
[majority] - val_bacc: 0.7630 - val_f1: 0.6112 - val_precision: 0.6679 - val_recall: 0.5634 

Epoch 00010: val_loss did not improve from 0.21051
Epoch 11/100
157298/157298 [==============================] - 130s 827us/step - loss: 0.1322 - val_loss: 0.2620
[last] - val_bacc: 0.7622 - val_f1: 0.5961 - val_precision: 0.6246 - val_recall: 0.5701 
[majority] - val_bacc: 0.7629 - val_f1: 0.6128 - val_precision: 0.6735 - val_recall: 0.5622 

Epoch 00011: val_loss did not improve from 0.21051
Epoch 00011: early stopping
	Min train loss: 0.13220467302802302 @epoch 10
	Min valid loss: 0.21051115696635314 @epoch 0

<Figure size 1000x600 with 1 Axes>

INFO:
	10fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 159312 samples, validate on 28086 samples
Epoch 1/100
159312/159312 [==============================] - 135s 850us/step - loss: 0.2393 - val_loss: 0.2119
[last] - val_bacc: 0.8179 - val_f1: 0.7258 - val_precision: 0.7979 - val_recall: 0.6656 
[majority] - val_bacc: 0.8041 - val_f1: 0.7159 - val_precision: 0.8260 - val_recall: 0.6317 

Epoch 00001: val_loss improved from inf to 0.21190, saving model to ./checkpoints/10fold_BN2_32-32u_32ws_12f/m_0001_0.2393_0.2119.hdf5
Epoch 2/100
159312/159312 [==============================] - 132s 831us/step - loss: 0.1797 - val_loss: 0.2116
[last] - val_bacc: 0.8185 - val_f1: 0.7310 - val_precision: 0.8130 - val_recall: 0.6640 
[majority] - val_bacc: 0.8062 - val_f1: 0.7216 - val_precision: 0.8367 - val_recall: 0.6343 

Epoch 00002: val_loss improved from 0.21190 to 0.21159, saving model to ./checkpoints/10fold_BN2_32-32u_32ws_12f/m_0002_0.1797_0.2116.hdf5
Epoch 3/100
159312/159312 [==============================] - 131s 820us/step - loss: 0.1680 - val_loss: 0.2094
[last] - val_bacc: 0.8523 - val_f1: 0.7561 - val_precision: 0.7682 - val_recall: 0.7444 
[majority] - val_bacc: 0.8330 - val_f1: 0.7443 - val_precision: 0.7983 - val_recall: 0.6972 

Epoch 00003: val_loss improved from 0.21159 to 0.20939, saving model to ./checkpoints/10fold_BN2_32-32u_32ws_12f/m_0003_0.1680_0.2094.hdf5
Epoch 4/100
159312/159312 [==============================] - 130s 816us/step - loss: 0.1600 - val_loss: 0.2093
[last] - val_bacc: 0.8430 - val_f1: 0.7558 - val_precision: 0.7975 - val_recall: 0.7183 
[majority] - val_bacc: 0.8278 - val_f1: 0.7429 - val_precision: 0.8141 - val_recall: 0.6832 

Epoch 00004: val_loss improved from 0.20939 to 0.20934, saving model to ./checkpoints/10fold_BN2_32-32u_32ws_12f/m_0004_0.1600_0.2093.hdf5
Epoch 5/100
159312/159312 [==============================] - 132s 830us/step - loss: 0.1531 - val_loss: 0.2179
[last] - val_bacc: 0.8105 - val_f1: 0.7236 - val_precision: 0.8231 - val_recall: 0.6455 
[majority] - val_bacc: 0.7982 - val_f1: 0.7119 - val_precision: 0.8413 - val_recall: 0.6170 

Epoch 00005: val_loss did not improve from 0.20934
Epoch 6/100
159312/159312 [==============================] - 131s 824us/step - loss: 0.1477 - val_loss: 0.2212
[last] - val_bacc: 0.8038 - val_f1: 0.7138 - val_precision: 0.8196 - val_recall: 0.6322 
[majority] - val_bacc: 0.7956 - val_f1: 0.7070 - val_precision: 0.8359 - val_recall: 0.6125 

Epoch 00006: val_loss did not improve from 0.20934
Epoch 7/100
159312/159312 [==============================] - 133s 834us/step - loss: 0.1427 - val_loss: 0.2135
[last] - val_bacc: 0.8317 - val_f1: 0.7434 - val_precision: 0.8005 - val_recall: 0.6939 
[majority] - val_bacc: 0.8126 - val_f1: 0.7257 - val_precision: 0.8210 - val_recall: 0.6502 

Epoch 00007: val_loss did not improve from 0.20934
Epoch 8/100
159312/159312 [==============================] - 132s 829us/step - loss: 0.1386 - val_loss: 0.2156
[last] - val_bacc: 0.8462 - val_f1: 0.7548 - val_precision: 0.7836 - val_recall: 0.7280 
[majority] - val_bacc: 0.8309 - val_f1: 0.7442 - val_precision: 0.8059 - val_recall: 0.6912 

Epoch 00008: val_loss did not improve from 0.20934
Epoch 9/100
159312/159312 [==============================] - 133s 834us/step - loss: 0.1354 - val_loss: 0.2319
[last] - val_bacc: 0.8197 - val_f1: 0.7251 - val_precision: 0.7881 - val_recall: 0.6713 
[majority] - val_bacc: 0.8120 - val_f1: 0.7221 - val_precision: 0.8107 - val_recall: 0.6509 

Epoch 00009: val_loss did not improve from 0.20934
Epoch 10/100
159312/159312 [==============================] - 130s 818us/step - loss: 0.1322 - val_loss: 0.2354
[last] - val_bacc: 0.8230 - val_f1: 0.7292 - val_precision: 0.7885 - val_recall: 0.6782 
[majority] - val_bacc: 0.8091 - val_f1: 0.7189 - val_precision: 0.8126 - val_recall: 0.6445 

Epoch 00010: val_loss did not improve from 0.20934
Epoch 11/100
159312/159312 [==============================] - 132s 829us/step - loss: 0.1297 - val_loss: 0.2298
[last] - val_bacc: 0.8303 - val_f1: 0.7368 - val_precision: 0.7851 - val_recall: 0.6941 
[majority] - val_bacc: 0.8229 - val_f1: 0.7353 - val_precision: 0.8090 - val_recall: 0.6739 

Epoch 00011: val_loss did not improve from 0.20934
Epoch 12/100
159312/159312 [==============================] - 133s 832us/step - loss: 0.1269 - val_loss: 0.2321
[last] - val_bacc: 0.8277 - val_f1: 0.7287 - val_precision: 0.7692 - val_recall: 0.6922 
[majority] - val_bacc: 0.8193 - val_f1: 0.7257 - val_precision: 0.7919 - val_recall: 0.6697 

Epoch 00012: val_loss did not improve from 0.20934
Epoch 13/100
159312/159312 [==============================] - 132s 827us/step - loss: 0.1242 - val_loss: 0.2483
[last] - val_bacc: 0.8068 - val_f1: 0.7093 - val_precision: 0.7893 - val_recall: 0.6441 
[majority] - val_bacc: 0.8019 - val_f1: 0.7101 - val_precision: 0.8155 - val_recall: 0.6289 

Epoch 00013: val_loss did not improve from 0.20934
Epoch 14/100
159312/159312 [==============================] - 132s 829us/step - loss: 0.1228 - val_loss: 0.2477
[last] - val_bacc: 0.8045 - val_f1: 0.7084 - val_precision: 0.7963 - val_recall: 0.6379 
[majority] - val_bacc: 0.7980 - val_f1: 0.7066 - val_precision: 0.8220 - val_recall: 0.6196 

Epoch 00014: val_loss did not improve from 0.20934
Epoch 00014: early stopping
	Min train loss: 0.12275375798606508 @epoch 13
	Min valid loss: 0.209343100227844 @epoch 3

<Figure size 1000x600 with 1 Axes>

INFO:
	11fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 159684 samples, validate on 28152 samples
Epoch 1/100
159684/159684 [==============================] - 136s 853us/step - loss: 0.2366 - val_loss: 0.2082
[last] - val_bacc: 0.8020 - val_f1: 0.7126 - val_precision: 0.8283 - val_recall: 0.6253 
[majority] - val_bacc: 0.7918 - val_f1: 0.7008 - val_precision: 0.8366 - val_recall: 0.6029 

Epoch 00001: val_loss improved from inf to 0.20822, saving model to ./checkpoints/11fold_BN2_32-32u_32ws_12f/m_0001_0.2366_0.2082.hdf5
Epoch 2/100
159684/159684 [==============================] - 133s 833us/step - loss: 0.1798 - val_loss: 0.2044
[last] - val_bacc: 0.8209 - val_f1: 0.7295 - val_precision: 0.8022 - val_recall: 0.6689 
[majority] - val_bacc: 0.8118 - val_f1: 0.7212 - val_precision: 0.8130 - val_recall: 0.6480 

Epoch 00002: val_loss improved from 0.20822 to 0.20445, saving model to ./checkpoints/11fold_BN2_32-32u_32ws_12f/m_0002_0.1798_0.2044.hdf5
Epoch 3/100
159684/159684 [==============================] - 132s 829us/step - loss: 0.1687 - val_loss: 0.1983
[last] - val_bacc: 0.8411 - val_f1: 0.7544 - val_precision: 0.8040 - val_recall: 0.7105 
[majority] - val_bacc: 0.8252 - val_f1: 0.7370 - val_precision: 0.8094 - val_recall: 0.6765 

Epoch 00003: val_loss improved from 0.20445 to 0.19832, saving model to ./checkpoints/11fold_BN2_32-32u_32ws_12f/m_0003_0.1687_0.1983.hdf5
Epoch 4/100
159684/159684 [==============================] - 135s 844us/step - loss: 0.1597 - val_loss: 0.2030
[last] - val_bacc: 0.8246 - val_f1: 0.7410 - val_precision: 0.8252 - val_recall: 0.6725 
[majority] - val_bacc: 0.8137 - val_f1: 0.7305 - val_precision: 0.8371 - val_recall: 0.6480 

Epoch 00004: val_loss did not improve from 0.19832
Epoch 5/100
159684/159684 [==============================] - 132s 827us/step - loss: 0.1528 - val_loss: 0.2183
[last] - val_bacc: 0.7685 - val_f1: 0.6700 - val_precision: 0.8500 - val_recall: 0.5530 
[majority] - val_bacc: 0.7658 - val_f1: 0.6670 - val_precision: 0.8546 - val_recall: 0.5469 

Epoch 00005: val_loss did not improve from 0.19832
Epoch 6/100
159684/159684 [==============================] - 133s 831us/step - loss: 0.1463 - val_loss: 0.2112
[last] - val_bacc: 0.8329 - val_f1: 0.7396 - val_precision: 0.7885 - val_recall: 0.6964 
[majority] - val_bacc: 0.8198 - val_f1: 0.7320 - val_precision: 0.8150 - val_recall: 0.6644 

Epoch 00006: val_loss did not improve from 0.19832
Epoch 7/100
159684/159684 [==============================] - 132s 828us/step - loss: 0.1422 - val_loss: 0.2092
[last] - val_bacc: 0.8511 - val_f1: 0.7583 - val_precision: 0.7823 - val_recall: 0.7358 
[majority] - val_bacc: 0.8354 - val_f1: 0.7486 - val_precision: 0.8068 - val_recall: 0.6982 

Epoch 00007: val_loss did not improve from 0.19832
Epoch 8/100
159684/159684 [==============================] - 131s 819us/step - loss: 0.1371 - val_loss: 0.2123
[last] - val_bacc: 0.8483 - val_f1: 0.7536 - val_precision: 0.7780 - val_recall: 0.7307 
[majority] - val_bacc: 0.8315 - val_f1: 0.7436 - val_precision: 0.8062 - val_recall: 0.6901 

Epoch 00008: val_loss did not improve from 0.19832
Epoch 9/100
159684/159684 [==============================] - 133s 833us/step - loss: 0.1335 - val_loss: 0.2137
[last] - val_bacc: 0.8210 - val_f1: 0.7262 - val_precision: 0.7913 - val_recall: 0.6710 
[majority] - val_bacc: 0.8107 - val_f1: 0.7197 - val_precision: 0.8128 - val_recall: 0.6457 

Epoch 00009: val_loss did not improve from 0.19832
Epoch 10/100
159684/159684 [==============================] - 130s 816us/step - loss: 0.1299 - val_loss: 0.2128
[last] - val_bacc: 0.8408 - val_f1: 0.7447 - val_precision: 0.7766 - val_recall: 0.7153 
[majority] - val_bacc: 0.8284 - val_f1: 0.7423 - val_precision: 0.8134 - val_recall: 0.6826 

Epoch 00010: val_loss did not improve from 0.19832
Epoch 11/100
159684/159684 [==============================] - 132s 829us/step - loss: 0.1282 - val_loss: 0.2248
[last] - val_bacc: 0.8162 - val_f1: 0.7224 - val_precision: 0.7981 - val_recall: 0.6599 
[majority] - val_bacc: 0.8009 - val_f1: 0.7084 - val_precision: 0.8183 - val_recall: 0.6246 

Epoch 00011: val_loss did not improve from 0.19832
Epoch 12/100
159684/159684 [==============================] - 130s 816us/step - loss: 0.1249 - val_loss: 0.2302
[last] - val_bacc: 0.8162 - val_f1: 0.7116 - val_precision: 0.7636 - val_recall: 0.6662 
[majority] - val_bacc: 0.8087 - val_f1: 0.7121 - val_precision: 0.7951 - val_recall: 0.6447 

Epoch 00012: val_loss did not improve from 0.19832
Epoch 13/100
159684/159684 [==============================] - 131s 822us/step - loss: 0.1224 - val_loss: 0.2290
[last] - val_bacc: 0.8423 - val_f1: 0.7403 - val_precision: 0.7595 - val_recall: 0.7221 
[majority] - val_bacc: 0.8288 - val_f1: 0.7363 - val_precision: 0.7931 - val_recall: 0.6871 

Epoch 00013: val_loss did not improve from 0.19832
Epoch 00013: early stopping
	Min train loss: 0.12244386906869718 @epoch 12
	Min valid loss: 0.19831866463611889 @epoch 2

<Figure size 1000x600 with 1 Axes>

INFO:
	12fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 160858 samples, validate on 28359 samples
Epoch 1/100
160858/160858 [==============================] - 140s 870us/step - loss: 0.2467 - val_loss: 0.2101
[last] - val_bacc: 0.7656 - val_f1: 0.6532 - val_precision: 0.7992 - val_recall: 0.5524 
[majority] - val_bacc: 0.7610 - val_f1: 0.6513 - val_precision: 0.8203 - val_recall: 0.5401 

Epoch 00001: val_loss improved from inf to 0.21014, saving model to ./checkpoints/12fold_BN2_32-32u_32ws_12f/m_0001_0.2467_0.2101.hdf5
Epoch 2/100
160858/160858 [==============================] - 136s 842us/step - loss: 0.1894 - val_loss: 0.2041
[last] - val_bacc: 0.7835 - val_f1: 0.6755 - val_precision: 0.7877 - val_recall: 0.5913 
[majority] - val_bacc: 0.7782 - val_f1: 0.6721 - val_precision: 0.8030 - val_recall: 0.5779 

Epoch 00002: val_loss improved from 0.21014 to 0.20415, saving model to ./checkpoints/12fold_BN2_32-32u_32ws_12f/m_0002_0.1894_0.2041.hdf5
Epoch 3/100
160858/160858 [==============================] - 133s 829us/step - loss: 0.1778 - val_loss: 0.2020
[last] - val_bacc: 0.8211 - val_f1: 0.6955 - val_precision: 0.7054 - val_recall: 0.6859 
[majority] - val_bacc: 0.8179 - val_f1: 0.7120 - val_precision: 0.7628 - val_recall: 0.6675 

Epoch 00003: val_loss improved from 0.20415 to 0.20205, saving model to ./checkpoints/12fold_BN2_32-32u_32ws_12f/m_0003_0.1778_0.2020.hdf5
Epoch 4/100
160858/160858 [==============================] - 134s 833us/step - loss: 0.1689 - val_loss: 0.2059
[last] - val_bacc: 0.7832 - val_f1: 0.6770 - val_precision: 0.7946 - val_recall: 0.5897 
[majority] - val_bacc: 0.7805 - val_f1: 0.6784 - val_precision: 0.8147 - val_recall: 0.5811 

Epoch 00004: val_loss did not improve from 0.20205
Epoch 5/100
160858/160858 [==============================] - 133s 829us/step - loss: 0.1624 - val_loss: 0.2050
[last] - val_bacc: 0.8064 - val_f1: 0.7008 - val_precision: 0.7720 - val_recall: 0.6416 
[majority] - val_bacc: 0.7975 - val_f1: 0.6952 - val_precision: 0.7912 - val_recall: 0.6200 

Epoch 00005: val_loss did not improve from 0.20205
Epoch 6/100
160858/160858 [==============================] - 133s 826us/step - loss: 0.1575 - val_loss: 0.2089
[last] - val_bacc: 0.8175 - val_f1: 0.7062 - val_precision: 0.7470 - val_recall: 0.6696 
[majority] - val_bacc: 0.8119 - val_f1: 0.7124 - val_precision: 0.7870 - val_recall: 0.6507 

Epoch 00006: val_loss did not improve from 0.20205
Epoch 7/100
160858/160858 [==============================] - 134s 831us/step - loss: 0.1519 - val_loss: 0.2148
[last] - val_bacc: 0.7876 - val_f1: 0.6787 - val_precision: 0.7793 - val_recall: 0.6011 
[majority] - val_bacc: 0.7835 - val_f1: 0.6804 - val_precision: 0.8062 - val_recall: 0.5886 

Epoch 00007: val_loss did not improve from 0.20205
Epoch 8/100
160858/160858 [==============================] - 134s 830us/step - loss: 0.1481 - val_loss: 0.2222
[last] - val_bacc: 0.8065 - val_f1: 0.6889 - val_precision: 0.7343 - val_recall: 0.6488 
[majority] - val_bacc: 0.8055 - val_f1: 0.7029 - val_precision: 0.7822 - val_recall: 0.6382 

Epoch 00008: val_loss did not improve from 0.20205
Epoch 9/100
160858/160858 [==============================] - 133s 826us/step - loss: 0.1444 - val_loss: 0.2182
[last] - val_bacc: 0.7974 - val_f1: 0.6881 - val_precision: 0.7677 - val_recall: 0.6235 
[majority] - val_bacc: 0.7985 - val_f1: 0.6984 - val_precision: 0.7977 - val_recall: 0.6211 

Epoch 00009: val_loss did not improve from 0.20205
Epoch 10/100
160858/160858 [==============================] - 134s 831us/step - loss: 0.1404 - val_loss: 0.2211
[last] - val_bacc: 0.8070 - val_f1: 0.6954 - val_precision: 0.7525 - val_recall: 0.6464 
[majority] - val_bacc: 0.8055 - val_f1: 0.7059 - val_precision: 0.7921 - val_recall: 0.6366 

Epoch 00010: val_loss did not improve from 0.20205
Epoch 11/100
160858/160858 [==============================] - 133s 829us/step - loss: 0.1372 - val_loss: 0.2277
[last] - val_bacc: 0.7949 - val_f1: 0.6751 - val_precision: 0.7353 - val_recall: 0.6240 
[majority] - val_bacc: 0.7944 - val_f1: 0.6901 - val_precision: 0.7875 - val_recall: 0.6142 

Epoch 00011: val_loss did not improve from 0.20205
Epoch 12/100
160858/160858 [==============================] - 134s 836us/step - loss: 0.1349 - val_loss: 0.2427
[last] - val_bacc: 0.7830 - val_f1: 0.6571 - val_precision: 0.7248 - val_recall: 0.6009 
[majority] - val_bacc: 0.7845 - val_f1: 0.6755 - val_precision: 0.7825 - val_recall: 0.5942 

Epoch 00012: val_loss did not improve from 0.20205
Epoch 13/100
160858/160858 [==============================] - 133s 830us/step - loss: 0.1319 - val_loss: 0.2338
[last] - val_bacc: 0.8013 - val_f1: 0.6816 - val_precision: 0.7309 - val_recall: 0.6384 
[majority] - val_bacc: 0.8017 - val_f1: 0.6955 - val_precision: 0.7740 - val_recall: 0.6315 

Epoch 00013: val_loss did not improve from 0.20205
Epoch 00013: early stopping
	Min train loss: 0.13186890026198397 @epoch 12
	Min valid loss: 0.20204539715292125 @epoch 2

<Figure size 1000x600 with 1 Axes>

INFO:
	13fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 158289 samples, validate on 27906 samples
Epoch 1/100
158289/158289 [==============================] - 137s 864us/step - loss: 0.2535 - val_loss: 0.1648
[last] - val_bacc: 0.8475 - val_f1: 0.7184 - val_precision: 0.7060 - val_recall: 0.7312 
[majority] - val_bacc: 0.8451 - val_f1: 0.7258 - val_precision: 0.7295 - val_recall: 0.7221 

Epoch 00001: val_loss improved from inf to 0.16484, saving model to ./checkpoints/13fold_BN2_32-32u_32ws_12f/m_0001_0.2535_0.1648.hdf5
Epoch 2/100
158289/158289 [==============================] - 131s 825us/step - loss: 0.1959 - val_loss: 0.1603
[last] - val_bacc: 0.8485 - val_f1: 0.7254 - val_precision: 0.7200 - val_recall: 0.7309 
[majority] - val_bacc: 0.8460 - val_f1: 0.7416 - val_precision: 0.7666 - val_recall: 0.7181 

Epoch 00002: val_loss improved from 0.16484 to 0.16027, saving model to ./checkpoints/13fold_BN2_32-32u_32ws_12f/m_0002_0.1959_0.1603.hdf5
Epoch 3/100
158289/158289 [==============================] - 131s 830us/step - loss: 0.1827 - val_loss: 0.1612
[last] - val_bacc: 0.8430 - val_f1: 0.7201 - val_precision: 0.7210 - val_recall: 0.7191 
[majority] - val_bacc: 0.8439 - val_f1: 0.7415 - val_precision: 0.7728 - val_recall: 0.7127 

Epoch 00003: val_loss did not improve from 0.16027
Epoch 4/100
158289/158289 [==============================] - 132s 835us/step - loss: 0.1745 - val_loss: 0.1625
[last] - val_bacc: 0.8407 - val_f1: 0.7118 - val_precision: 0.7070 - val_recall: 0.7167 
[majority] - val_bacc: 0.8407 - val_f1: 0.7313 - val_precision: 0.7555 - val_recall: 0.7087 

Epoch 00004: val_loss did not improve from 0.16027
Epoch 5/100
158289/158289 [==============================] - 130s 821us/step - loss: 0.1677 - val_loss: 0.1638
[last] - val_bacc: 0.8196 - val_f1: 0.7005 - val_precision: 0.7368 - val_recall: 0.6676 
[majority] - val_bacc: 0.8219 - val_f1: 0.7162 - val_precision: 0.7729 - val_recall: 0.6672 

Epoch 00005: val_loss did not improve from 0.16027
Epoch 6/100
158289/158289 [==============================] - 129s 814us/step - loss: 0.1611 - val_loss: 0.1649
[last] - val_bacc: 0.8248 - val_f1: 0.7040 - val_precision: 0.7305 - val_recall: 0.6794 
[majority] - val_bacc: 0.8294 - val_f1: 0.7265 - val_precision: 0.7771 - val_recall: 0.6820 

Epoch 00006: val_loss did not improve from 0.16027
Epoch 7/100
158289/158289 [==============================] - 132s 831us/step - loss: 0.1565 - val_loss: 0.1649
[last] - val_bacc: 0.8376 - val_f1: 0.7157 - val_precision: 0.7244 - val_recall: 0.7073 
[majority] - val_bacc: 0.8373 - val_f1: 0.7309 - val_precision: 0.7643 - val_recall: 0.7002 

Epoch 00007: val_loss did not improve from 0.16027
Epoch 8/100
158289/158289 [==============================] - 131s 829us/step - loss: 0.1518 - val_loss: 0.1700
[last] - val_bacc: 0.8234 - val_f1: 0.7109 - val_precision: 0.7534 - val_recall: 0.6730 
[majority] - val_bacc: 0.8263 - val_f1: 0.7288 - val_precision: 0.7942 - val_recall: 0.6733 

Epoch 00008: val_loss did not improve from 0.16027
Epoch 9/100
158289/158289 [==============================] - 130s 823us/step - loss: 0.1482 - val_loss: 0.1701
[last] - val_bacc: 0.8273 - val_f1: 0.7019 - val_precision: 0.7177 - val_recall: 0.6868 
[majority] - val_bacc: 0.8337 - val_f1: 0.7253 - val_precision: 0.7601 - val_recall: 0.6935 

Epoch 00009: val_loss did not improve from 0.16027
Epoch 10/100
158289/158289 [==============================] - 132s 833us/step - loss: 0.1454 - val_loss: 0.1706
[last] - val_bacc: 0.8275 - val_f1: 0.7029 - val_precision: 0.7197 - val_recall: 0.6868 
[majority] - val_bacc: 0.8340 - val_f1: 0.7296 - val_precision: 0.7709 - val_recall: 0.6925 

Epoch 00010: val_loss did not improve from 0.16027
Epoch 11/100
158289/158289 [==============================] - 129s 818us/step - loss: 0.1423 - val_loss: 0.1761
[last] - val_bacc: 0.8415 - val_f1: 0.6991 - val_precision: 0.6754 - val_recall: 0.7245 
[majority] - val_bacc: 0.8462 - val_f1: 0.7267 - val_precision: 0.7289 - val_recall: 0.7245 

Epoch 00011: val_loss did not improve from 0.16027
Epoch 12/100
158289/158289 [==============================] - 132s 832us/step - loss: 0.1402 - val_loss: 0.1706
[last] - val_bacc: 0.8289 - val_f1: 0.7068 - val_precision: 0.7257 - val_recall: 0.6888 
[majority] - val_bacc: 0.8290 - val_f1: 0.7180 - val_precision: 0.7551 - val_recall: 0.6844 

Epoch 00012: val_loss did not improve from 0.16027
Epoch 00012: early stopping
	Min train loss: 0.1401804225064879 @epoch 11
	Min valid loss: 0.1602657404655813 @epoch 1

<Figure size 1000x600 with 1 Axes>

INFO:
	14fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 160075 samples, validate on 28221 samples
Epoch 1/100
160075/160075 [==============================] - 137s 858us/step - loss: 0.2443 - val_loss: 0.2301
[last] - val_bacc: 0.7866 - val_f1: 0.6714 - val_precision: 0.7569 - val_recall: 0.6032 
[majority] - val_bacc: 0.7760 - val_f1: 0.6597 - val_precision: 0.7659 - val_recall: 0.5794 

Epoch 00001: val_loss improved from inf to 0.23014, saving model to ./checkpoints/14fold_BN2_32-32u_32ws_12f/m_0001_0.2443_0.2301.hdf5
Epoch 2/100
160075/160075 [==============================] - 131s 816us/step - loss: 0.1835 - val_loss: 0.2397
[last] - val_bacc: 0.7940 - val_f1: 0.6812 - val_precision: 0.7580 - val_recall: 0.6186 
[majority] - val_bacc: 0.7792 - val_f1: 0.6662 - val_precision: 0.7735 - val_recall: 0.5850 

Epoch 00002: val_loss did not improve from 0.23014
Epoch 3/100
160075/160075 [==============================] - 131s 820us/step - loss: 0.1724 - val_loss: 0.2374
[last] - val_bacc: 0.8159 - val_f1: 0.6916 - val_precision: 0.7095 - val_recall: 0.6746 
[majority] - val_bacc: 0.8040 - val_f1: 0.6860 - val_precision: 0.7336 - val_recall: 0.6442 

Epoch 00003: val_loss did not improve from 0.23014
Epoch 4/100
160075/160075 [==============================] - 131s 821us/step - loss: 0.1643 - val_loss: 0.2446
[last] - val_bacc: 0.8182 - val_f1: 0.6905 - val_precision: 0.6995 - val_recall: 0.6817 
[majority] - val_bacc: 0.8050 - val_f1: 0.6863 - val_precision: 0.7309 - val_recall: 0.6468 

Epoch 00004: val_loss did not improve from 0.23014
Epoch 5/100
160075/160075 [==============================] - 131s 817us/step - loss: 0.1588 - val_loss: 0.2512
[last] - val_bacc: 0.7898 - val_f1: 0.6627 - val_precision: 0.7147 - val_recall: 0.6178 
[majority] - val_bacc: 0.7833 - val_f1: 0.6614 - val_precision: 0.7376 - val_recall: 0.5995 

Epoch 00005: val_loss did not improve from 0.23014
Epoch 6/100
160075/160075 [==============================] - 131s 819us/step - loss: 0.1531 - val_loss: 0.2482
[last] - val_bacc: 0.8099 - val_f1: 0.6840 - val_precision: 0.7072 - val_recall: 0.6622 
[majority] - val_bacc: 0.7979 - val_f1: 0.6832 - val_precision: 0.7483 - val_recall: 0.6286 

Epoch 00006: val_loss did not improve from 0.23014
Epoch 7/100
160075/160075 [==============================] - 132s 827us/step - loss: 0.1494 - val_loss: 0.2501
[last] - val_bacc: 0.8246 - val_f1: 0.6889 - val_precision: 0.6775 - val_recall: 0.7008 
[majority] - val_bacc: 0.8183 - val_f1: 0.6949 - val_precision: 0.7111 - val_recall: 0.6794 

Epoch 00007: val_loss did not improve from 0.23014
Epoch 8/100
160075/160075 [==============================] - 133s 832us/step - loss: 0.1451 - val_loss: 0.2662
[last] - val_bacc: 0.7845 - val_f1: 0.6564 - val_precision: 0.7153 - val_recall: 0.6064 
[majority] - val_bacc: 0.7796 - val_f1: 0.6594 - val_precision: 0.7472 - val_recall: 0.5900 

Epoch 00008: val_loss did not improve from 0.23014
Epoch 9/100
160075/160075 [==============================] - 131s 817us/step - loss: 0.1409 - val_loss: 0.2606
[last] - val_bacc: 0.7979 - val_f1: 0.6661 - val_precision: 0.6954 - val_recall: 0.6392 
[majority] - val_bacc: 0.7955 - val_f1: 0.6754 - val_precision: 0.7330 - val_recall: 0.6262 

Epoch 00009: val_loss did not improve from 0.23014
Epoch 10/100
160075/160075 [==============================] - 132s 826us/step - loss: 0.1391 - val_loss: 0.2646
[last] - val_bacc: 0.8060 - val_f1: 0.6678 - val_precision: 0.6744 - val_recall: 0.6614 
[majority] - val_bacc: 0.8057 - val_f1: 0.6826 - val_precision: 0.7174 - val_recall: 0.6511 

Epoch 00010: val_loss did not improve from 0.23014
Epoch 11/100
160075/160075 [==============================] - 130s 814us/step - loss: 0.1356 - val_loss: 0.2673
[last] - val_bacc: 0.8169 - val_f1: 0.6784 - val_precision: 0.6713 - val_recall: 0.6857 
[majority] - val_bacc: 0.8160 - val_f1: 0.6962 - val_precision: 0.7222 - val_recall: 0.6720 

Epoch 00011: val_loss did not improve from 0.23014
Epoch 00011: early stopping
	Min train loss: 0.13564312910649173 @epoch 10
	Min valid loss: 0.2301380254553582 @epoch 0

<Figure size 1000x600 with 1 Axes>

INFO:
	15fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 158929 samples, validate on 28019 samples
Epoch 1/100
158929/158929 [==============================] - 138s 870us/step - loss: 0.2394 - val_loss: 0.2640
[last] - val_bacc: 0.7332 - val_f1: 0.5949 - val_precision: 0.7488 - val_recall: 0.4935 
[majority] - val_bacc: 0.7205 - val_f1: 0.5778 - val_precision: 0.7645 - val_recall: 0.4644 

Epoch 00001: val_loss improved from inf to 0.26395, saving model to ./checkpoints/15fold_BN2_32-32u_32ws_12f/m_0001_0.2394_0.2640.hdf5
Epoch 2/100
158929/158929 [==============================] - 132s 830us/step - loss: 0.1803 - val_loss: 0.2578
[last] - val_bacc: 0.7735 - val_f1: 0.6475 - val_precision: 0.7290 - val_recall: 0.5824 
[majority] - val_bacc: 0.7554 - val_f1: 0.6288 - val_precision: 0.7528 - val_recall: 0.5399 

Epoch 00002: val_loss improved from 0.26395 to 0.25780, saving model to ./checkpoints/15fold_BN2_32-32u_32ws_12f/m_0002_0.1803_0.2578.hdf5
Epoch 3/100
158929/158929 [==============================] - 131s 827us/step - loss: 0.1683 - val_loss: 0.2480
[last] - val_bacc: 0.7916 - val_f1: 0.6757 - val_precision: 0.7459 - val_recall: 0.6176 
[majority] - val_bacc: 0.7720 - val_f1: 0.6528 - val_precision: 0.7556 - val_recall: 0.5746 

Epoch 00003: val_loss improved from 0.25780 to 0.24798, saving model to ./checkpoints/15fold_BN2_32-32u_32ws_12f/m_0003_0.1683_0.2480.hdf5
Epoch 4/100
158929/158929 [==============================] - 134s 841us/step - loss: 0.1613 - val_loss: 0.2662
[last] - val_bacc: 0.7766 - val_f1: 0.6532 - val_precision: 0.7346 - val_recall: 0.5880 
[majority] - val_bacc: 0.7520 - val_f1: 0.6229 - val_precision: 0.7482 - val_recall: 0.5336 

Epoch 00004: val_loss did not improve from 0.24798
Epoch 5/100
158929/158929 [==============================] - 131s 823us/step - loss: 0.1537 - val_loss: 0.2559
[last] - val_bacc: 0.7998 - val_f1: 0.6814 - val_precision: 0.7311 - val_recall: 0.6381 
[majority] - val_bacc: 0.7835 - val_f1: 0.6637 - val_precision: 0.7400 - val_recall: 0.6017 

Epoch 00005: val_loss did not improve from 0.24798
Epoch 6/100
158929/158929 [==============================] - 133s 836us/step - loss: 0.1478 - val_loss: 0.2764
[last] - val_bacc: 0.7890 - val_f1: 0.6643 - val_precision: 0.7186 - val_recall: 0.6176 
[majority] - val_bacc: 0.7708 - val_f1: 0.6438 - val_precision: 0.7283 - val_recall: 0.5769 

Epoch 00006: val_loss did not improve from 0.24798
Epoch 7/100
158929/158929 [==============================] - 131s 826us/step - loss: 0.1433 - val_loss: 0.2758
[last] - val_bacc: 0.7888 - val_f1: 0.6606 - val_precision: 0.7074 - val_recall: 0.6197 
[majority] - val_bacc: 0.7649 - val_f1: 0.6377 - val_precision: 0.7350 - val_recall: 0.5632 

Epoch 00007: val_loss did not improve from 0.24798
Epoch 8/100
158929/158929 [==============================] - 132s 829us/step - loss: 0.1392 - val_loss: 0.2872
[last] - val_bacc: 0.7725 - val_f1: 0.6400 - val_precision: 0.7064 - val_recall: 0.5850 
[majority] - val_bacc: 0.7582 - val_f1: 0.6272 - val_precision: 0.7300 - val_recall: 0.5498 

Epoch 00008: val_loss did not improve from 0.24798
Epoch 9/100
158929/158929 [==============================] - 131s 826us/step - loss: 0.1350 - val_loss: 0.2738
[last] - val_bacc: 0.7679 - val_f1: 0.6390 - val_precision: 0.7249 - val_recall: 0.5713 
[majority] - val_bacc: 0.7597 - val_f1: 0.6349 - val_precision: 0.7528 - val_recall: 0.5490 

Epoch 00009: val_loss did not improve from 0.24798
Epoch 10/100
158929/158929 [==============================] - 130s 820us/step - loss: 0.1324 - val_loss: 0.2775
[last] - val_bacc: 0.8031 - val_f1: 0.6699 - val_precision: 0.6845 - val_recall: 0.6559 
[majority] - val_bacc: 0.7863 - val_f1: 0.6582 - val_precision: 0.7096 - val_recall: 0.6138 

Epoch 00010: val_loss did not improve from 0.24798
Epoch 11/100
158929/158929 [==============================] - 132s 833us/step - loss: 0.1291 - val_loss: 0.2956
[last] - val_bacc: 0.7762 - val_f1: 0.6455 - val_precision: 0.7092 - val_recall: 0.5923 
[majority] - val_bacc: 0.7697 - val_f1: 0.6451 - val_precision: 0.7386 - val_recall: 0.5726 

Epoch 00011: val_loss did not improve from 0.24798
Epoch 12/100
158929/158929 [==============================] - 132s 834us/step - loss: 0.1272 - val_loss: 0.2886
[last] - val_bacc: 0.7792 - val_f1: 0.6491 - val_precision: 0.7085 - val_recall: 0.5989 
[majority] - val_bacc: 0.7667 - val_f1: 0.6396 - val_precision: 0.7332 - val_recall: 0.5672 

Epoch 00012: val_loss did not improve from 0.24798
Epoch 13/100
158929/158929 [==============================] - 133s 838us/step - loss: 0.1252 - val_loss: 0.3029
[last] - val_bacc: 0.7804 - val_f1: 0.6488 - val_precision: 0.7025 - val_recall: 0.6027 
[majority] - val_bacc: 0.7661 - val_f1: 0.6396 - val_precision: 0.7360 - val_recall: 0.5655 

Epoch 00013: val_loss did not improve from 0.24798
Epoch 00013: early stopping
	Min train loss: 0.12517592162778937 @epoch 12
	Min valid loss: 0.24798142920023547 @epoch 2

<Figure size 1000x600 with 1 Axes>

INFO:
	16fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 159337 samples, validate on 28091 samples
Epoch 1/100
159337/159337 [==============================] - 140s 876us/step - loss: 0.2437 - val_loss: 0.2206
[last] - val_bacc: 0.7789 - val_f1: 0.6491 - val_precision: 0.7204 - val_recall: 0.5906 
[majority] - val_bacc: 0.7715 - val_f1: 0.6492 - val_precision: 0.7546 - val_recall: 0.5696 

Epoch 00001: val_loss improved from inf to 0.22057, saving model to ./checkpoints/16fold_BN2_32-32u_32ws_12f/m_0001_0.2437_0.2206.hdf5
Epoch 2/100
159337/159337 [==============================] - 133s 837us/step - loss: 0.1833 - val_loss: 0.2241
[last] - val_bacc: 0.7803 - val_f1: 0.6567 - val_precision: 0.7398 - val_recall: 0.5903 
[majority] - val_bacc: 0.7721 - val_f1: 0.6522 - val_precision: 0.7629 - val_recall: 0.5696 

Epoch 00002: val_loss did not improve from 0.22057
Epoch 3/100
159337/159337 [==============================] - 130s 816us/step - loss: 0.1709 - val_loss: 0.2280
[last] - val_bacc: 0.7738 - val_f1: 0.6509 - val_precision: 0.7499 - val_recall: 0.5750 
[majority] - val_bacc: 0.7683 - val_f1: 0.6480 - val_precision: 0.7667 - val_recall: 0.5611 

Epoch 00003: val_loss did not improve from 0.22057
Epoch 4/100
159337/159337 [==============================] - 131s 821us/step - loss: 0.1620 - val_loss: 0.2300
[last] - val_bacc: 0.7708 - val_f1: 0.6412 - val_precision: 0.7296 - val_recall: 0.5719 
[majority] - val_bacc: 0.7668 - val_f1: 0.6430 - val_precision: 0.7561 - val_recall: 0.5594 

Epoch 00004: val_loss did not improve from 0.22057
Epoch 5/100
159337/159337 [==============================] - 132s 826us/step - loss: 0.1556 - val_loss: 0.2387
[last] - val_bacc: 0.7653 - val_f1: 0.6376 - val_precision: 0.7432 - val_recall: 0.5582 
[majority] - val_bacc: 0.7638 - val_f1: 0.6397 - val_precision: 0.7590 - val_recall: 0.5528 

Epoch 00005: val_loss did not improve from 0.22057
Epoch 6/100
159337/159337 [==============================] - 131s 823us/step - loss: 0.1499 - val_loss: 0.2446
[last] - val_bacc: 0.7617 - val_f1: 0.6281 - val_precision: 0.7261 - val_recall: 0.5534 
[majority] - val_bacc: 0.7599 - val_f1: 0.6344 - val_precision: 0.7603 - val_recall: 0.5443 

Epoch 00006: val_loss did not improve from 0.22057
Epoch 7/100
159337/159337 [==============================] - 132s 830us/step - loss: 0.1454 - val_loss: 0.2535
[last] - val_bacc: 0.7626 - val_f1: 0.6265 - val_precision: 0.7161 - val_recall: 0.5568 
[majority] - val_bacc: 0.7656 - val_f1: 0.6410 - val_precision: 0.7545 - val_recall: 0.5571 

Epoch 00007: val_loss did not improve from 0.22057
Epoch 8/100
159337/159337 [==============================] - 130s 817us/step - loss: 0.1416 - val_loss: 0.2499
[last] - val_bacc: 0.7683 - val_f1: 0.6347 - val_precision: 0.7185 - val_recall: 0.5685 
[majority] - val_bacc: 0.7682 - val_f1: 0.6445 - val_precision: 0.7541 - val_recall: 0.5628 

Epoch 00008: val_loss did not improve from 0.22057
Epoch 9/100
159337/159337 [==============================] - 131s 822us/step - loss: 0.1377 - val_loss: 0.2496
[last] - val_bacc: 0.7552 - val_f1: 0.6164 - val_precision: 0.7159 - val_recall: 0.5412 
[majority] - val_bacc: 0.7621 - val_f1: 0.6364 - val_precision: 0.7556 - val_recall: 0.5497 

Epoch 00009: val_loss did not improve from 0.22057
Epoch 10/100
159337/159337 [==============================] - 130s 817us/step - loss: 0.1346 - val_loss: 0.2540
[last] - val_bacc: 0.7659 - val_f1: 0.6292 - val_precision: 0.7103 - val_recall: 0.5648 
[majority] - val_bacc: 0.7678 - val_f1: 0.6397 - val_precision: 0.7386 - val_recall: 0.5642 

Epoch 00010: val_loss did not improve from 0.22057
Epoch 11/100
159337/159337 [==============================] - 132s 827us/step - loss: 0.1321 - val_loss: 0.2565
[last] - val_bacc: 0.7610 - val_f1: 0.6248 - val_precision: 0.7179 - val_recall: 0.5531 
[majority] - val_bacc: 0.7637 - val_f1: 0.6373 - val_precision: 0.7508 - val_recall: 0.5537 

Epoch 00011: val_loss did not improve from 0.22057
Epoch 00011: early stopping
	Min train loss: 0.13214771453718166 @epoch 10
	Min valid loss: 0.22056923070719925 @epoch 0

<Figure size 1000x600 with 1 Axes>

INFO:
	17fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 156186 samples, validate on 27534 samples
Epoch 1/100
156186/156186 [==============================] - 136s 872us/step - loss: 0.2524 - val_loss: 0.1734
[last] - val_bacc: 0.8339 - val_f1: 0.7149 - val_precision: 0.7268 - val_recall: 0.7035 
[majority] - val_bacc: 0.8323 - val_f1: 0.7225 - val_precision: 0.7517 - val_recall: 0.6955 

Epoch 00001: val_loss improved from inf to 0.17342, saving model to ./checkpoints/17fold_BN2_32-32u_32ws_12f/m_0001_0.2524_0.1734.hdf5
Epoch 2/100
156186/156186 [==============================] - 129s 827us/step - loss: 0.1943 - val_loss: 0.1729
[last] - val_bacc: 0.8457 - val_f1: 0.7158 - val_precision: 0.6983 - val_recall: 0.7343 
[majority] - val_bacc: 0.8432 - val_f1: 0.7264 - val_precision: 0.7304 - val_recall: 0.7224 

Epoch 00002: val_loss improved from 0.17342 to 0.17294, saving model to ./checkpoints/17fold_BN2_32-32u_32ws_12f/m_0002_0.1943_0.1729.hdf5
Epoch 3/100
156186/156186 [==============================] - 131s 837us/step - loss: 0.1816 - val_loss: 0.1687
[last] - val_bacc: 0.8396 - val_f1: 0.7286 - val_precision: 0.7459 - val_recall: 0.7120 
[majority] - val_bacc: 0.8387 - val_f1: 0.7360 - val_precision: 0.7683 - val_recall: 0.7062 

Epoch 00003: val_loss improved from 0.17294 to 0.16865, saving model to ./checkpoints/17fold_BN2_32-32u_32ws_12f/m_0003_0.1816_0.1687.hdf5
Epoch 4/100
156186/156186 [==============================] - 131s 841us/step - loss: 0.1720 - val_loss: 0.1729
[last] - val_bacc: 0.8620 - val_f1: 0.7396 - val_precision: 0.7158 - val_recall: 0.7651 
[majority] - val_bacc: 0.8587 - val_f1: 0.7415 - val_precision: 0.7279 - val_recall: 0.7556 

Epoch 00004: val_loss did not improve from 0.16865
Epoch 5/100
156186/156186 [==============================] - 131s 836us/step - loss: 0.1647 - val_loss: 0.1676
[last] - val_bacc: 0.8363 - val_f1: 0.7282 - val_precision: 0.7548 - val_recall: 0.7035 
[majority] - val_bacc: 0.8342 - val_f1: 0.7361 - val_precision: 0.7832 - val_recall: 0.6943 

Epoch 00005: val_loss improved from 0.16865 to 0.16762, saving model to ./checkpoints/17fold_BN2_32-32u_32ws_12f/m_0005_0.1647_0.1676.hdf5
Epoch 6/100
156186/156186 [==============================] - 133s 849us/step - loss: 0.1594 - val_loss: 0.1773
[last] - val_bacc: 0.8506 - val_f1: 0.7263 - val_precision: 0.7113 - val_recall: 0.7419 
[majority] - val_bacc: 0.8506 - val_f1: 0.7422 - val_precision: 0.7503 - val_recall: 0.7343 

Epoch 00006: val_loss did not improve from 0.16762
Epoch 7/100
156186/156186 [==============================] - 131s 839us/step - loss: 0.1558 - val_loss: 0.1792
[last] - val_bacc: 0.8267 - val_f1: 0.7107 - val_precision: 0.7363 - val_recall: 0.6867 
[majority] - val_bacc: 0.8270 - val_f1: 0.7224 - val_precision: 0.7680 - val_recall: 0.6818 

Epoch 00007: val_loss did not improve from 0.16762
Epoch 8/100
156186/156186 [==============================] - 129s 827us/step - loss: 0.1513 - val_loss: 0.1874
[last] - val_bacc: 0.8341 - val_f1: 0.7001 - val_precision: 0.6888 - val_recall: 0.7117 
[majority] - val_bacc: 0.8355 - val_f1: 0.7153 - val_precision: 0.7230 - val_recall: 0.7077 

Epoch 00008: val_loss did not improve from 0.16762
Epoch 9/100
156186/156186 [==============================] - 127s 815us/step - loss: 0.1477 - val_loss: 0.1925
[last] - val_bacc: 0.8268 - val_f1: 0.6894 - val_precision: 0.6813 - val_recall: 0.6977 
[majority] - val_bacc: 0.8240 - val_f1: 0.7026 - val_precision: 0.7230 - val_recall: 0.6833 

Epoch 00009: val_loss did not improve from 0.16762
Epoch 10/100
156186/156186 [==============================] - 130s 834us/step - loss: 0.1436 - val_loss: 0.1892
[last] - val_bacc: 0.8118 - val_f1: 0.6899 - val_precision: 0.7262 - val_recall: 0.6571 
[majority] - val_bacc: 0.8166 - val_f1: 0.7070 - val_precision: 0.7589 - val_recall: 0.6617 

Epoch 00010: val_loss did not improve from 0.16762
Epoch 11/100
156186/156186 [==============================] - 130s 831us/step - loss: 0.1418 - val_loss: 0.1892
[last] - val_bacc: 0.8182 - val_f1: 0.6898 - val_precision: 0.7057 - val_recall: 0.6745 
[majority] - val_bacc: 0.8200 - val_f1: 0.7090 - val_precision: 0.7533 - val_recall: 0.6696 

Epoch 00011: val_loss did not improve from 0.16762
Epoch 12/100
156186/156186 [==============================] - 129s 824us/step - loss: 0.1392 - val_loss: 0.1953
[last] - val_bacc: 0.8097 - val_f1: 0.6807 - val_precision: 0.7071 - val_recall: 0.6562 
[majority] - val_bacc: 0.8099 - val_f1: 0.6925 - val_precision: 0.7401 - val_recall: 0.6507 

Epoch 00012: val_loss did not improve from 0.16762
Epoch 13/100
156186/156186 [==============================] - 132s 847us/step - loss: 0.1363 - val_loss: 0.1947
[last] - val_bacc: 0.8115 - val_f1: 0.6871 - val_precision: 0.7191 - val_recall: 0.6577 
[majority] - val_bacc: 0.8150 - val_f1: 0.7002 - val_precision: 0.7450 - val_recall: 0.6605 

Epoch 00013: val_loss did not improve from 0.16762
Epoch 14/100
156186/156186 [==============================] - 131s 838us/step - loss: 0.1345 - val_loss: 0.1955
[last] - val_bacc: 0.8166 - val_f1: 0.7017 - val_precision: 0.7438 - val_recall: 0.6641 
[majority] - val_bacc: 0.8143 - val_f1: 0.7054 - val_precision: 0.7625 - val_recall: 0.6562 

Epoch 00014: val_loss did not improve from 0.16762
Epoch 15/100
156186/156186 [==============================] - 131s 838us/step - loss: 0.1330 - val_loss: 0.2052
[last] - val_bacc: 0.8530 - val_f1: 0.7058 - val_precision: 0.6596 - val_recall: 0.7590 
[majority] - val_bacc: 0.8504 - val_f1: 0.7202 - val_precision: 0.6975 - val_recall: 0.7444 

Epoch 00015: val_loss did not improve from 0.16762
Epoch 00015: early stopping
	Min train loss: 0.13298059178798027 @epoch 14
	Min valid loss: 0.16762002300774947 @epoch 4

<Figure size 1000x600 with 1 Axes>

INFO:
	18fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 158884 samples, validate on 28010 samples
Epoch 1/100
158884/158884 [==============================] - 139s 877us/step - loss: 0.2478 - val_loss: 0.2059
[last] - val_bacc: 0.8071 - val_f1: 0.6720 - val_precision: 0.6928 - val_recall: 0.6525 
[majority] - val_bacc: 0.8030 - val_f1: 0.6762 - val_precision: 0.7175 - val_recall: 0.6393 

Epoch 00001: val_loss improved from inf to 0.20589, saving model to ./checkpoints/18fold_BN2_32-32u_32ws_12f/m_0001_0.2478_0.2059.hdf5
Epoch 2/100
158884/158884 [==============================] - 133s 837us/step - loss: 0.1899 - val_loss: 0.2037
[last] - val_bacc: 0.7986 - val_f1: 0.6620 - val_precision: 0.6920 - val_recall: 0.6344 
[majority] - val_bacc: 0.7941 - val_f1: 0.6636 - val_precision: 0.7121 - val_recall: 0.6213 

Epoch 00002: val_loss improved from 0.20589 to 0.20373, saving model to ./checkpoints/18fold_BN2_32-32u_32ws_12f/m_0002_0.1899_0.2037.hdf5
Epoch 3/100
158884/158884 [==============================] - 134s 843us/step - loss: 0.1782 - val_loss: 0.2103
[last] - val_bacc: 0.8197 - val_f1: 0.6703 - val_precision: 0.6541 - val_recall: 0.6874 
[majority] - val_bacc: 0.8208 - val_f1: 0.6843 - val_precision: 0.6856 - val_recall: 0.6831 

Epoch 00003: val_loss did not improve from 0.20373
Epoch 4/100
158884/158884 [==============================] - 133s 836us/step - loss: 0.1699 - val_loss: 0.2162
[last] - val_bacc: 0.7810 - val_f1: 0.6350 - val_precision: 0.6736 - val_recall: 0.6005 
[majority] - val_bacc: 0.7811 - val_f1: 0.6458 - val_precision: 0.7060 - val_recall: 0.5950 

Epoch 00004: val_loss did not improve from 0.20373
Epoch 5/100
158884/158884 [==============================] - 133s 837us/step - loss: 0.1627 - val_loss: 0.2198
[last] - val_bacc: 0.8045 - val_f1: 0.6433 - val_precision: 0.6266 - val_recall: 0.6611 
[majority] - val_bacc: 0.8087 - val_f1: 0.6697 - val_precision: 0.6818 - val_recall: 0.6580 

Epoch 00005: val_loss did not improve from 0.20373
Epoch 6/100
158884/158884 [==============================] - 133s 834us/step - loss: 0.1571 - val_loss: 0.2216
[last] - val_bacc: 0.7994 - val_f1: 0.6541 - val_precision: 0.6678 - val_recall: 0.6409 
[majority] - val_bacc: 0.7990 - val_f1: 0.6606 - val_precision: 0.6869 - val_recall: 0.6363 

Epoch 00006: val_loss did not improve from 0.20373
Epoch 7/100
158884/158884 [==============================] - 131s 822us/step - loss: 0.1527 - val_loss: 0.2145
[last] - val_bacc: 0.7925 - val_f1: 0.6566 - val_precision: 0.6970 - val_recall: 0.6207 
[majority] - val_bacc: 0.7915 - val_f1: 0.6633 - val_precision: 0.7203 - val_recall: 0.6146 

Epoch 00007: val_loss did not improve from 0.20373
Epoch 8/100
158884/158884 [==============================] - 132s 828us/step - loss: 0.1486 - val_loss: 0.2269
[last] - val_bacc: 0.7889 - val_f1: 0.6452 - val_precision: 0.6765 - val_recall: 0.6167 
[majority] - val_bacc: 0.7863 - val_f1: 0.6537 - val_precision: 0.7107 - val_recall: 0.6051 

Epoch 00008: val_loss did not improve from 0.20373
Epoch 9/100
158884/158884 [==============================] - 133s 836us/step - loss: 0.1454 - val_loss: 0.2357
[last] - val_bacc: 0.7799 - val_f1: 0.6369 - val_precision: 0.6836 - val_recall: 0.5962 
[majority] - val_bacc: 0.7820 - val_f1: 0.6479 - val_precision: 0.7095 - val_recall: 0.5962 

Epoch 00009: val_loss did not improve from 0.20373
Epoch 10/100
158884/158884 [==============================] - 132s 833us/step - loss: 0.1428 - val_loss: 0.2328
[last] - val_bacc: 0.7855 - val_f1: 0.6502 - val_precision: 0.7031 - val_recall: 0.6048 
[majority] - val_bacc: 0.7845 - val_f1: 0.6555 - val_precision: 0.7234 - val_recall: 0.5993 

Epoch 00010: val_loss did not improve from 0.20373
Epoch 11/100
158884/158884 [==============================] - 133s 835us/step - loss: 0.1401 - val_loss: 0.2355
[last] - val_bacc: 0.7957 - val_f1: 0.6513 - val_precision: 0.6715 - val_recall: 0.6323 
[majority] - val_bacc: 0.7953 - val_f1: 0.6610 - val_precision: 0.6998 - val_recall: 0.6262 

Epoch 00011: val_loss did not improve from 0.20373
Epoch 12/100
158884/158884 [==============================] - 133s 840us/step - loss: 0.1375 - val_loss: 0.2338
[last] - val_bacc: 0.8020 - val_f1: 0.6483 - val_precision: 0.6453 - val_recall: 0.6513 
[majority] - val_bacc: 0.7983 - val_f1: 0.6598 - val_precision: 0.6869 - val_recall: 0.6348 

Epoch 00012: val_loss did not improve from 0.20373
Epoch 00012: early stopping
	Min train loss: 0.13754935692148426 @epoch 11
	Min valid loss: 0.20373090400787117 @epoch 1

<Figure size 1000x600 with 1 Axes>

INFO:
	19fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 159258 samples, validate on 28076 samples
Epoch 1/100
159258/159258 [==============================] - 140s 881us/step - loss: 0.2383 - val_loss: 0.2473
[last] - val_bacc: 0.7534 - val_f1: 0.6204 - val_precision: 0.7385 - val_recall: 0.5349 
[majority] - val_bacc: 0.7446 - val_f1: 0.6098 - val_precision: 0.7472 - val_recall: 0.5151 

Epoch 00001: val_loss improved from inf to 0.24732, saving model to ./checkpoints/19fold_BN2_32-32u_32ws_12f/m_0001_0.2383_0.2473.hdf5
Epoch 2/100
159258/159258 [==============================] - 134s 839us/step - loss: 0.1767 - val_loss: 0.2465
[last] - val_bacc: 0.7642 - val_f1: 0.6338 - val_precision: 0.7322 - val_recall: 0.5587 
[majority] - val_bacc: 0.7492 - val_f1: 0.6158 - val_precision: 0.7446 - val_recall: 0.5250 

Epoch 00002: val_loss improved from 0.24732 to 0.24646, saving model to ./checkpoints/19fold_BN2_32-32u_32ws_12f/m_0002_0.1767_0.2465.hdf5
Epoch 3/100
159258/159258 [==============================] - 132s 829us/step - loss: 0.1653 - val_loss: 0.2491
[last] - val_bacc: 0.7690 - val_f1: 0.6322 - val_precision: 0.7037 - val_recall: 0.5739 
[majority] - val_bacc: 0.7561 - val_f1: 0.6192 - val_precision: 0.7189 - val_recall: 0.5438 

Epoch 00003: val_loss did not improve from 0.24646
Epoch 4/100
159258/159258 [==============================] - 133s 837us/step - loss: 0.1580 - val_loss: 0.2601
[last] - val_bacc: 0.7722 - val_f1: 0.6377 - val_precision: 0.7086 - val_recall: 0.5797 
[majority] - val_bacc: 0.7574 - val_f1: 0.6237 - val_precision: 0.7296 - val_recall: 0.5446 

Epoch 00004: val_loss did not improve from 0.24646
Epoch 5/100
159258/159258 [==============================] - 134s 842us/step - loss: 0.1515 - val_loss: 0.2623
[last] - val_bacc: 0.7828 - val_f1: 0.6461 - val_precision: 0.6928 - val_recall: 0.6054 
[majority] - val_bacc: 0.7722 - val_f1: 0.6400 - val_precision: 0.7164 - val_recall: 0.5783 

Epoch 00005: val_loss did not improve from 0.24646
Epoch 6/100
159258/159258 [==============================] - 133s 833us/step - loss: 0.1457 - val_loss: 0.2660
[last] - val_bacc: 0.7679 - val_f1: 0.6320 - val_precision: 0.7083 - val_recall: 0.5706 
[majority] - val_bacc: 0.7570 - val_f1: 0.6226 - val_precision: 0.7277 - val_recall: 0.5440 

Epoch 00006: val_loss did not improve from 0.24646
Epoch 7/100
159258/159258 [==============================] - 132s 830us/step - loss: 0.1411 - val_loss: 0.2759
[last] - val_bacc: 0.7543 - val_f1: 0.6141 - val_precision: 0.7090 - val_recall: 0.5416 
[majority] - val_bacc: 0.7463 - val_f1: 0.6057 - val_precision: 0.7198 - val_recall: 0.5228 

Epoch 00007: val_loss did not improve from 0.24646
Epoch 8/100
159258/159258 [==============================] - 134s 842us/step - loss: 0.1371 - val_loss: 0.2887
[last] - val_bacc: 0.7506 - val_f1: 0.6155 - val_precision: 0.7350 - val_recall: 0.5294 
[majority] - val_bacc: 0.7435 - val_f1: 0.6050 - val_precision: 0.7336 - val_recall: 0.5148 

Epoch 00008: val_loss did not improve from 0.24646
Epoch 9/100
159258/159258 [==============================] - 135s 845us/step - loss: 0.1340 - val_loss: 0.2797
[last] - val_bacc: 0.7663 - val_f1: 0.6301 - val_precision: 0.7085 - val_recall: 0.5672 
[majority] - val_bacc: 0.7605 - val_f1: 0.6251 - val_precision: 0.7191 - val_recall: 0.5529 

Epoch 00009: val_loss did not improve from 0.24646
Epoch 10/100
159258/159258 [==============================] - 134s 842us/step - loss: 0.1313 - val_loss: 0.2882
[last] - val_bacc: 0.7668 - val_f1: 0.6296 - val_precision: 0.7048 - val_recall: 0.5689 
[majority] - val_bacc: 0.7569 - val_f1: 0.6199 - val_precision: 0.7175 - val_recall: 0.5457 

Epoch 00010: val_loss did not improve from 0.24646
Epoch 11/100
159258/159258 [==============================] - 133s 833us/step - loss: 0.1285 - val_loss: 0.2801
[last] - val_bacc: 0.7750 - val_f1: 0.6330 - val_precision: 0.6819 - val_recall: 0.5907 
[majority] - val_bacc: 0.7644 - val_f1: 0.6222 - val_precision: 0.6901 - val_recall: 0.5664 

Epoch 00011: val_loss did not improve from 0.24646
Epoch 12/100
159258/159258 [==============================] - 132s 828us/step - loss: 0.1266 - val_loss: 0.2951
[last] - val_bacc: 0.7888 - val_f1: 0.6469 - val_precision: 0.6737 - val_recall: 0.6222 
[majority] - val_bacc: 0.7760 - val_f1: 0.6382 - val_precision: 0.6944 - val_recall: 0.5904 

Epoch 00012: val_loss did not improve from 0.24646
Epoch 00012: early stopping
	Min train loss: 0.1266410862439966 @epoch 11
	Min valid loss: 0.24646306697922477 @epoch 1

<Figure size 1000x600 with 1 Axes>

INFO:
	20fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 157060 samples, validate on 27689 samples
Epoch 1/100
157060/157060 [==============================] - 138s 882us/step - loss: 0.2386 - val_loss: 0.2270
[last] - val_bacc: 0.7526 - val_f1: 0.6435 - val_precision: 0.8375 - val_recall: 0.5224 
[majority] - val_bacc: 0.7491 - val_f1: 0.6407 - val_precision: 0.8516 - val_recall: 0.5135 

Epoch 00001: val_loss improved from inf to 0.22696, saving model to ./checkpoints/20fold_BN2_32-32u_32ws_12f/m_0001_0.2386_0.2270.hdf5
Epoch 2/100
157060/157060 [==============================] - 131s 832us/step - loss: 0.1838 - val_loss: 0.2174
[last] - val_bacc: 0.7958 - val_f1: 0.6887 - val_precision: 0.7692 - val_recall: 0.6235 
[majority] - val_bacc: 0.7873 - val_f1: 0.6873 - val_precision: 0.8057 - val_recall: 0.5993 

Epoch 00002: val_loss improved from 0.22696 to 0.21741, saving model to ./checkpoints/20fold_BN2_32-32u_32ws_12f/m_0002_0.1838_0.2174.hdf5
Epoch 3/100
157060/157060 [==============================] - 128s 813us/step - loss: 0.1717 - val_loss: 0.2341
[last] - val_bacc: 0.7512 - val_f1: 0.6446 - val_precision: 0.8553 - val_recall: 0.5172 
[majority] - val_bacc: 0.7509 - val_f1: 0.6471 - val_precision: 0.8712 - val_recall: 0.5147 

Epoch 00003: val_loss did not improve from 0.21741
Epoch 4/100
157060/157060 [==============================] - 130s 827us/step - loss: 0.1628 - val_loss: 0.2379
[last] - val_bacc: 0.7680 - val_f1: 0.6661 - val_precision: 0.8330 - val_recall: 0.5549 
[majority] - val_bacc: 0.7693 - val_f1: 0.6738 - val_precision: 0.8593 - val_recall: 0.5542 

Epoch 00004: val_loss did not improve from 0.21741
Epoch 5/100
157060/157060 [==============================] - 131s 834us/step - loss: 0.1557 - val_loss: 0.2352
[last] - val_bacc: 0.7773 - val_f1: 0.6756 - val_precision: 0.8148 - val_recall: 0.5770 
[majority] - val_bacc: 0.7720 - val_f1: 0.6730 - val_precision: 0.8373 - val_recall: 0.5626 

Epoch 00005: val_loss did not improve from 0.21741
Epoch 6/100
157060/157060 [==============================] - 130s 828us/step - loss: 0.1500 - val_loss: 0.2566
[last] - val_bacc: 0.7385 - val_f1: 0.6256 - val_precision: 0.8652 - val_recall: 0.4900 
[majority] - val_bacc: 0.7389 - val_f1: 0.6318 - val_precision: 0.8985 - val_recall: 0.4872 

Epoch 00006: val_loss did not improve from 0.21741
Epoch 7/100
157060/157060 [==============================] - 132s 838us/step - loss: 0.1456 - val_loss: 0.2461
[last] - val_bacc: 0.7737 - val_f1: 0.6734 - val_precision: 0.8279 - val_recall: 0.5675 
[majority] - val_bacc: 0.7663 - val_f1: 0.6672 - val_precision: 0.8497 - val_recall: 0.5492 

Epoch 00007: val_loss did not improve from 0.21741
Epoch 8/100
157060/157060 [==============================] - 131s 834us/step - loss: 0.1416 - val_loss: 0.2326
[last] - val_bacc: 0.8055 - val_f1: 0.7055 - val_precision: 0.7842 - val_recall: 0.6411 
[majority] - val_bacc: 0.7976 - val_f1: 0.7039 - val_precision: 0.8155 - val_recall: 0.6191 

Epoch 00008: val_loss did not improve from 0.21741
Epoch 9/100
157060/157060 [==============================] - 131s 836us/step - loss: 0.1377 - val_loss: 0.2449
[last] - val_bacc: 0.7795 - val_f1: 0.6804 - val_precision: 0.8220 - val_recall: 0.5804 
[majority] - val_bacc: 0.7735 - val_f1: 0.6746 - val_precision: 0.8348 - val_recall: 0.5660 

Epoch 00009: val_loss did not improve from 0.21741
Epoch 10/100
157060/157060 [==============================] - 104s 664us/step - loss: 0.1339 - val_loss: 0.2537
[last] - val_bacc: 0.7736 - val_f1: 0.6755 - val_precision: 0.8381 - val_recall: 0.5658 
[majority] - val_bacc: 0.7685 - val_f1: 0.6712 - val_precision: 0.8526 - val_recall: 0.5534 

Epoch 00010: val_loss did not improve from 0.21741
Epoch 11/100
157060/157060 [==============================] - 92s 587us/step - loss: 0.1308 - val_loss: 0.2629
[last] - val_bacc: 0.7598 - val_f1: 0.6532 - val_precision: 0.8306 - val_recall: 0.5383 
[majority] - val_bacc: 0.7556 - val_f1: 0.6502 - val_precision: 0.8475 - val_recall: 0.5274 

Epoch 00011: val_loss did not improve from 0.21741
Epoch 12/100
157060/157060 [==============================] - 93s 590us/step - loss: 0.1284 - val_loss: 0.2464
[last] - val_bacc: 0.8027 - val_f1: 0.7070 - val_precision: 0.8021 - val_recall: 0.6320 
[majority] - val_bacc: 0.7900 - val_f1: 0.6954 - val_precision: 0.8231 - val_recall: 0.6020 

Epoch 00012: val_loss did not improve from 0.21741
Epoch 00012: early stopping
	Min train loss: 0.12836139536184382 @epoch 11
	Min valid loss: 0.21740635145870016 @epoch 1

<Figure size 1000x600 with 1 Axes>

INFO:
	21fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 159014 samples, validate on 28033 samples
Epoch 1/100
159014/159014 [==============================] - 109s 687us/step - loss: 0.2482 - val_loss: 0.1740
[last] - val_bacc: 0.8053 - val_f1: 0.7124 - val_precision: 0.8204 - val_recall: 0.6296 
[majority] - val_bacc: 0.8027 - val_f1: 0.7174 - val_precision: 0.8503 - val_recall: 0.6204 

Epoch 00001: val_loss improved from inf to 0.17399, saving model to ./checkpoints/21fold_BN2_32-32u_32ws_12f/m_0001_0.2482_0.1740.hdf5
Epoch 2/100
159014/159014 [==============================] - 96s 605us/step - loss: 0.1905 - val_loss: 0.1723
[last] - val_bacc: 0.8145 - val_f1: 0.7216 - val_precision: 0.8116 - val_recall: 0.6496 
[majority] - val_bacc: 0.8136 - val_f1: 0.7265 - val_precision: 0.8312 - val_recall: 0.6452 

Epoch 00002: val_loss improved from 0.17399 to 0.17230, saving model to ./checkpoints/21fold_BN2_32-32u_32ws_12f/m_0002_0.1905_0.1723.hdf5
Epoch 3/100
159014/159014 [==============================] - 101s 638us/step - loss: 0.1802 - val_loss: 0.1734
[last] - val_bacc: 0.8321 - val_f1: 0.7197 - val_precision: 0.7437 - val_recall: 0.6972 
[majority] - val_bacc: 0.8284 - val_f1: 0.7370 - val_precision: 0.8052 - val_recall: 0.6795 

Epoch 00003: val_loss did not improve from 0.17230
Epoch 4/100
159014/159014 [==============================] - 112s 703us/step - loss: 0.1720 - val_loss: 0.1756
[last] - val_bacc: 0.8105 - val_f1: 0.7091 - val_precision: 0.7874 - val_recall: 0.6449 
[majority] - val_bacc: 0.8142 - val_f1: 0.7287 - val_precision: 0.8361 - val_recall: 0.6458 

Epoch 00004: val_loss did not improve from 0.17230
Epoch 5/100
159014/159014 [==============================] - 116s 732us/step - loss: 0.1652 - val_loss: 0.1824
[last] - val_bacc: 0.8145 - val_f1: 0.7102 - val_precision: 0.7756 - val_recall: 0.6550 
[majority] - val_bacc: 0.8154 - val_f1: 0.7289 - val_precision: 0.8316 - val_recall: 0.6488 

Epoch 00005: val_loss did not improve from 0.17230
Epoch 6/100
159014/159014 [==============================] - 124s 783us/step - loss: 0.1591 - val_loss: 0.1825
[last] - val_bacc: 0.8174 - val_f1: 0.7127 - val_precision: 0.7720 - val_recall: 0.6617 
[majority] - val_bacc: 0.8208 - val_f1: 0.7300 - val_precision: 0.8125 - val_recall: 0.6626 

Epoch 00006: val_loss did not improve from 0.17230
Epoch 7/100
159014/159014 [==============================] - 123s 773us/step - loss: 0.1551 - val_loss: 0.1823
[last] - val_bacc: 0.8229 - val_f1: 0.7256 - val_precision: 0.7912 - val_recall: 0.6700 
[majority] - val_bacc: 0.8240 - val_f1: 0.7378 - val_precision: 0.8243 - val_recall: 0.6677 

Epoch 00007: val_loss did not improve from 0.17230
Epoch 8/100
159014/159014 [==============================] - 124s 778us/step - loss: 0.1507 - val_loss: 0.1982
[last] - val_bacc: 0.7889 - val_f1: 0.6767 - val_precision: 0.7724 - val_recall: 0.6021 
[majority] - val_bacc: 0.7988 - val_f1: 0.7022 - val_precision: 0.8148 - val_recall: 0.6169 

Epoch 00008: val_loss did not improve from 0.17230
Epoch 9/100
159014/159014 [==============================] - 122s 766us/step - loss: 0.1473 - val_loss: 0.1922
[last] - val_bacc: 0.8051 - val_f1: 0.6877 - val_precision: 0.7420 - val_recall: 0.6408 
[majority] - val_bacc: 0.8114 - val_f1: 0.7122 - val_precision: 0.7939 - val_recall: 0.6458 

Epoch 00009: val_loss did not improve from 0.17230
Epoch 10/100
159014/159014 [==============================] - 124s 780us/step - loss: 0.1434 - val_loss: 0.1921
[last] - val_bacc: 0.8067 - val_f1: 0.6962 - val_precision: 0.7616 - val_recall: 0.6411 
[majority] - val_bacc: 0.8108 - val_f1: 0.7166 - val_precision: 0.8104 - val_recall: 0.6423 

Epoch 00010: val_loss did not improve from 0.17230
Epoch 11/100
159014/159014 [==============================] - 123s 772us/step - loss: 0.1409 - val_loss: 0.2006
[last] - val_bacc: 0.7861 - val_f1: 0.6756 - val_precision: 0.7814 - val_recall: 0.5950 
[majority] - val_bacc: 0.7915 - val_f1: 0.6939 - val_precision: 0.8210 - val_recall: 0.6009 

Epoch 00011: val_loss did not improve from 0.17230
Epoch 12/100
159014/159014 [==============================] - 123s 773us/step - loss: 0.1382 - val_loss: 0.2021
[last] - val_bacc: 0.7804 - val_f1: 0.6678 - val_precision: 0.7810 - val_recall: 0.5832 
[majority] - val_bacc: 0.7860 - val_f1: 0.6837 - val_precision: 0.8112 - val_recall: 0.5909 

Epoch 00012: val_loss did not improve from 0.17230
Epoch 00012: early stopping
	Min train loss: 0.13816541252549513 @epoch 11
	Min valid loss: 0.17229850237513825 @epoch 1

<Figure size 1000x600 with 1 Axes>

INFO:
	22fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 159042 samples, validate on 28038 samples
Epoch 1/100
159042/159042 [==============================] - 132s 830us/step - loss: 0.2459 - val_loss: 0.1929
[last] - val_bacc: 0.7881 - val_f1: 0.6493 - val_precision: 0.6952 - val_recall: 0.6091 
[majority] - val_bacc: 0.7797 - val_f1: 0.6439 - val_precision: 0.7101 - val_recall: 0.5889 

Epoch 00001: val_loss improved from inf to 0.19293, saving model to ./checkpoints/22fold_BN2_32-32u_32ws_12f/m_0001_0.2459_0.1929.hdf5
Epoch 2/100
159042/159042 [==============================] - 123s 776us/step - loss: 0.1856 - val_loss: 0.2000
[last] - val_bacc: 0.8126 - val_f1: 0.6606 - val_precision: 0.6523 - val_recall: 0.6691 
[majority] - val_bacc: 0.7969 - val_f1: 0.6546 - val_precision: 0.6812 - val_recall: 0.6300 

Epoch 00002: val_loss did not improve from 0.19293
Epoch 3/100
159042/159042 [==============================] - 125s 785us/step - loss: 0.1735 - val_loss: 0.1957
[last] - val_bacc: 0.8221 - val_f1: 0.6653 - val_precision: 0.6407 - val_recall: 0.6919 
[majority] - val_bacc: 0.8118 - val_f1: 0.6723 - val_precision: 0.6837 - val_recall: 0.6612 

Epoch 00003: val_loss did not improve from 0.19293
Epoch 4/100
159042/159042 [==============================] - 123s 775us/step - loss: 0.1654 - val_loss: 0.1928
[last] - val_bacc: 0.8100 - val_f1: 0.6759 - val_precision: 0.6984 - val_recall: 0.6547 
[majority] - val_bacc: 0.8018 - val_f1: 0.6730 - val_precision: 0.7164 - val_recall: 0.6345 

Epoch 00004: val_loss improved from 0.19293 to 0.19285, saving model to ./checkpoints/22fold_BN2_32-32u_32ws_12f/m_0004_0.1654_0.1928.hdf5
Epoch 5/100
159042/159042 [==============================] - 123s 775us/step - loss: 0.1573 - val_loss: 0.1983
[last] - val_bacc: 0.7905 - val_f1: 0.6533 - val_precision: 0.6983 - val_recall: 0.6137 
[majority] - val_bacc: 0.7833 - val_f1: 0.6482 - val_precision: 0.7093 - val_recall: 0.5967 

Epoch 00005: val_loss did not improve from 0.19285
Epoch 6/100
159042/159042 [==============================] - 123s 776us/step - loss: 0.1519 - val_loss: 0.2140
[last] - val_bacc: 0.8250 - val_f1: 0.6541 - val_precision: 0.6096 - val_recall: 0.7055 
[majority] - val_bacc: 0.8201 - val_f1: 0.6637 - val_precision: 0.6417 - val_recall: 0.6873 

Epoch 00006: val_loss did not improve from 0.19285
Epoch 7/100
159042/159042 [==============================] - 123s 774us/step - loss: 0.1473 - val_loss: 0.2173
[last] - val_bacc: 0.8056 - val_f1: 0.6359 - val_precision: 0.6107 - val_recall: 0.6632 
[majority] - val_bacc: 0.8018 - val_f1: 0.6460 - val_precision: 0.6444 - val_recall: 0.6476 

Epoch 00007: val_loss did not improve from 0.19285
Epoch 8/100
159042/159042 [==============================] - 124s 782us/step - loss: 0.1432 - val_loss: 0.2133
[last] - val_bacc: 0.8153 - val_f1: 0.6553 - val_precision: 0.6334 - val_recall: 0.6788 
[majority] - val_bacc: 0.8116 - val_f1: 0.6644 - val_precision: 0.6643 - val_recall: 0.6645 

Epoch 00008: val_loss did not improve from 0.19285
Epoch 9/100
159042/159042 [==============================] - 124s 781us/step - loss: 0.1389 - val_loss: 0.2223
[last] - val_bacc: 0.7956 - val_f1: 0.6374 - val_precision: 0.6396 - val_recall: 0.6352 
[majority] - val_bacc: 0.7908 - val_f1: 0.6442 - val_precision: 0.6716 - val_recall: 0.6189 

Epoch 00009: val_loss did not improve from 0.19285
Epoch 10/100
159042/159042 [==============================] - 124s 781us/step - loss: 0.1363 - val_loss: 0.2200
[last] - val_bacc: 0.7901 - val_f1: 0.6391 - val_precision: 0.6600 - val_recall: 0.6195 
[majority] - val_bacc: 0.7859 - val_f1: 0.6408 - val_precision: 0.6783 - val_recall: 0.6072 

Epoch 00010: val_loss did not improve from 0.19285
Epoch 11/100
159042/159042 [==============================] - 123s 775us/step - loss: 0.1342 - val_loss: 0.2387
[last] - val_bacc: 0.8025 - val_f1: 0.6257 - val_precision: 0.5946 - val_recall: 0.6603 
[majority] - val_bacc: 0.7978 - val_f1: 0.6357 - val_precision: 0.6295 - val_recall: 0.6420 

Epoch 00011: val_loss did not improve from 0.19285
Epoch 12/100
159042/159042 [==============================] - 122s 765us/step - loss: 0.1309 - val_loss: 0.2407
[last] - val_bacc: 0.8202 - val_f1: 0.6366 - val_precision: 0.5822 - val_recall: 0.7023 
[majority] - val_bacc: 0.8182 - val_f1: 0.6536 - val_precision: 0.6227 - val_recall: 0.6876 

Epoch 00012: val_loss did not improve from 0.19285
Epoch 13/100
159042/159042 [==============================] - 116s 731us/step - loss: 0.1290 - val_loss: 0.2317
[last] - val_bacc: 0.7961 - val_f1: 0.6359 - val_precision: 0.6344 - val_recall: 0.6375 
[majority] - val_bacc: 0.7946 - val_f1: 0.6447 - val_precision: 0.6616 - val_recall: 0.6287 

Epoch 00013: val_loss did not improve from 0.19285
Epoch 14/100
159042/159042 [==============================] - 117s 734us/step - loss: 0.1268 - val_loss: 0.2339
[last] - val_bacc: 0.7889 - val_f1: 0.6298 - val_precision: 0.6390 - val_recall: 0.6208 
[majority] - val_bacc: 0.7938 - val_f1: 0.6444 - val_precision: 0.6632 - val_recall: 0.6267 

Epoch 00014: val_loss did not improve from 0.19285
Epoch 00014: early stopping
	Min train loss: 0.12681265708775685 @epoch 13
	Min valid loss: 0.1928497939957507 @epoch 3

<Figure size 1000x600 with 1 Axes>

INFO:
	23fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 160924 samples, validate on 28370 samples
Epoch 1/100
160924/160924 [==============================] - 142s 884us/step - loss: 0.2453 - val_loss: 0.1687
[last] - val_bacc: 0.8211 - val_f1: 0.7064 - val_precision: 0.7470 - val_recall: 0.6700 
[majority] - val_bacc: 0.8188 - val_f1: 0.7109 - val_precision: 0.7674 - val_recall: 0.6623 

Epoch 00001: val_loss improved from inf to 0.16871, saving model to ./checkpoints/23fold_BN2_32-32u_32ws_12f/m_0001_0.2453_0.1687.hdf5
Epoch 2/100
160924/160924 [==============================] - 134s 832us/step - loss: 0.1870 - val_loss: 0.1727
[last] - val_bacc: 0.8066 - val_f1: 0.7043 - val_precision: 0.7927 - val_recall: 0.6336 
[majority] - val_bacc: 0.8032 - val_f1: 0.7049 - val_precision: 0.8088 - val_recall: 0.6246 

Epoch 00002: val_loss did not improve from 0.16871
Epoch 3/100
160924/160924 [==============================] - 134s 831us/step - loss: 0.1751 - val_loss: 0.1733
[last] - val_bacc: 0.8267 - val_f1: 0.6983 - val_precision: 0.7088 - val_recall: 0.6880 
[majority] - val_bacc: 0.8201 - val_f1: 0.7036 - val_precision: 0.7424 - val_recall: 0.6687 

Epoch 00003: val_loss did not improve from 0.16871
Epoch 4/100
160924/160924 [==============================] - 133s 826us/step - loss: 0.1674 - val_loss: 0.1765
[last] - val_bacc: 0.7912 - val_f1: 0.6795 - val_precision: 0.7772 - val_recall: 0.6036 
[majority] - val_bacc: 0.7907 - val_f1: 0.6832 - val_precision: 0.7918 - val_recall: 0.6007 

Epoch 00004: val_loss did not improve from 0.16871
Epoch 5/100
160924/160924 [==============================] - 133s 824us/step - loss: 0.1596 - val_loss: 0.1777
[last] - val_bacc: 0.8356 - val_f1: 0.7072 - val_precision: 0.7073 - val_recall: 0.7071 
[majority] - val_bacc: 0.8307 - val_f1: 0.7139 - val_precision: 0.7377 - val_recall: 0.6916 

Epoch 00005: val_loss did not improve from 0.16871
Epoch 6/100
160924/160924 [==============================] - 133s 825us/step - loss: 0.1534 - val_loss: 0.1793
[last] - val_bacc: 0.8124 - val_f1: 0.6886 - val_precision: 0.7256 - val_recall: 0.6552 
[majority] - val_bacc: 0.8116 - val_f1: 0.6944 - val_precision: 0.7445 - val_recall: 0.6507 

Epoch 00006: val_loss did not improve from 0.16871
Epoch 7/100
160924/160924 [==============================] - 134s 833us/step - loss: 0.1481 - val_loss: 0.1766
[last] - val_bacc: 0.8221 - val_f1: 0.7100 - val_precision: 0.7533 - val_recall: 0.6713 
[majority] - val_bacc: 0.8196 - val_f1: 0.7157 - val_precision: 0.7784 - val_recall: 0.6623 

Epoch 00007: val_loss did not improve from 0.16871
Epoch 8/100
160924/160924 [==============================] - 136s 845us/step - loss: 0.1448 - val_loss: 0.1885
[last] - val_bacc: 0.8204 - val_f1: 0.6791 - val_precision: 0.6776 - val_recall: 0.6806 
[majority] - val_bacc: 0.8216 - val_f1: 0.6947 - val_precision: 0.7139 - val_recall: 0.6764 

Epoch 00008: val_loss did not improve from 0.16871
Epoch 9/100
160924/160924 [==============================] - 136s 844us/step - loss: 0.1402 - val_loss: 0.1872
[last] - val_bacc: 0.8045 - val_f1: 0.6748 - val_precision: 0.7128 - val_recall: 0.6407 
[majority] - val_bacc: 0.8122 - val_f1: 0.6977 - val_precision: 0.7520 - val_recall: 0.6507 

Epoch 00009: val_loss did not improve from 0.16871
Epoch 10/100
160924/160924 [==============================] - 133s 827us/step - loss: 0.1372 - val_loss: 0.1921
[last] - val_bacc: 0.8096 - val_f1: 0.6766 - val_precision: 0.7016 - val_recall: 0.6532 
[majority] - val_bacc: 0.8128 - val_f1: 0.6971 - val_precision: 0.7481 - val_recall: 0.6526 

Epoch 00010: val_loss did not improve from 0.16871
Epoch 11/100
160924/160924 [==============================] - 134s 831us/step - loss: 0.1337 - val_loss: 0.1923
[last] - val_bacc: 0.7962 - val_f1: 0.6736 - val_precision: 0.7381 - val_recall: 0.6194 
[majority] - val_bacc: 0.7990 - val_f1: 0.6839 - val_precision: 0.7590 - val_recall: 0.6223 

Epoch 00011: val_loss did not improve from 0.16871
Epoch 00011: early stopping
	Min train loss: 0.13372754893373054 @epoch 10
	Min valid loss: 0.16870999187034097 @epoch 0

<Figure size 1000x600 with 1 Axes>

INFO:
	24fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 160113 samples, validate on 28227 samples
Epoch 1/100
160113/160113 [==============================] - 143s 894us/step - loss: 0.2365 - val_loss: 0.1942
[last] - val_bacc: 0.7975 - val_f1: 0.7008 - val_precision: 0.8118 - val_recall: 0.6166 
[majority] - val_bacc: 0.7941 - val_f1: 0.6997 - val_precision: 0.8246 - val_recall: 0.6077 

Epoch 00001: val_loss improved from inf to 0.19419, saving model to ./checkpoints/24fold_BN2_32-32u_32ws_12f/m_0001_0.2365_0.1942.hdf5
Epoch 2/100
160113/160113 [==============================] - 135s 845us/step - loss: 0.1775 - val_loss: 0.1841
[last] - val_bacc: 0.8175 - val_f1: 0.7199 - val_precision: 0.7891 - val_recall: 0.6618 
[majority] - val_bacc: 0.8080 - val_f1: 0.7182 - val_precision: 0.8240 - val_recall: 0.6365 

Epoch 00002: val_loss improved from 0.19419 to 0.18409, saving model to ./checkpoints/24fold_BN2_32-32u_32ws_12f/m_0002_0.1775_0.1841.hdf5
Epoch 3/100
160113/160113 [==============================] - 134s 836us/step - loss: 0.1664 - val_loss: 0.1847
[last] - val_bacc: 0.8235 - val_f1: 0.7321 - val_precision: 0.8043 - val_recall: 0.6718 
[majority] - val_bacc: 0.8140 - val_f1: 0.7281 - val_precision: 0.8306 - val_recall: 0.6481 

Epoch 00003: val_loss did not improve from 0.18409
Epoch 4/100
160113/160113 [==============================] - 133s 828us/step - loss: 0.1574 - val_loss: 0.1872
[last] - val_bacc: 0.8222 - val_f1: 0.7265 - val_precision: 0.7917 - val_recall: 0.6712 
[majority] - val_bacc: 0.8128 - val_f1: 0.7230 - val_precision: 0.8188 - val_recall: 0.6473 

Epoch 00004: val_loss did not improve from 0.18409
Epoch 5/100
160113/160113 [==============================] - 135s 841us/step - loss: 0.1502 - val_loss: 0.1939
[last] - val_bacc: 0.8260 - val_f1: 0.7240 - val_precision: 0.7704 - val_recall: 0.6828 
[majority] - val_bacc: 0.8123 - val_f1: 0.7186 - val_precision: 0.8059 - val_recall: 0.6484 

Epoch 00005: val_loss did not improve from 0.18409
Epoch 6/100
160113/160113 [==============================] - 135s 843us/step - loss: 0.1444 - val_loss: 0.1936
[last] - val_bacc: 0.8316 - val_f1: 0.7238 - val_precision: 0.7514 - val_recall: 0.6982 
[majority] - val_bacc: 0.8245 - val_f1: 0.7282 - val_precision: 0.7883 - val_recall: 0.6766 

Epoch 00006: val_loss did not improve from 0.18409
Epoch 7/100
160113/160113 [==============================] - 133s 832us/step - loss: 0.1402 - val_loss: 0.1993
[last] - val_bacc: 0.8223 - val_f1: 0.7226 - val_precision: 0.7796 - val_recall: 0.6734 
[majority] - val_bacc: 0.8121 - val_f1: 0.7209 - val_precision: 0.8147 - val_recall: 0.6465 

Epoch 00007: val_loss did not improve from 0.18409
Epoch 8/100
160113/160113 [==============================] - 135s 841us/step - loss: 0.1363 - val_loss: 0.1984
[last] - val_bacc: 0.8323 - val_f1: 0.7347 - val_precision: 0.7803 - val_recall: 0.6941 
[majority] - val_bacc: 0.8251 - val_f1: 0.7345 - val_precision: 0.8058 - val_recall: 0.6747 

Epoch 00008: val_loss did not improve from 0.18409
Epoch 9/100
160113/160113 [==============================] - 135s 843us/step - loss: 0.1316 - val_loss: 0.2194
[last] - val_bacc: 0.7828 - val_f1: 0.6704 - val_precision: 0.7731 - val_recall: 0.5918 
[majority] - val_bacc: 0.7713 - val_f1: 0.6660 - val_precision: 0.8186 - val_recall: 0.5614 

Epoch 00009: val_loss did not improve from 0.18409
Epoch 10/100
160113/160113 [==============================] - 133s 829us/step - loss: 0.1288 - val_loss: 0.2089
[last] - val_bacc: 0.8150 - val_f1: 0.7095 - val_precision: 0.7663 - val_recall: 0.6605 
[majority] - val_bacc: 0.8052 - val_f1: 0.7077 - val_precision: 0.8003 - val_recall: 0.6344 

Epoch 00010: val_loss did not improve from 0.18409
Epoch 11/100
160113/160113 [==============================] - 132s 827us/step - loss: 0.1260 - val_loss: 0.2125
[last] - val_bacc: 0.8009 - val_f1: 0.7055 - val_precision: 0.8121 - val_recall: 0.6236 
[majority] - val_bacc: 0.7921 - val_f1: 0.6997 - val_precision: 0.8347 - val_recall: 0.6023 

Epoch 00011: val_loss did not improve from 0.18409
Epoch 12/100
160113/160113 [==============================] - 134s 837us/step - loss: 0.1236 - val_loss: 0.2080
[last] - val_bacc: 0.8307 - val_f1: 0.7309 - val_precision: 0.7746 - val_recall: 0.6920 
[majority] - val_bacc: 0.8164 - val_f1: 0.7211 - val_precision: 0.7974 - val_recall: 0.6581 

Epoch 00012: val_loss did not improve from 0.18409
Epoch 00012: early stopping
	Min train loss: 0.12360293198183013 @epoch 11
	Min valid loss: 0.18408967294214978 @epoch 1

<Figure size 1000x600 with 1 Axes>

INFO:
	25fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 159990 samples, validate on 28206 samples
Epoch 1/100
159990/159990 [==============================] - 143s 895us/step - loss: 0.2430 - val_loss: 0.1840
[last] - val_bacc: 0.8107 - val_f1: 0.6600 - val_precision: 0.6603 - val_recall: 0.6598 
[majority] - val_bacc: 0.8020 - val_f1: 0.6617 - val_precision: 0.6890 - val_recall: 0.6365 

Epoch 00001: val_loss improved from inf to 0.18401, saving model to ./checkpoints/25fold_BN2_32-32u_32ws_12f/m_0001_0.2430_0.1840.hdf5
Epoch 2/100
159990/159990 [==============================] - 134s 838us/step - loss: 0.1807 - val_loss: 0.1804
[last] - val_bacc: 0.7959 - val_f1: 0.6572 - val_precision: 0.6960 - val_recall: 0.6226 
[majority] - val_bacc: 0.7898 - val_f1: 0.6606 - val_precision: 0.7267 - val_recall: 0.6055 

Epoch 00002: val_loss improved from 0.18401 to 0.18043, saving model to ./checkpoints/25fold_BN2_32-32u_32ws_12f/m_0002_0.1807_0.1804.hdf5
Epoch 3/100
159990/159990 [==============================] - 135s 847us/step - loss: 0.1692 - val_loss: 0.1820
[last] - val_bacc: 0.7931 - val_f1: 0.6548 - val_precision: 0.6985 - val_recall: 0.6163 
[majority] - val_bacc: 0.7843 - val_f1: 0.6553 - val_precision: 0.7316 - val_recall: 0.5933 

Epoch 00003: val_loss did not improve from 0.18043
Epoch 4/100
159990/159990 [==============================] - 135s 842us/step - loss: 0.1608 - val_loss: 0.1846
[last] - val_bacc: 0.8107 - val_f1: 0.6747 - val_precision: 0.6973 - val_recall: 0.6536 
[majority] - val_bacc: 0.7993 - val_f1: 0.6720 - val_precision: 0.7262 - val_recall: 0.6253 

Epoch 00004: val_loss did not improve from 0.18043
Epoch 5/100
159990/159990 [==============================] - 134s 838us/step - loss: 0.1542 - val_loss: 0.1857
[last] - val_bacc: 0.8121 - val_f1: 0.6743 - val_precision: 0.6921 - val_recall: 0.6574 
[majority] - val_bacc: 0.8072 - val_f1: 0.6809 - val_precision: 0.7248 - val_recall: 0.6421 

Epoch 00005: val_loss did not improve from 0.18043
Epoch 6/100
159990/159990 [==============================] - 134s 840us/step - loss: 0.1493 - val_loss: 0.1964
[last] - val_bacc: 0.7908 - val_f1: 0.6449 - val_precision: 0.6783 - val_recall: 0.6146 
[majority] - val_bacc: 0.7830 - val_f1: 0.6503 - val_precision: 0.7216 - val_recall: 0.5919 

Epoch 00006: val_loss did not improve from 0.18043
Epoch 7/100
159990/159990 [==============================] - 133s 833us/step - loss: 0.1459 - val_loss: 0.1870
[last] - val_bacc: 0.8137 - val_f1: 0.6756 - val_precision: 0.6909 - val_recall: 0.6609 
[majority] - val_bacc: 0.8013 - val_f1: 0.6753 - val_precision: 0.7286 - val_recall: 0.6292 

Epoch 00007: val_loss did not improve from 0.18043
Epoch 8/100
159990/159990 [==============================] - 136s 848us/step - loss: 0.1421 - val_loss: 0.2009
[last] - val_bacc: 0.8132 - val_f1: 0.6661 - val_precision: 0.6685 - val_recall: 0.6636 
[majority] - val_bacc: 0.8005 - val_f1: 0.6690 - val_precision: 0.7138 - val_recall: 0.6295 

Epoch 00008: val_loss did not improve from 0.18043
Epoch 9/100
159990/159990 [==============================] - 134s 838us/step - loss: 0.1390 - val_loss: 0.2074
[last] - val_bacc: 0.7746 - val_f1: 0.6293 - val_precision: 0.6896 - val_recall: 0.5787 
[majority] - val_bacc: 0.7748 - val_f1: 0.6402 - val_precision: 0.7227 - val_recall: 0.5745 

Epoch 00009: val_loss did not improve from 0.18043
Epoch 10/100
159990/159990 [==============================] - 131s 820us/step - loss: 0.1357 - val_loss: 0.2003
[last] - val_bacc: 0.7957 - val_f1: 0.6595 - val_precision: 0.7029 - val_recall: 0.6212 
[majority] - val_bacc: 0.7900 - val_f1: 0.6634 - val_precision: 0.7345 - val_recall: 0.6048 

Epoch 00010: val_loss did not improve from 0.18043
Epoch 11/100
159990/159990 [==============================] - 136s 849us/step - loss: 0.1328 - val_loss: 0.2099
[last] - val_bacc: 0.7958 - val_f1: 0.6541 - val_precision: 0.6878 - val_recall: 0.6236 
[majority] - val_bacc: 0.7907 - val_f1: 0.6584 - val_precision: 0.7170 - val_recall: 0.6086 

Epoch 00011: val_loss did not improve from 0.18043
Epoch 12/100
159990/159990 [==============================] - 134s 838us/step - loss: 0.1314 - val_loss: 0.2055
[last] - val_bacc: 0.7943 - val_f1: 0.6556 - val_precision: 0.6967 - val_recall: 0.6191 
[majority] - val_bacc: 0.7919 - val_f1: 0.6643 - val_precision: 0.7301 - val_recall: 0.6093 

Epoch 00012: val_loss did not improve from 0.18043
Epoch 00012: early stopping
	Min train loss: 0.13139081323530177 @epoch 11
	Min valid loss: 0.18043044082791226 @epoch 1

<Figure size 1000x600 with 1 Axes>

INFO:
	26fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 160641 samples, validate on 28320 samples
Epoch 1/100
160641/160641 [==============================] - 147s 914us/step - loss: 0.2428 - val_loss: 0.1990
[last] - val_bacc: 0.8333 - val_f1: 0.7221 - val_precision: 0.7391 - val_recall: 0.7058 
[majority] - val_bacc: 0.8262 - val_f1: 0.7255 - val_precision: 0.7722 - val_recall: 0.6842 

Epoch 00001: val_loss improved from inf to 0.19898, saving model to ./checkpoints/26fold_BN2_32-32u_32ws_12f/m_0001_0.2428_0.1990.hdf5
Epoch 2/100
160641/160641 [==============================] - 136s 845us/step - loss: 0.1856 - val_loss: 0.1969
[last] - val_bacc: 0.8253 - val_f1: 0.7218 - val_precision: 0.7644 - val_recall: 0.6837 
[majority] - val_bacc: 0.8177 - val_f1: 0.7205 - val_precision: 0.7882 - val_recall: 0.6635 

Epoch 00002: val_loss improved from 0.19898 to 0.19693, saving model to ./checkpoints/26fold_BN2_32-32u_32ws_12f/m_0002_0.1856_0.1969.hdf5
Epoch 3/100
160641/160641 [==============================] - 134s 832us/step - loss: 0.1737 - val_loss: 0.2034
[last] - val_bacc: 0.8345 - val_f1: 0.7201 - val_precision: 0.7299 - val_recall: 0.7104 
[majority] - val_bacc: 0.8204 - val_f1: 0.7170 - val_precision: 0.7670 - val_recall: 0.6731 

Epoch 00003: val_loss did not improve from 0.19693
Epoch 4/100
160641/160641 [==============================] - 135s 839us/step - loss: 0.1656 - val_loss: 0.1976
[last] - val_bacc: 0.8179 - val_f1: 0.7106 - val_precision: 0.7569 - val_recall: 0.6697 
[majority] - val_bacc: 0.8105 - val_f1: 0.7129 - val_precision: 0.7929 - val_recall: 0.6476 

Epoch 00004: val_loss did not improve from 0.19693
Epoch 5/100
160641/160641 [==============================] - 134s 834us/step - loss: 0.1596 - val_loss: 0.2020
[last] - val_bacc: 0.8239 - val_f1: 0.7231 - val_precision: 0.7730 - val_recall: 0.6793 
[majority] - val_bacc: 0.8123 - val_f1: 0.7191 - val_precision: 0.8058 - val_recall: 0.6492 

Epoch 00005: val_loss did not improve from 0.19693
Epoch 6/100
160641/160641 [==============================] - 135s 842us/step - loss: 0.1538 - val_loss: 0.2009
[last] - val_bacc: 0.8145 - val_f1: 0.7113 - val_precision: 0.7715 - val_recall: 0.6598 
[majority] - val_bacc: 0.8077 - val_f1: 0.7091 - val_precision: 0.7919 - val_recall: 0.6419 

Epoch 00006: val_loss did not improve from 0.19693
Epoch 7/100
160641/160641 [==============================] - 138s 857us/step - loss: 0.1488 - val_loss: 0.2061
[last] - val_bacc: 0.8092 - val_f1: 0.7088 - val_precision: 0.7846 - val_recall: 0.6463 
[majority] - val_bacc: 0.8037 - val_f1: 0.7092 - val_precision: 0.8098 - val_recall: 0.6308 

Epoch 00007: val_loss did not improve from 0.19693
Epoch 8/100
160641/160641 [==============================] - 136s 846us/step - loss: 0.1452 - val_loss: 0.2149
[last] - val_bacc: 0.7977 - val_f1: 0.6901 - val_precision: 0.7710 - val_recall: 0.6245 
[majority] - val_bacc: 0.7983 - val_f1: 0.6997 - val_precision: 0.8014 - val_recall: 0.6209 

Epoch 00008: val_loss did not improve from 0.19693
Epoch 9/100
160641/160641 [==============================] - 135s 839us/step - loss: 0.1418 - val_loss: 0.2112
[last] - val_bacc: 0.8439 - val_f1: 0.7214 - val_precision: 0.7077 - val_recall: 0.7356 
[majority] - val_bacc: 0.8307 - val_f1: 0.7237 - val_precision: 0.7517 - val_recall: 0.6977 

Epoch 00009: val_loss did not improve from 0.19693
Epoch 10/100
160641/160641 [==============================] - 135s 838us/step - loss: 0.1379 - val_loss: 0.2194
[last] - val_bacc: 0.8612 - val_f1: 0.7324 - val_precision: 0.6930 - val_recall: 0.7766 
[majority] - val_bacc: 0.8435 - val_f1: 0.7293 - val_precision: 0.7291 - val_recall: 0.7296 

Epoch 00010: val_loss did not improve from 0.19693
Epoch 11/100
160641/160641 [==============================] - 134s 837us/step - loss: 0.1364 - val_loss: 0.2184
[last] - val_bacc: 0.8395 - val_f1: 0.7243 - val_precision: 0.7271 - val_recall: 0.7216 
[majority] - val_bacc: 0.8283 - val_f1: 0.7287 - val_precision: 0.7743 - val_recall: 0.6881 

Epoch 00011: val_loss did not improve from 0.19693
Epoch 12/100
160641/160641 [==============================] - 133s 826us/step - loss: 0.1340 - val_loss: 0.2228
[last] - val_bacc: 0.8369 - val_f1: 0.7169 - val_precision: 0.7147 - val_recall: 0.7190 
[majority] - val_bacc: 0.8238 - val_f1: 0.7129 - val_precision: 0.7434 - val_recall: 0.6847 

Epoch 00012: val_loss did not improve from 0.19693
Epoch 00012: early stopping
	Min train loss: 0.13402554466693845 @epoch 11
	Min valid loss: 0.1969304118573792 @epoch 1

<Figure size 1000x600 with 1 Axes>

INFO:
	27fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 160533 samples, validate on 28302 samples
Epoch 1/100
160533/160533 [==============================] - 148s 922us/step - loss: 0.2296 - val_loss: 0.2633
[last] - val_bacc: 0.7216 - val_f1: 0.5794 - val_precision: 0.7649 - val_recall: 0.4663 
[majority] - val_bacc: 0.7115 - val_f1: 0.5629 - val_precision: 0.7663 - val_recall: 0.4448 

Epoch 00001: val_loss improved from inf to 0.26327, saving model to ./checkpoints/27fold_BN2_32-32u_32ws_12f/m_0001_0.2296_0.2633.hdf5
Epoch 2/100
160533/160533 [==============================] - 138s 857us/step - loss: 0.1702 - val_loss: 0.2627
[last] - val_bacc: 0.7206 - val_f1: 0.5809 - val_precision: 0.7833 - val_recall: 0.4617 
[majority] - val_bacc: 0.7125 - val_f1: 0.5686 - val_precision: 0.7910 - val_recall: 0.4438 

Epoch 00002: val_loss improved from 0.26327 to 0.26273, saving model to ./checkpoints/27fold_BN2_32-32u_32ws_12f/m_0002_0.1702_0.2627.hdf5
Epoch 3/100
160533/160533 [==============================] - 134s 834us/step - loss: 0.1582 - val_loss: 0.2599
[last] - val_bacc: 0.7487 - val_f1: 0.6288 - val_precision: 0.7997 - val_recall: 0.5181 
[majority] - val_bacc: 0.7356 - val_f1: 0.6098 - val_precision: 0.8069 - val_recall: 0.4900 

Epoch 00003: val_loss improved from 0.26273 to 0.25991, saving model to ./checkpoints/27fold_BN2_32-32u_32ws_12f/m_0003_0.1582_0.2599.hdf5
Epoch 4/100
160533/160533 [==============================] - 135s 841us/step - loss: 0.1499 - val_loss: 0.2648
[last] - val_bacc: 0.7390 - val_f1: 0.6103 - val_precision: 0.7823 - val_recall: 0.5003 
[majority] - val_bacc: 0.7296 - val_f1: 0.5962 - val_precision: 0.7865 - val_recall: 0.4801 

Epoch 00004: val_loss did not improve from 0.25991
Epoch 5/100
160533/160533 [==============================] - 133s 828us/step - loss: 0.1431 - val_loss: 0.2784
[last] - val_bacc: 0.7439 - val_f1: 0.6156 - val_precision: 0.7712 - val_recall: 0.5123 
[majority] - val_bacc: 0.7318 - val_f1: 0.5986 - val_precision: 0.7806 - val_recall: 0.4854 

Epoch 00005: val_loss did not improve from 0.25991
Epoch 6/100
160533/160533 [==============================] - 135s 842us/step - loss: 0.1382 - val_loss: 0.2780
[last] - val_bacc: 0.7233 - val_f1: 0.5871 - val_precision: 0.7924 - val_recall: 0.4663 
[majority] - val_bacc: 0.7167 - val_f1: 0.5765 - val_precision: 0.7948 - val_recall: 0.4522 

Epoch 00006: val_loss did not improve from 0.25991
Epoch 7/100
160533/160533 [==============================] - 135s 844us/step - loss: 0.1336 - val_loss: 0.2915
[last] - val_bacc: 0.7194 - val_f1: 0.5796 - val_precision: 0.7867 - val_recall: 0.4589 
[majority] - val_bacc: 0.7150 - val_f1: 0.5750 - val_precision: 0.8047 - val_recall: 0.4474 

Epoch 00007: val_loss did not improve from 0.25991
Epoch 8/100
160533/160533 [==============================] - 134s 835us/step - loss: 0.1303 - val_loss: 0.2912
[last] - val_bacc: 0.7438 - val_f1: 0.6171 - val_precision: 0.7788 - val_recall: 0.5110 
[majority] - val_bacc: 0.7336 - val_f1: 0.6043 - val_precision: 0.7947 - val_recall: 0.4875 

Epoch 00008: val_loss did not improve from 0.25991
Epoch 9/100
160533/160533 [==============================] - 135s 842us/step - loss: 0.1261 - val_loss: 0.2996
[last] - val_bacc: 0.7495 - val_f1: 0.6240 - val_precision: 0.7711 - val_recall: 0.5240 
[majority] - val_bacc: 0.7408 - val_f1: 0.6134 - val_precision: 0.7838 - val_recall: 0.5038 

Epoch 00009: val_loss did not improve from 0.25991
Epoch 10/100
160533/160533 [==============================] - 134s 838us/step - loss: 0.1233 - val_loss: 0.2993
[last] - val_bacc: 0.7250 - val_f1: 0.5877 - val_precision: 0.7810 - val_recall: 0.4711 
[majority] - val_bacc: 0.7182 - val_f1: 0.5780 - val_precision: 0.7897 - val_recall: 0.4558 

Epoch 00010: val_loss did not improve from 0.25991
Epoch 11/100
160533/160533 [==============================] - 136s 848us/step - loss: 0.1199 - val_loss: 0.3063
[last] - val_bacc: 0.7439 - val_f1: 0.6150 - val_precision: 0.7686 - val_recall: 0.5125 
[majority] - val_bacc: 0.7321 - val_f1: 0.5989 - val_precision: 0.7796 - val_recall: 0.4862 

Epoch 00011: val_loss did not improve from 0.25991
Epoch 12/100
160533/160533 [==============================] - 135s 840us/step - loss: 0.1181 - val_loss: 0.3095
[last] - val_bacc: 0.7307 - val_f1: 0.5965 - val_precision: 0.7786 - val_recall: 0.4834 
[majority] - val_bacc: 0.7206 - val_f1: 0.5815 - val_precision: 0.7868 - val_recall: 0.4612 

Epoch 00012: val_loss did not improve from 0.25991
Epoch 13/100
160533/160533 [==============================] - 134s 837us/step - loss: 0.1158 - val_loss: 0.2983
[last] - val_bacc: 0.7636 - val_f1: 0.6379 - val_precision: 0.7451 - val_recall: 0.5577 
[majority] - val_bacc: 0.7507 - val_f1: 0.6252 - val_precision: 0.7688 - val_recall: 0.5268 

Epoch 00013: val_loss did not improve from 0.25991
Epoch 00013: early stopping
	Min train loss: 0.11577366986451874 @epoch 12
	Min valid loss: 0.25990968064481895 @epoch 2

<Figure size 1000x600 with 1 Axes>

INFO:
	28fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 159539 samples, validate on 28126 samples
Epoch 1/100
159539/159539 [==============================] - 146s 912us/step - loss: 0.2395 - val_loss: 0.2248
[last] - val_bacc: 0.7991 - val_f1: 0.6493 - val_precision: 0.6525 - val_recall: 0.6461 
[majority] - val_bacc: 0.8032 - val_f1: 0.6695 - val_precision: 0.6950 - val_recall: 0.6458 

Epoch 00001: val_loss improved from inf to 0.22485, saving model to ./checkpoints/28fold_BN2_32-32u_32ws_12f/m_0001_0.2395_0.2248.hdf5
Epoch 2/100
159539/159539 [==============================] - 136s 854us/step - loss: 0.1805 - val_loss: 0.2134
[last] - val_bacc: 0.7950 - val_f1: 0.6575 - val_precision: 0.6879 - val_recall: 0.6297 
[majority] - val_bacc: 0.7984 - val_f1: 0.6753 - val_precision: 0.7284 - val_recall: 0.6294 

Epoch 00002: val_loss improved from 0.22485 to 0.21338, saving model to ./checkpoints/28fold_BN2_32-32u_32ws_12f/m_0002_0.1805_0.2134.hdf5
Epoch 3/100
159539/159539 [==============================] - 134s 838us/step - loss: 0.1685 - val_loss: 0.2275
[last] - val_bacc: 0.7842 - val_f1: 0.6358 - val_precision: 0.6615 - val_recall: 0.6120 
[majority] - val_bacc: 0.7879 - val_f1: 0.6536 - val_precision: 0.7013 - val_recall: 0.6120 

Epoch 00003: val_loss did not improve from 0.21338
Epoch 4/100
159539/159539 [==============================] - 136s 854us/step - loss: 0.1595 - val_loss: 0.2149
[last] - val_bacc: 0.8001 - val_f1: 0.6655 - val_precision: 0.6938 - val_recall: 0.6394 
[majority] - val_bacc: 0.8025 - val_f1: 0.6833 - val_precision: 0.7375 - val_recall: 0.6364 

Epoch 00004: val_loss did not improve from 0.21338
Epoch 5/100
159539/159539 [==============================] - 135s 844us/step - loss: 0.1528 - val_loss: 0.2243
[last] - val_bacc: 0.8199 - val_f1: 0.6762 - val_precision: 0.6648 - val_recall: 0.6880 
[majority] - val_bacc: 0.8245 - val_f1: 0.7023 - val_precision: 0.7190 - val_recall: 0.6863 

Epoch 00005: val_loss did not improve from 0.21338
Epoch 6/100
159539/159539 [==============================] - 135s 844us/step - loss: 0.1478 - val_loss: 0.2094
[last] - val_bacc: 0.8210 - val_f1: 0.6992 - val_precision: 0.7213 - val_recall: 0.6784 
[majority] - val_bacc: 0.8182 - val_f1: 0.7076 - val_precision: 0.7542 - val_recall: 0.6665 

Epoch 00006: val_loss improved from 0.21338 to 0.20943, saving model to ./checkpoints/28fold_BN2_32-32u_32ws_12f/m_0006_0.1478_0.2094.hdf5
Epoch 7/100
159539/159539 [==============================] - 135s 845us/step - loss: 0.1433 - val_loss: 0.2293
[last] - val_bacc: 0.8254 - val_f1: 0.6820 - val_precision: 0.6652 - val_recall: 0.6997 
[majority] - val_bacc: 0.8255 - val_f1: 0.7034 - val_precision: 0.7192 - val_recall: 0.6883 

Epoch 00007: val_loss did not improve from 0.20943
Epoch 8/100
159539/159539 [==============================] - 137s 857us/step - loss: 0.1391 - val_loss: 0.2235
[last] - val_bacc: 0.8131 - val_f1: 0.6944 - val_precision: 0.7332 - val_recall: 0.6595 
[majority] - val_bacc: 0.8120 - val_f1: 0.7031 - val_precision: 0.7627 - val_recall: 0.6522 

Epoch 00008: val_loss did not improve from 0.20943
Epoch 9/100
159539/159539 [==============================] - 133s 834us/step - loss: 0.1367 - val_loss: 0.2251
[last] - val_bacc: 0.8167 - val_f1: 0.6928 - val_precision: 0.7169 - val_recall: 0.6703 
[majority] - val_bacc: 0.8168 - val_f1: 0.7048 - val_precision: 0.7508 - val_recall: 0.6641 

Epoch 00009: val_loss did not improve from 0.20943
Epoch 10/100
159539/159539 [==============================] - 134s 841us/step - loss: 0.1342 - val_loss: 0.2291
[last] - val_bacc: 0.7997 - val_f1: 0.6799 - val_precision: 0.7375 - val_recall: 0.6306 
[majority] - val_bacc: 0.8013 - val_f1: 0.6920 - val_precision: 0.7693 - val_recall: 0.6289 

Epoch 00010: val_loss did not improve from 0.20943
Epoch 11/100
159539/159539 [==============================] - 134s 838us/step - loss: 0.1307 - val_loss: 0.2219
[last] - val_bacc: 0.8233 - val_f1: 0.7058 - val_precision: 0.7325 - val_recall: 0.6810 
[majority] - val_bacc: 0.8197 - val_f1: 0.7126 - val_precision: 0.7632 - val_recall: 0.6682 

Epoch 00011: val_loss did not improve from 0.20943
Epoch 12/100
159539/159539 [==============================] - 134s 838us/step - loss: 0.1283 - val_loss: 0.2269
[last] - val_bacc: 0.8058 - val_f1: 0.6906 - val_precision: 0.7476 - val_recall: 0.6417 
[majority] - val_bacc: 0.8032 - val_f1: 0.6968 - val_precision: 0.7772 - val_recall: 0.6315 

Epoch 00012: val_loss did not improve from 0.20943
Epoch 13/100
159539/159539 [==============================] - 134s 841us/step - loss: 0.1262 - val_loss: 0.2253
[last] - val_bacc: 0.8118 - val_f1: 0.6969 - val_precision: 0.7447 - val_recall: 0.6548 
[majority] - val_bacc: 0.8092 - val_f1: 0.7005 - val_precision: 0.7649 - val_recall: 0.6461 

Epoch 00013: val_loss did not improve from 0.20943
Epoch 14/100
159539/159539 [==============================] - 134s 840us/step - loss: 0.1241 - val_loss: 0.2385
[last] - val_bacc: 0.8136 - val_f1: 0.6949 - val_precision: 0.7329 - val_recall: 0.6606 
[majority] - val_bacc: 0.8144 - val_f1: 0.7087 - val_precision: 0.7705 - val_recall: 0.6560 

Epoch 00014: val_loss did not improve from 0.20943
Epoch 15/100
159539/159539 [==============================] - 135s 849us/step - loss: 0.1223 - val_loss: 0.2359
[last] - val_bacc: 0.8026 - val_f1: 0.6913 - val_precision: 0.7619 - val_recall: 0.6327 
[majority] - val_bacc: 0.8080 - val_f1: 0.7058 - val_precision: 0.7863 - val_recall: 0.6402 

Epoch 00015: val_loss did not improve from 0.20943
Epoch 16/100
159539/159539 [==============================] - 135s 845us/step - loss: 0.1200 - val_loss: 0.2262
[last] - val_bacc: 0.8417 - val_f1: 0.7222 - val_precision: 0.7225 - val_recall: 0.7219 
[majority] - val_bacc: 0.8356 - val_f1: 0.7281 - val_precision: 0.7551 - val_recall: 0.7029 

Epoch 00016: val_loss did not improve from 0.20943
Epoch 00016: early stopping
	Min train loss: 0.11998767427207332 @epoch 15
	Min valid loss: 0.20943224290038162 @epoch 5

<Figure size 1000x600 with 1 Axes>

INFO:
	29fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 156437 samples, validate on 27579 samples
Epoch 1/100
156437/156437 [==============================] - 143s 915us/step - loss: 0.2476 - val_loss: 0.2233
[last] - val_bacc: 0.7861 - val_f1: 0.6620 - val_precision: 0.7279 - val_recall: 0.6070 
[majority] - val_bacc: 0.7874 - val_f1: 0.6695 - val_precision: 0.7475 - val_recall: 0.6062 

Epoch 00001: val_loss improved from inf to 0.22330, saving model to ./checkpoints/29fold_BN2_32-32u_32ws_12f/m_0001_0.2476_0.2233.hdf5
Epoch 2/100
156437/156437 [==============================] - 129s 827us/step - loss: 0.1893 - val_loss: 0.2236
[last] - val_bacc: 0.7824 - val_f1: 0.6609 - val_precision: 0.7403 - val_recall: 0.5969 
[majority] - val_bacc: 0.7790 - val_f1: 0.6641 - val_precision: 0.7675 - val_recall: 0.5852 

Epoch 00002: val_loss did not improve from 0.22330
Epoch 3/100
156437/156437 [==============================] - 133s 849us/step - loss: 0.1767 - val_loss: 0.2200
[last] - val_bacc: 0.8172 - val_f1: 0.6900 - val_precision: 0.7015 - val_recall: 0.6788 
[majority] - val_bacc: 0.8051 - val_f1: 0.6871 - val_precision: 0.7333 - val_recall: 0.6463 

Epoch 00003: val_loss improved from 0.22330 to 0.22004, saving model to ./checkpoints/29fold_BN2_32-32u_32ws_12f/m_0003_0.1767_0.2200.hdf5
Epoch 4/100
156437/156437 [==============================] - 131s 838us/step - loss: 0.1678 - val_loss: 0.2232
[last] - val_bacc: 0.7942 - val_f1: 0.6723 - val_precision: 0.7289 - val_recall: 0.6239 
[majority] - val_bacc: 0.7878 - val_f1: 0.6727 - val_precision: 0.7569 - val_recall: 0.6054 

Epoch 00004: val_loss did not improve from 0.22004
Epoch 5/100
156437/156437 [==============================] - 132s 842us/step - loss: 0.1619 - val_loss: 0.2287
[last] - val_bacc: 0.7904 - val_f1: 0.6596 - val_precision: 0.7034 - val_recall: 0.6209 
[majority] - val_bacc: 0.7835 - val_f1: 0.6629 - val_precision: 0.7424 - val_recall: 0.5989 

Epoch 00005: val_loss did not improve from 0.22004
Epoch 6/100
156437/156437 [==============================] - 132s 847us/step - loss: 0.1556 - val_loss: 0.2337
[last] - val_bacc: 0.7849 - val_f1: 0.6549 - val_precision: 0.7099 - val_recall: 0.6079 
[majority] - val_bacc: 0.7833 - val_f1: 0.6646 - val_precision: 0.7492 - val_recall: 0.5972 

Epoch 00006: val_loss did not improve from 0.22004
Epoch 7/100
156437/156437 [==============================] - 132s 845us/step - loss: 0.1508 - val_loss: 0.2417
[last] - val_bacc: 0.7823 - val_f1: 0.6509 - val_precision: 0.7071 - val_recall: 0.6029 
[majority] - val_bacc: 0.7832 - val_f1: 0.6623 - val_precision: 0.7417 - val_recall: 0.5983 

Epoch 00007: val_loss did not improve from 0.22004
Epoch 8/100
156437/156437 [==============================] - 131s 840us/step - loss: 0.1474 - val_loss: 0.2460
[last] - val_bacc: 0.7590 - val_f1: 0.6182 - val_precision: 0.6982 - val_recall: 0.5547 
[majority] - val_bacc: 0.7620 - val_f1: 0.6335 - val_precision: 0.7399 - val_recall: 0.5539 

Epoch 00008: val_loss did not improve from 0.22004
Epoch 9/100
156437/156437 [==============================] - 134s 854us/step - loss: 0.1441 - val_loss: 0.2478
[last] - val_bacc: 0.7984 - val_f1: 0.6605 - val_precision: 0.6784 - val_recall: 0.6436 
[majority] - val_bacc: 0.7979 - val_f1: 0.6723 - val_precision: 0.7149 - val_recall: 0.6346 

Epoch 00009: val_loss did not improve from 0.22004
Epoch 10/100
156437/156437 [==============================] - 132s 843us/step - loss: 0.1405 - val_loss: 0.2525
[last] - val_bacc: 0.7932 - val_f1: 0.6573 - val_precision: 0.6865 - val_recall: 0.6305 
[majority] - val_bacc: 0.7916 - val_f1: 0.6666 - val_precision: 0.7205 - val_recall: 0.6201 

Epoch 00010: val_loss did not improve from 0.22004
Epoch 11/100
156437/156437 [==============================] - 133s 849us/step - loss: 0.1382 - val_loss: 0.2539
[last] - val_bacc: 0.7988 - val_f1: 0.6612 - val_precision: 0.6788 - val_recall: 0.6444 
[majority] - val_bacc: 0.7912 - val_f1: 0.6614 - val_precision: 0.7060 - val_recall: 0.6220 

Epoch 00011: val_loss did not improve from 0.22004
Epoch 12/100
156437/156437 [==============================] - 132s 844us/step - loss: 0.1354 - val_loss: 0.2642
[last] - val_bacc: 0.7818 - val_f1: 0.6500 - val_precision: 0.7061 - val_recall: 0.6021 
[majority] - val_bacc: 0.7825 - val_f1: 0.6581 - val_precision: 0.7303 - val_recall: 0.5989 

Epoch 00012: val_loss did not improve from 0.22004
Epoch 13/100
156437/156437 [==============================] - 131s 835us/step - loss: 0.1332 - val_loss: 0.2712
[last] - val_bacc: 0.8056 - val_f1: 0.6661 - val_precision: 0.6716 - val_recall: 0.6608 
[majority] - val_bacc: 0.7995 - val_f1: 0.6705 - val_precision: 0.7036 - val_recall: 0.6403 

Epoch 00013: val_loss did not improve from 0.22004
Epoch 00013: early stopping
	Min train loss: 0.13323424694240726 @epoch 12
	Min valid loss: 0.22004337970307472 @epoch 2

<Figure size 1000x600 with 1 Axes>

INFO:
	30fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 160594 samples, validate on 28312 samples
Epoch 1/100
160594/160594 [==============================] - 147s 916us/step - loss: 0.2527 - val_loss: 0.1961
[last] - val_bacc: 0.8023 - val_f1: 0.6627 - val_precision: 0.6845 - val_recall: 0.6423 
[majority] - val_bacc: 0.8029 - val_f1: 0.6710 - val_precision: 0.7056 - val_recall: 0.6397 

Epoch 00001: val_loss improved from inf to 0.19607, saving model to ./checkpoints/30fold_BN2_32-32u_32ws_12f/m_0001_0.2527_0.1961.hdf5
Epoch 2/100
160594/160594 [==============================] - 135s 841us/step - loss: 0.1892 - val_loss: 0.1901
[last] - val_bacc: 0.8189 - val_f1: 0.6835 - val_precision: 0.6909 - val_recall: 0.6764 
[majority] - val_bacc: 0.8144 - val_f1: 0.6908 - val_precision: 0.7233 - val_recall: 0.6610 

Epoch 00002: val_loss improved from 0.19607 to 0.19010, saving model to ./checkpoints/30fold_BN2_32-32u_32ws_12f/m_0002_0.1892_0.1901.hdf5
Epoch 3/100
160594/160594 [==============================] - 133s 831us/step - loss: 0.1776 - val_loss: 0.1905
[last] - val_bacc: 0.8263 - val_f1: 0.6899 - val_precision: 0.6873 - val_recall: 0.6926 
[majority] - val_bacc: 0.8177 - val_f1: 0.6952 - val_precision: 0.7253 - val_recall: 0.6676 

Epoch 00003: val_loss did not improve from 0.19010
Epoch 4/100
160594/160594 [==============================] - 135s 841us/step - loss: 0.1693 - val_loss: 0.1894
[last] - val_bacc: 0.7989 - val_f1: 0.6772 - val_precision: 0.7370 - val_recall: 0.6263 
[majority] - val_bacc: 0.7935 - val_f1: 0.6818 - val_precision: 0.7733 - val_recall: 0.6097 

Epoch 00004: val_loss improved from 0.19010 to 0.18944, saving model to ./checkpoints/30fold_BN2_32-32u_32ws_12f/m_0004_0.1693_0.1894.hdf5
Epoch 5/100
160594/160594 [==============================] - 135s 839us/step - loss: 0.1632 - val_loss: 0.2014
[last] - val_bacc: 0.8283 - val_f1: 0.6724 - val_precision: 0.6410 - val_recall: 0.7070 
[majority] - val_bacc: 0.8216 - val_f1: 0.6845 - val_precision: 0.6862 - val_recall: 0.6829 

Epoch 00005: val_loss did not improve from 0.18944
Epoch 6/100
160594/160594 [==============================] - 135s 840us/step - loss: 0.1575 - val_loss: 0.2055
[last] - val_bacc: 0.8283 - val_f1: 0.6780 - val_precision: 0.6540 - val_recall: 0.7039 
[majority] - val_bacc: 0.8205 - val_f1: 0.6888 - val_precision: 0.7001 - val_recall: 0.6779 

Epoch 00006: val_loss did not improve from 0.18944
Epoch 7/100
160594/160594 [==============================] - 135s 838us/step - loss: 0.1513 - val_loss: 0.2021
[last] - val_bacc: 0.8227 - val_f1: 0.6884 - val_precision: 0.6930 - val_recall: 0.6839 
[majority] - val_bacc: 0.8182 - val_f1: 0.6981 - val_precision: 0.7315 - val_recall: 0.6676 

Epoch 00007: val_loss did not improve from 0.18944
Epoch 8/100
160594/160594 [==============================] - 136s 850us/step - loss: 0.1479 - val_loss: 0.2107
[last] - val_bacc: 0.8109 - val_f1: 0.6634 - val_precision: 0.6618 - val_recall: 0.6651 
[majority] - val_bacc: 0.8060 - val_f1: 0.6779 - val_precision: 0.7145 - val_recall: 0.6448 

Epoch 00008: val_loss did not improve from 0.18944
Epoch 9/100
160594/160594 [==============================] - 133s 827us/step - loss: 0.1440 - val_loss: 0.2081
[last] - val_bacc: 0.8117 - val_f1: 0.6704 - val_precision: 0.6773 - val_recall: 0.6635 
[majority] - val_bacc: 0.8056 - val_f1: 0.6798 - val_precision: 0.7212 - val_recall: 0.6429 

Epoch 00009: val_loss did not improve from 0.18944
Epoch 10/100
160594/160594 [==============================] - 138s 857us/step - loss: 0.1400 - val_loss: 0.2064
[last] - val_bacc: 0.8193 - val_f1: 0.6880 - val_precision: 0.7015 - val_recall: 0.6751 
[majority] - val_bacc: 0.8199 - val_f1: 0.6977 - val_precision: 0.7252 - val_recall: 0.6723 

Epoch 00010: val_loss did not improve from 0.18944
Epoch 11/100
160594/160594 [==============================] - 135s 838us/step - loss: 0.1381 - val_loss: 0.2144
[last] - val_bacc: 0.8378 - val_f1: 0.6840 - val_precision: 0.6464 - val_recall: 0.7261 
[majority] - val_bacc: 0.8318 - val_f1: 0.6915 - val_precision: 0.6772 - val_recall: 0.7064 

Epoch 00011: val_loss did not improve from 0.18944
Epoch 12/100
160594/160594 [==============================] - 135s 841us/step - loss: 0.1347 - val_loss: 0.2194
[last] - val_bacc: 0.8085 - val_f1: 0.6646 - val_precision: 0.6714 - val_recall: 0.6579 
[majority] - val_bacc: 0.8054 - val_f1: 0.6756 - val_precision: 0.7104 - val_recall: 0.6441 

Epoch 00012: val_loss did not improve from 0.18944
Epoch 13/100
160594/160594 [==============================] - 136s 848us/step - loss: 0.1322 - val_loss: 0.2214
[last] - val_bacc: 0.7853 - val_f1: 0.6503 - val_precision: 0.7064 - val_recall: 0.6025 
[majority] - val_bacc: 0.7898 - val_f1: 0.6675 - val_precision: 0.7424 - val_recall: 0.6063 

Epoch 00013: val_loss did not improve from 0.18944
Epoch 14/100
160594/160594 [==============================] - 135s 843us/step - loss: 0.1305 - val_loss: 0.2200
[last] - val_bacc: 0.8225 - val_f1: 0.6809 - val_precision: 0.6750 - val_recall: 0.6870 
[majority] - val_bacc: 0.8167 - val_f1: 0.6848 - val_precision: 0.7005 - val_recall: 0.6698 

Epoch 00014: val_loss did not improve from 0.18944
Epoch 00014: early stopping
	Min train loss: 0.1304539783549255 @epoch 13
	Min valid loss: 0.18944395514095624 @epoch 3

<Figure size 1000x600 with 1 Axes>

INFO:
	31fold_BN2_32-32u_32ws_12f
	 batch_size=128
	 n_epochs=100
Train on 159256 samples, validate on 28076 samples
Epoch 1/100
159256/159256 [==============================] - 147s 925us/step - loss: 0.2423 - val_loss: 0.2375
[last] - val_bacc: 0.7946 - val_f1: 0.6860 - val_precision: 0.7654 - val_recall: 0.6215 
[majority] - val_bacc: 0.7856 - val_f1: 0.6749 - val_precision: 0.7678 - val_recall: 0.6021 

Epoch 00001: val_loss improved from inf to 0.23751, saving model to ./checkpoints/31fold_BN2_32-32u_32ws_12f/m_0001_0.2423_0.2375.hdf5
Epoch 2/100
159256/159256 [==============================] - 134s 839us/step - loss: 0.1838 - val_loss: 0.2306
[last] - val_bacc: 0.8199 - val_f1: 0.7099 - val_precision: 0.7430 - val_recall: 0.6797 
[majority] - val_bacc: 0.8099 - val_f1: 0.7036 - val_precision: 0.7601 - val_recall: 0.6549 

Epoch 00002: val_loss improved from 0.23751 to 0.23061, saving model to ./checkpoints/31fold_BN2_32-32u_32ws_12f/m_0002_0.1838_0.2306.hdf5
Epoch 3/100
159256/159256 [==============================] - 134s 840us/step - loss: 0.1718 - val_loss: 0.2312
[last] - val_bacc: 0.8263 - val_f1: 0.7134 - val_precision: 0.7318 - val_recall: 0.6959 
[majority] - val_bacc: 0.8167 - val_f1: 0.7079 - val_precision: 0.7480 - val_recall: 0.6718 

Epoch 00003: val_loss did not improve from 0.23061
Epoch 4/100
 26752/159256 [====>.........................] - ETA: 1:44 - loss: 0.1655