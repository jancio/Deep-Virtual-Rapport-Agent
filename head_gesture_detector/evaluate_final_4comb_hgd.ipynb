{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# Project: Deep Virtual Rapport Agent (head gesture detector)\n",
    "#\n",
    "#     Jan Ondras (jo951030@gmail.com)\n",
    "#     Institute for Creative Technologies, University of Southern California\n",
    "#     April-October 2019\n",
    "#\n",
    "#######################################################################################################################\n",
    "#\n",
    "#     Test the final nod, shake, and tilt detector models trained on the whole 4comb dataset on other datasets\n",
    "#\n",
    "#     Namely, cross-dataset testing on the ccdb dataset. \n",
    "#     Requires the segmented ccdb datasets to be generated before (datasets_scripts/ccdb/generate_dataset.ipynb).\n",
    "#\n",
    "#     This tests the nod, shake, and tilt models independently. \n",
    "#     For testing of a fused 4-class (none/nod/shake/tilt) HGD model see the script ./evaluate_fused_final_4comb_hgd.ipynb\n",
    "#\n",
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "import numpy as np\n",
    "random_seed = 37\n",
    "np.random.seed(random_seed)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(random_seed)\n",
    "###########################################################\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "# from keras_tqdm import TQDMNotebookCallback\n",
    "from utils import load_train_history, save_train_history, plot_loss_history, arch_to_str, evaluate_custom_metrics\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from keras import backend as K\n",
    "import glob\n",
    "import time\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ICT2000/jondras/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ICT2000/jondras/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/ICT2000/jondras/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Loading model:\n",
      "\tfinal_4comb_nod_32ws_12f_A1_16u.hdf5 \t #params: 1409\n",
      "\n",
      "\tTesting on ccdb\n",
      "\t\t[last] bacc: \t\t 0.7948\n",
      "\t\t[last] f1: \t\t 0.2187\n",
      "\t\t[last] precision: \t\t 0.1248\n",
      "\t\t[last] recall: \t\t 0.8842\n",
      "\n",
      "\t\t[majority] bacc: \t\t 0.7984\n",
      "\t\t[majority] f1: \t\t 0.2210\n",
      "\t\t[majority] precision: \t\t 0.1262\n",
      "\t\t[majority] recall: \t\t 0.8891\n",
      "\n",
      "\n",
      "\t\t Total time taken: 12.261885643005371 s\n",
      "\n",
      "Saved to ./test_results_final_4comb_S/test_results_final_4comb_S_nod_32ws_12f_A1_16u.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################################################################################################\n",
    "# For a given head gesture: evaluate the best model on the test dataset (ccdb) and save the results as one .pkl file\n",
    "#\n",
    "# Saved to the folder ./test_results_final_4comb_{S,nonS}\n",
    "# Filenames naming convention: test_results_{dataset_name}_{S,nonS}_{head_gesture}_{window_size}ws_{number_of_features}f_{model_architecture}.pkl \n",
    "#######################################################################################################################\n",
    "\n",
    "#############################################\n",
    "# Smooth the predicted labels?\n",
    "smooth_labels = False # Name infix *_nonS_*\n",
    "smooth_labels = True  # Name infix *_S_*\n",
    "#############################################\n",
    "\n",
    "HEAD_GESTURE = 'nod'\n",
    "GRU_ARCH = [16]\n",
    "\n",
    "# HEAD_GESTURE = 'shake'\n",
    "# GRU_ARCH = [8]\n",
    "\n",
    "# HEAD_GESTURE = 'tilt'\n",
    "# GRU_ARCH = [16]\n",
    "\n",
    "\n",
    "WINDOW_SIZE = 32\n",
    "N_FEATURES = 12\n",
    "\n",
    "dataset_type = f'{WINDOW_SIZE}ws_{N_FEATURES}f'\n",
    "# Augmentation\n",
    "model_type = f'{dataset_type}_A1_{arch_to_str(GRU_ARCH)}u'\n",
    "# model_type = f'{dataset_type}_{arch_to_str(GRU_ARCH)}u'\n",
    "\n",
    "metrics_names = ['bacc', 'f1', 'precision', 'recall']\n",
    "voting_strategies = ['last', 'majority']\n",
    "train_dataset_name = 'final_4comb'\n",
    "test_dataset_names = ['ccdb']\n",
    "\n",
    "datasets_path_prefix = f'/home/ICT2000/jondras/dvra_datasets'\n",
    "checkpoints_path_prefix = f'/home/ICT2000/jondras/deep-virtual-rapport-agent/head_gesture_detector/checkpoints/final_4comb'\n",
    "\n",
    "test_results_type = f'test_results_final_4comb_S' if smooth_labels else f'test_results_final_4comb_nonS'\n",
    "if not os.path.exists(f'./test_results/{test_results_type}'):\n",
    "    os.makedirs(f'./test_results/{test_results_type}')\n",
    "    \n",
    "# Test results are saved as dictionary:\n",
    "# test_results[train_dataset_name][test_dataset_name][voting_strategy][metric_name]\n",
    "test_results = dict()\n",
    "test_results[train_dataset_name] = dict()\n",
    "\n",
    "# Just one final model\n",
    "model_path = f'{checkpoints_path_prefix}/{train_dataset_name}_{HEAD_GESTURE}_{model_type}.hdf5'\n",
    "\n",
    "start_time = time.time()\n",
    "# Load best model for this dataset\n",
    "# local_start_time = time.time() \n",
    "K.clear_session()\n",
    "best_model = load_model(model_path)\n",
    "print(f'Loading model:\\n\\t{model_path.split(\"/\")[-1]} \\t #params: {best_model.count_params()}\\n')  \n",
    "# print(f'\\t\\t Time to load model: {time.time() - local_start_time} s')\n",
    "\n",
    "# Iterate over datasets to test the model\n",
    "for test_dataset_name in test_dataset_names:\n",
    "\n",
    "    test_results[train_dataset_name][test_dataset_name] = dict()\n",
    "    for vs in voting_strategies:\n",
    "        test_results[train_dataset_name][test_dataset_name][vs] = defaultdict(list)  \n",
    "\n",
    "    # Load testing data\n",
    "    # local_start_time = time.time() \n",
    "    print(f'\\tTesting on {test_dataset_name}')\n",
    "    data = np.load(f'{datasets_path_prefix}/{test_dataset_name}/segmented_datasets/{test_dataset_name}_{HEAD_GESTURE}_{dataset_type}.npz')\n",
    "    X_test,  Y_test  = data['X_test'],  data['Y_test']\n",
    "    # print(f'\\t\\t Time to load data: {time.time() - local_start_time} s')\n",
    "\n",
    "    # local_start_time = time.time() \n",
    "    test_metrics = evaluate_custom_metrics(Y_true=Y_test, \n",
    "                                           Y_pred=best_model.predict_classes(X_test, \n",
    "                                                                             batch_size=10000\n",
    "#                                                                                  batch_size=len(X_test)\n",
    "                                                                            ), \n",
    "                                           chunk_lens=data['test_len'], window_size=WINDOW_SIZE, \n",
    "                                           smooth=smooth_labels\n",
    "                                          )\n",
    "    # print(f'\\t\\t Time to calculate test metrics: {time.time() - local_start_time} s')\n",
    "    for vs in voting_strategies:\n",
    "        for mn in metrics_names:\n",
    "            test_results[train_dataset_name][test_dataset_name][vs][mn].append( test_metrics[vs][mn] )\n",
    "            print(f'\\t\\t[{vs}] {mn}: \\t\\t {test_metrics[vs][mn]:.4f}')\n",
    "        print()\n",
    "    print()        \n",
    "\n",
    "print(f'\\t\\t Total time taken: {time.time() - start_time} s\\n')\n",
    "\n",
    "# Save test results (from all testing of the current model on all datasets)\n",
    "save_results_path = f'./test_results/{test_results_type}/{test_results_type}_{HEAD_GESTURE}_{model_type}.pkl'\n",
    "with open(save_results_path, 'wb') as pickle_filehandler:\n",
    "    pickle.dump(test_results, pickle_filehandler)\n",
    "print(f'Saved to {save_results_path}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_results_4comb_S_nod_32ws_12f_16u.pkl\n",
      "\t\t[last] bacc: \t\t 0.8422\n",
      "\t\t[last] f1: \t\t 0.5045\n",
      "\t\t[last] precision: \t\t 0.3628\n",
      "\t\t[last] recall: \t\t 0.8278\n",
      "\n",
      "\t\t[majority] bacc: \t\t 0.8494\n",
      "\t\t[majority] f1: \t\t 0.5195\n",
      "\t\t[majority] precision: \t\t 0.3771\n",
      "\t\t[majority] recall: \t\t 0.8347\n",
      "\n",
      "\n",
      "test_results_4comb_S_shake_32ws_12f_8u.pkl\n",
      "\t\t[last] bacc: \t\t 0.8558\n",
      "\t\t[last] f1: \t\t 0.5003\n",
      "\t\t[last] precision: \t\t 0.3586\n",
      "\t\t[last] recall: \t\t 0.8275\n",
      "\n",
      "\t\t[majority] bacc: \t\t 0.8371\n",
      "\t\t[majority] f1: \t\t 0.4944\n",
      "\t\t[majority] precision: \t\t 0.3613\n",
      "\t\t[majority] recall: \t\t 0.7826\n",
      "\n",
      "\n",
      "test_results_4comb_S_tilt_32ws_12f_16u.pkl\n",
      "\t\t[last] bacc: \t\t 0.7409\n",
      "\t\t[last] f1: \t\t 0.2470\n",
      "\t\t[last] precision: \t\t 0.1538\n",
      "\t\t[last] recall: \t\t 0.6267\n",
      "\n",
      "\t\t[majority] bacc: \t\t 0.7537\n",
      "\t\t[majority] f1: \t\t 0.2580\n",
      "\t\t[majority] precision: \t\t 0.1609\n",
      "\t\t[majority] recall: \t\t 0.6499\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "# Print testing summary: 4comb\n",
    "################################\n",
    "import pickle\n",
    "from tabulate import tabulate\n",
    "from utils import arch_to_str\n",
    "import numpy as np\n",
    "\n",
    "#############################################\n",
    "# Smooth the predicted labels?\n",
    "smooth_labels = False # Name infix *_nonS_*\n",
    "smooth_labels = True  # Name infix *_S_*\n",
    "#############################################\n",
    "\n",
    "metrics_names = ['bacc', 'f1', 'precision', 'recall']\n",
    "voting_strategies = ['last', 'majority']\n",
    "\n",
    "train_dataset_name = '4comb'\n",
    "test_dataset_names = ['4comb']\n",
    "\n",
    "test_results_type = f'test_results_final_4comb_S' if smooth_labels else f'test_results_final_4comb_nonS'\n",
    "    \n",
    "for save_results_path in sorted(glob.glob(f'./test_results/{test_results_type}/{test_results_type}_*.pkl')):\n",
    "    print(save_results_path.split('/')[-1])\n",
    "    with open(save_results_path, 'rb') as pickle_filehandler:\n",
    "        test_results = pickle.load(pickle_filehandler)\n",
    "        \n",
    "        for test_dataset_name in test_dataset_names:\n",
    "            for vs in voting_strategies:\n",
    "                for mn in metrics_names:\n",
    "                    print(f'\\t\\t[{vs}] {mn}: \\t\\t {test_results[train_dataset_name][test_dataset_name][vs][mn][0]:.4f}')\n",
    "                print()\n",
    "            print()    \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
