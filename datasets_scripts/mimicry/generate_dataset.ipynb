{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality of vision features: 27\n",
      "Dimensionality of audio features: 53\n",
      "Dimensionality of target labels: 7\n",
      "\n",
      "Window size (sequence length): 32\n",
      "Audio feature type: emobase\n",
      "\n",
      "sessid_01_P1_sid_09_P2_sid_02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ICT2000/jondras/anaconda3/envs/dvra/lib/python3.7/site-packages/ipykernel_launcher.py:193: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "/home/ICT2000/jondras/anaconda3/envs/dvra/lib/python3.7/site-packages/ipykernel_launcher.py:197: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "/home/ICT2000/jondras/anaconda3/envs/dvra/lib/python3.7/site-packages/ipykernel_launcher.py:257: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Skipping last video frame (# video_frames: 18401, # audio_frames: 18400)\n",
      "\t Sequence count: 11336\n",
      "\t Time taken: 362.20870542526245 s\n",
      "\n",
      "sessid_01_P2_sid_02_P1_sid_09\n",
      "\t Skipping last video frame (# video_frames: 18401, # audio_frames: 18400)\n",
      "\t Sequence count: 8726\n",
      "\t Time taken: 651.6289031505585 s\n",
      "\n",
      "sessid_02_P1_sid_09_P2_sid_17\n",
      "\t Sequence count: 14783\n",
      "\t Time taken: 1113.0946781635284 s\n",
      "\n",
      "sessid_02_P2_sid_17_P1_sid_09\n",
      "\t Sequence count: 15788\n",
      "\t Time taken: 1601.8737754821777 s\n",
      "\n",
      "sessid_03_P1_sid_17_P2_sid_02\n",
      "\t Skipping last audio frame (# video_frames: 13470, # audio_frames: 13471)\n",
      "\t Sequence count: 9705\n",
      "\t Time taken: 1767.3841366767883 s\n",
      "\n",
      "sessid_03_P2_sid_02_P1_sid_17\n",
      "\t Skipping last audio frame (# video_frames: 13470, # audio_frames: 13471)\n",
      "\t Sequence count: 5652\n",
      "\t Time taken: 1908.0728766918182 s\n",
      "\n",
      "sessid_04_P1_sid_12_P2_sid_23\n",
      "\t Skipping last audio frame (# video_frames: 28919, # audio_frames: 28920)\n",
      "\t Sequence count: 7719\n",
      "\t Time taken: 2136.0258531570435 s\n",
      "\n",
      "sessid_04_P2_sid_23_P1_sid_12\n",
      "\t Skipping last audio frame (# video_frames: 28919, # audio_frames: 28920)\n",
      "\t Sequence count: 19718\n",
      "\t Time taken: 2505.877534866333 s\n",
      "\n",
      "sessid_05_P1_sid_12_P2_sid_21\n",
      "\t Sequence count: 14025\n",
      "\t Time taken: 2787.0593571662903 s\n",
      "\n",
      "sessid_05_P2_sid_21_P1_sid_12\n",
      "\t Sequence count: 19600\n",
      "\t Time taken: 3151.864530324936 s\n",
      "\n",
      "sessid_06_P1_sid_23_P2_sid_21\n",
      "\t Skipping last audio frame (# video_frames: 29317, # audio_frames: 29318)\n",
      "\t Sequence count: 12443\n",
      "\t Time taken: 3428.366576194763 s\n",
      "\n",
      "sessid_06_P2_sid_21_P1_sid_23\n",
      "\t Skipping last audio frame (# video_frames: 29317, # audio_frames: 29318)\n",
      "\t Sequence count: 23273\n",
      "\t Time taken: 3838.0574629306793 s\n",
      "\n",
      "sessid_07_P1_sid_09_P2_sid_01\n",
      "\t Sequence count: 16265\n",
      "\t Time taken: 4150.850223779678 s\n",
      "\n",
      "sessid_07_P2_sid_01_P1_sid_09\n",
      "\t Sequence count: 15750\n",
      "\t Time taken: 4464.628846168518 s\n",
      "\n",
      "sessid_08_P1_sid_09_P2_sid_04\n",
      "\t Skipping last audio frame (# video_frames: 29073, # audio_frames: 29074)\n",
      "\t Sequence count: 17353\n",
      "\t Time taken: 4795.636905670166 s\n",
      "\n",
      "sessid_08_P2_sid_04_P1_sid_09\n",
      "\t Skipping last audio frame (# video_frames: 29073, # audio_frames: 29074)\n",
      "\t Sequence count: 13631\n",
      "\t Time taken: 5077.220869541168 s\n",
      "\n",
      "sessid_09_P1_sid_01_P2_sid_04\n",
      "\t Skipping last audio frame (# video_frames: 29302, # audio_frames: 29303)\n",
      "\t Sequence count: 15250\n",
      "\t Time taken: 5381.929481267929 s\n",
      "\n",
      "sessid_09_P2_sid_04_P1_sid_01\n",
      "\t Skipping last audio frame (# video_frames: 29302, # audio_frames: 29303)\n",
      "\t Sequence count: 16781\n",
      "\t Time taken: 5708.91121673584 s\n",
      "\n",
      "sessid_10_P1_sid_09_P2_sid_34\n",
      "\t Sequence count: 12765\n",
      "\t Time taken: 5983.033875703812 s\n",
      "\n",
      "sessid_10_P2_sid_34_P1_sid_09\n",
      "\t Sequence count: 18392\n",
      "\t Time taken: 6322.539104938507 s\n",
      "\n",
      "sessid_11_P1_sid_09_P2_sid_15\n",
      "\t Sequence count: 14582\n",
      "\t Time taken: 6620.465203523636 s\n",
      "\n",
      "sessid_11_P2_sid_15_P1_sid_09\n",
      "\t Sequence count: 16491\n",
      "\t Time taken: 6928.550626277924 s\n",
      "\n",
      "sessid_12_P1_sid_15_P2_sid_11\n",
      "\t Sequence count: 9364\n",
      "\t Time taken: 7106.061851263046 s\n",
      "\n",
      "sessid_12_P2_sid_11_P1_sid_15\n",
      "\t Sequence count: 10687\n",
      "\t Time taken: 7304.724068164825 s\n",
      "\n",
      "sessid_13_P1_sid_09_P2_sid_19\n",
      "\t Sequence count: 10851\n",
      "\t Time taken: 7553.079851150513 s\n",
      "\n",
      "sessid_13_P2_sid_19_P1_sid_09\n",
      "\t Sequence count: 16265\n",
      "\t Time taken: 7866.121307373047 s\n",
      "\n",
      "sessid_14_P1_sid_19_P2_sid_06\n",
      "\t Skipping last audio frame (# video_frames: 21109, # audio_frames: 21110)\n",
      "\t Sequence count: 9383\n",
      "\t Time taken: 8058.1693749427795 s\n",
      "\n",
      "sessid_14_P2_sid_06_P1_sid_19\n",
      "\t Skipping last audio frame (# video_frames: 21109, # audio_frames: 21110)\n",
      "\t Sequence count: 11672\n",
      "\t Time taken: 8281.744593858719 s\n",
      "\n",
      "sessid_15_P1_sid_09_P2_sid_16\n",
      "\t Skipping last audio frame (# video_frames: 31026, # audio_frames: 31027)\n",
      "\t Sequence count: 15308\n",
      "\t Time taken: 8623.457236289978 s\n",
      "\n",
      "sessid_15_P2_sid_16_P1_sid_09\n",
      "\t Skipping last audio frame (# video_frames: 31026, # audio_frames: 31027)\n",
      "\t Sequence count: 16772\n",
      "\t Time taken: 8943.23380112648 s\n",
      "\n",
      "sessid_16_P1_sid_24_P2_sid_16\n",
      "\t Skipping last audio frame (# video_frames: 32372, # audio_frames: 32373)\n",
      "\t Sequence count: 30209\n",
      "\t Time taken: 9441.075946807861 s\n",
      "\n",
      "sessid_16_P2_sid_16_P1_sid_24\n",
      "\t Skipping last audio frame (# video_frames: 32372, # audio_frames: 32373)\n",
      "\t Sequence count: 11149\n",
      "\t Time taken: 9701.565263032913 s\n",
      "\n",
      "sessid_17_P1_sid_09_P2_sid_43\n",
      "\t Sequence count: 9187\n",
      "\t Time taken: 9888.988698720932 s\n",
      "\n",
      "sessid_17_P2_sid_43_P1_sid_09\n",
      "\t Sequence count: 10201\n",
      "\t Time taken: 10082.183714866638 s\n",
      "\n",
      "sessid_18_P1_sid_03_P2_sid_43\n",
      "\t Sequence count: 12576\n",
      "\t Time taken: 10299.638349533081 s\n",
      "\n",
      "sessid_18_P2_sid_43_P1_sid_03\n",
      "\t Sequence count: 8897\n",
      "\t Time taken: 10474.784019231796 s\n",
      "\n",
      "sessid_19_P1_sid_09_P2_sid_22\n",
      "\t Skipping last audio frame (# video_frames: 27494, # audio_frames: 27495)\n",
      "\t Sequence count: 9242\n",
      "\t Time taken: 10706.599990844727 s\n",
      "\n",
      "sessid_19_P2_sid_22_P1_sid_09\n",
      "\t Skipping last audio frame (# video_frames: 27494, # audio_frames: 27495)\n",
      "\t Sequence count: 20260\n",
      "\t Time taken: 11063.115322828293 s\n",
      "\n",
      "sessid_20_P1_sid_09_P2_sid_50\n",
      "\t Skipping last audio frame (# video_frames: 25187, # audio_frames: 25188)\n",
      "\t Sequence count: 8185\n",
      "\t Time taken: 11267.037091970444 s\n",
      "\n",
      "sessid_20_P2_sid_50_P1_sid_09\n",
      "\t Skipping last audio frame (# video_frames: 25187, # audio_frames: 25188)\n",
      "\t Sequence count: 17937\n",
      "\t Time taken: 11576.323492765427 s\n",
      "\n",
      "sessid_21_P1_sid_50_P2_sid_22\n",
      "\t Sequence count: 11174\n",
      "\t Time taken: 11789.708695173264 s\n",
      "\n",
      "sessid_21_P2_sid_22_P1_sid_50\n",
      "\t Sequence count: 10845\n",
      "\t Time taken: 11995.49735045433 s\n",
      "\n",
      "sessid_22_P1_sid_12_P2_sid_18\n",
      "\t Sequence count: 9964\n",
      "\t Time taken: 12231.039709568024 s\n",
      "\n",
      "sessid_22_P2_sid_18_P1_sid_12\n",
      "\t Sequence count: 18348\n",
      "\t Time taken: 12570.614778995514 s\n",
      "\n",
      "sessid_23_P1_sid_12_P2_sid_39\n",
      "\t Sequence count: 12647\n",
      "\t Time taken: 12829.187899112701 s\n",
      "\n",
      "sessid_23_P2_sid_39_P1_sid_12\n",
      "\t Sequence count: 16590\n",
      "\t Time taken: 13149.49199295044 s\n",
      "\n",
      "sessid_24_P1_sid_18_P2_sid_39\n",
      "\t Skipping last audio frame (# video_frames: 13731, # audio_frames: 13732)\n",
      "\t Sequence count: 8689\n",
      "\t Time taken: 13311.280906915665 s\n",
      "\n",
      "sessid_24_P2_sid_39_P1_sid_18\n",
      "\t Skipping last audio frame (# video_frames: 13731, # audio_frames: 13732)\n",
      "\t Sequence count: 6768\n",
      "\t Time taken: 13446.151230335236 s\n",
      "\n",
      "sessid_25_P1_sid_12_P2_sid_27\n",
      "\t Sequence count: 10453\n",
      "\t Time taken: 13673.654945373535 s\n",
      "\n",
      "sessid_25_P2_sid_27_P1_sid_12\n",
      "\t Sequence count: 13594\n",
      "\t Time taken: 13936.862681627274 s\n",
      "\n",
      "sessid_26_P1_sid_12_P2_sid_32\n",
      "\t Skipping last audio frame (# video_frames: 14773, # audio_frames: 14774)\n",
      "\t Sequence count: 6675\n",
      "\t Time taken: 14089.610921382904 s\n",
      "\n",
      "sessid_26_P2_sid_32_P1_sid_12\n",
      "\t Skipping last audio frame (# video_frames: 14773, # audio_frames: 14774)\n",
      "\t Sequence count: 10823\n",
      "\t Time taken: 14272.298981428146 s\n",
      "\n",
      "sessid_27_P1_sid_27_P2_sid_32\n",
      "\t Sequence count: 9224\n",
      "\t Time taken: 14456.435074567795 s\n",
      "\n",
      "sessid_27_P2_sid_32_P1_sid_27\n",
      "\t Sequence count: 8617\n",
      "\t Time taken: 14640.105678319931 s\n",
      "\n",
      "sessid_28_P1_sid_09_P2_sid_07\n",
      "\t Skipping last audio frame (# video_frames: 28692, # audio_frames: 28693)\n",
      "\t Sequence count: 12811\n",
      "\t Time taken: 14917.390837669373 s\n",
      "\n",
      "sessid_28_P2_sid_07_P1_sid_09\n",
      "\t Skipping last audio frame (# video_frames: 28692, # audio_frames: 28693)\n",
      "\t Sequence count: 17845\n",
      "\t Time taken: 15247.822214603424 s\n",
      "\n",
      "sessid_29_P1_sid_09_P2_sid_05\n",
      "\t Sequence count: 9756\n",
      "\t Time taken: 15454.267888784409 s\n",
      "\n",
      "sessid_29_P2_sid_05_P1_sid_09\n",
      "\t Sequence count: 13641\n",
      "\t Time taken: 15711.589544296265 s\n",
      "\n",
      "sessid_30_P1_sid_07_P2_sid_05\n",
      "\t Sequence count: 3413\n",
      "\t Time taken: 15781.51327252388 s\n",
      "\n",
      "sessid_30_P2_sid_05_P1_sid_07\n",
      "\t Sequence count: 3827\n",
      "\t Time taken: 15845.01504778862 s\n",
      "\n",
      "sessid_31_P1_sid_28_P2_sid_09\n",
      "\t Skipping last audio frame (# video_frames: 26103, # audio_frames: 26104)\n",
      "\t Sequence count: 21776\n",
      "\t Time taken: 16212.671882629395 s\n",
      "\n",
      "sessid_31_P2_sid_09_P1_sid_28\n",
      "\t Skipping last audio frame (# video_frames: 26103, # audio_frames: 26104)\n",
      "\t Sequence count: 6547\n",
      "\t Time taken: 16396.527269124985 s\n",
      "\n",
      "sessid_32_P1_sid_28_P2_sid_14\n",
      "\t Sequence count: 12527\n",
      "\t Time taken: 16632.296200990677 s\n",
      "\n",
      "sessid_32_P2_sid_14_P1_sid_28\n",
      "\t Sequence count: 13038\n",
      "\t Time taken: 16871.007039546967 s\n",
      "\n",
      "sessid_33_P1_sid_09_P2_sid_33\n",
      "\t Skipping last audio frame (# video_frames: 29180, # audio_frames: 29181)\n",
      "\t Sequence count: 9973\n",
      "\t Time taken: 17123.993955373764 s\n",
      "\n",
      "sessid_33_P2_sid_33_P1_sid_09\n",
      "\t Skipping last audio frame (# video_frames: 29180, # audio_frames: 29181)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Sequence count: 21642\n",
      "\t Time taken: 17504.542641162872 s\n",
      "\n",
      "sessid_34_P1_sid_09_P2_sid_36\n",
      "\t Sequence count: 11481\n",
      "\t Time taken: 17748.160537481308 s\n",
      "\n",
      "sessid_34_P2_sid_36_P1_sid_09\n",
      "\t Sequence count: 17165\n",
      "\t Time taken: 18065.5277967453 s\n",
      "\n",
      "sessid_35_P1_sid_33_P2_sid_36\n",
      "\t Sequence count: 16665\n",
      "\t Time taken: 18391.749469280243 s\n",
      "\n",
      "sessid_35_P2_sid_36_P1_sid_33\n",
      "\t Sequence count: 16757\n",
      "\t Time taken: 18726.99099421501 s\n",
      "\n",
      "sessid_36_P1_sid_13_P2_sid_40\n",
      "\t Skipping last audio frame (# video_frames: 27413, # audio_frames: 27414)\n",
      "\t Sequence count: 10782\n",
      "\t Time taken: 18966.261962652206 s\n",
      "\n",
      "sessid_36_P2_sid_40_P1_sid_13\n",
      "\t Skipping last audio frame (# video_frames: 27413, # audio_frames: 27414)\n",
      "\t Sequence count: 18696\n",
      "\t Time taken: 19303.085735797882 s\n",
      "\n",
      "sessid_37_P1_sid_13_P2_sid_40\n",
      "\t Sequence count: 12255\n",
      "\t Time taken: 19549.96816635132 s\n",
      "\n",
      "sessid_37_P2_sid_40_P1_sid_13\n",
      "\t Sequence count: 16783\n",
      "\t Time taken: 19852.74633526802 s\n",
      "\n",
      "sessid_38_P1_sid_56_P2_sid_30\n",
      "\t Skipping last audio frame (# video_frames: 33788, # audio_frames: 33789)\n",
      "\t Sequence count: 19884\n",
      "\t Time taken: 20230.538697719574 s\n",
      "\n",
      "sessid_38_P2_sid_30_P1_sid_56\n",
      "\t Skipping last audio frame (# video_frames: 33788, # audio_frames: 33789)\n",
      "\t Sequence count: 18444\n",
      "\t Time taken: 20597.481760263443 s\n",
      "\n",
      "sessid_40_P1_sid_56_P2_sid_51\n",
      "\t Skipping last audio frame (# video_frames: 26479, # audio_frames: 26480)\n",
      "\t Sequence count: 16611\n",
      "\t Time taken: 20895.370771169662 s\n",
      "\n",
      "sessid_40_P2_sid_51_P1_sid_56\n",
      "\t Skipping last audio frame (# video_frames: 26479, # audio_frames: 26480)\n",
      "\t Sequence count: 15588\n",
      "\t Time taken: 21202.26676750183 s\n",
      "\n",
      "sessid_41_P1_sid_57_P2_sid_51\n",
      "\t Sequence count: 14232\n",
      "\t Time taken: 21448.52202296257 s\n",
      "\n",
      "sessid_41_P2_sid_51_P1_sid_57\n",
      "\t Sequence count: 12095\n",
      "\t Time taken: 21674.105921268463 s\n",
      "\n",
      "sessid_42_P1_sid_12_P2_sid_10\n",
      "\t Skipping last audio frame (# video_frames: 22269, # audio_frames: 22270)\n",
      "\t Sequence count: 7177\n",
      "\t Time taken: 21856.06591796875 s\n",
      "\n",
      "sessid_42_P2_sid_10_P1_sid_12\n",
      "\t Skipping last audio frame (# video_frames: 22269, # audio_frames: 22270)\n",
      "\t Sequence count: 15776\n",
      "\t Time taken: 22142.29254078865 s\n",
      "\n",
      "sessid_43_P1_sid_12_P2_sid_20\n",
      "\t Sequence count: 8776\n",
      "\t Time taken: 22345.469237089157 s\n",
      "\n",
      "sessid_43_P2_sid_20_P1_sid_12\n",
      "\t Sequence count: 16543\n",
      "\t Time taken: 22636.06523156166 s\n",
      "\n",
      "sessid_44_P1_sid_10_P2_sid_20\n",
      "\t Sequence count: 4150\n",
      "\t Time taken: 22739.967703819275 s\n",
      "\n",
      "sessid_44_P2_sid_20_P1_sid_10\n",
      "\t Sequence count: 5562\n",
      "\t Time taken: 22830.863575696945 s\n",
      "\n",
      "sessid_45_P1_sid_56_P2_sid_38\n",
      "\t Sequence count: 15107\n",
      "\t Time taken: 23159.169160842896 s\n",
      "\n",
      "sessid_45_P2_sid_38_P1_sid_56\n",
      "\t Sequence count: 19431\n",
      "\t Time taken: 23515.805480241776 s\n",
      "\n",
      "sessid_46_P1_sid_55_P2_sid_38\n",
      "\t Sequence count: 4735\n",
      "\t Time taken: 23620.778302192688 s\n",
      "\n",
      "sessid_46_P2_sid_38_P1_sid_55\n",
      "\t Sequence count: 5716\n",
      "\t Time taken: 23714.58149266243 s\n",
      "\n",
      "sessid_47_P1_sid_56_P2_sid_37\n",
      "\t Skipping last audio frame (# video_frames: 29800, # audio_frames: 29801)\n",
      "\t Sequence count: 17369\n",
      "\t Time taken: 24053.38237452507 s\n",
      "\n",
      "sessid_47_P2_sid_37_P1_sid_56\n",
      "\t Skipping last audio frame (# video_frames: 29800, # audio_frames: 29801)\n",
      "\t Sequence count: 14816\n",
      "\t Time taken: 24352.35128068924 s\n",
      "\n",
      "sessid_48_P1_sid_58_P2_sid_37\n",
      "\t Sequence count: 9132\n",
      "\t Time taken: 24558.728754997253 s\n",
      "\n",
      "sessid_48_P2_sid_37_P1_sid_58\n",
      "\t Sequence count: 13879\n",
      "\t Time taken: 24823.157081127167 s\n",
      "\n",
      "sessid_49_P1_sid_56_P2_sid_49\n",
      "\t Skipping last audio frame (# video_frames: 26660, # audio_frames: 26661)\n",
      "\t Sequence count: 15449\n",
      "\t Time taken: 25109.708516836166 s\n",
      "\n",
      "sessid_49_P2_sid_49_P1_sid_56\n",
      "\t Skipping last audio frame (# video_frames: 26660, # audio_frames: 26661)\n",
      "\t Sequence count: 13279\n",
      "\t Time taken: 25383.588681459427 s\n",
      "\n",
      "sessid_50_P1_sid_12_P2_sid_31\n",
      "\t Sequence count: 10920\n",
      "\t Time taken: 25620.48209643364 s\n",
      "\n",
      "sessid_50_P2_sid_31_P1_sid_12\n",
      "\t Sequence count: 17782\n",
      "\t Time taken: 25938.30433988571 s\n",
      "\n",
      "sessid_51_P1_sid_12_P2_sid_59\n",
      "\t Skipping last audio frame (# video_frames: 19621, # audio_frames: 19622)\n",
      "\t Sequence count: 6573\n",
      "\t Time taken: 26102.008803367615 s\n",
      "\n",
      "sessid_51_P2_sid_59_P1_sid_12\n",
      "\t Skipping last audio frame (# video_frames: 19621, # audio_frames: 19622)\n",
      "\t Sequence count: 14401\n",
      "\t Time taken: 26356.513252019882 s\n",
      "\n",
      "sessid_52_P1_sid_12_P2_sid_08\n",
      "\t Sequence count: 7404\n",
      "\t Time taken: 26542.88873720169 s\n",
      "\n",
      "sessid_52_P2_sid_08_P1_sid_12\n",
      "\t Sequence count: 12232\n",
      "\t Time taken: 26774.180987119675 s\n",
      "\n",
      "sessid_53_P1_sid_59_P2_sid_08\n",
      "\t Sequence count: 4175\n",
      "\t Time taken: 26883.417258501053 s\n",
      "\n",
      "sessid_53_P2_sid_08_P1_sid_59\n",
      "\t Sequence count: 6787\n",
      "\t Time taken: 26992.985934257507 s\n",
      "\n",
      "sessid_54_P1_sid_12_P2_sid_60\n",
      "\t Sequence count: 9927\n",
      "\t Time taken: 27204.798023462296 s\n",
      "\n",
      "sessid_54_P2_sid_60_P1_sid_12\n",
      "\t Sequence count: 13988\n",
      "\t Time taken: 27466.70920395851 s\n",
      "\n",
      "\n",
      " Processed 106 feature files.\n",
      "\n",
      " Total sequence count: 1367904\n"
     ]
    }
   ],
   "source": [
    "#######################################################################################################################\n",
    "# Project: Deep Virtual Rapport Agent (rapport model)\n",
    "#\n",
    "#     Jan Ondras (jo951030@gmail.com)\n",
    "#     Institute for Creative Technologies, University of Southern California\n",
    "#     April-October 2019\n",
    "#\n",
    "#######################################################################################################################\n",
    "# Generate sequences/segments of speaker's audio and vision features and listener's labels ready for PyTorch.\n",
    "#\n",
    "#     Sequences of features are saved as npy files.\n",
    "#     Sequence metadata and labels are saved in a csv file (dataset file).\n",
    "#\n",
    "#     Set the OUTPUT_DATASET_VERSION, COMMON_DATA_RATE and vision/audio_features_names below.\n",
    "#     (for explanations on the dataset versions see the section Generating sequences/segments in the \n",
    "#     Report_Rapport_model.odt)\n",
    "#\n",
    "#     Run after the smile_gaze_va_turn_annotate_frames.ipynb script was run.\n",
    "#\n",
    "#     Input audio features: dvra_datasets/mimicry/audio_features/{emobase/mfcc}\n",
    "#     Input vision features: dvra_datasets/mimicry/vision_features/annotated_features\n",
    "#     Input voice activity: dvra_datasets/mimicry/voice_activity_detection/voice_activity_ibm_watson\n",
    "#     Output dataset: dvra_datasets/mimicry/segmented_datasets\n",
    "#\n",
    "#     The generated dataset was used for the development of the Rappport Model.\n",
    "#######################################################################################################################\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Common rate of features and labels for all modalities (in Hz)\n",
    "# COMMON_DATA_RATE = 5.\n",
    "# common_data_period = str(int(1000. / COMMON_DATA_RATE)) + 'ms'\n",
    "\n",
    "# UPDATED: keep the data rate at 30 Hz (i.e. don't discard information by downsampling to 5 Hz)\n",
    "COMMON_DATA_RATE = 30.\n",
    "common_data_period = str(int(1000. / COMMON_DATA_RATE)) + 'ms'\n",
    "\n",
    "# First, upsample vision features using padding (duplicating) the frames, then groupby and average\n",
    "# This upsampling avoids empty groups. \n",
    "# Upsampling is not needed for audio features that are sampled at much higher rate.\n",
    "vision_upsample_period = str(int(1000. / (2 * COMMON_DATA_RATE))) + 'ms'\n",
    "\n",
    "# Dataset version\n",
    "# OUTPUT_DATASET_VERSION = 'v0'\n",
    "# OUTPUT_DATASET_VERSION = 'v1'\n",
    "# OUTPUT_DATASET_VERSION = 'v2'\n",
    "OUTPUT_DATASET_VERSION = 'v3'\n",
    "\n",
    "AUDIO_FEATURE_TYPE = 'emobase'\n",
    "# AUDIO_FEATURE_TYPE = 'mfcc'\n",
    "\n",
    "# Window size in frames\n",
    "# WINDOW_SIZE = 4\n",
    "# WINDOW_SIZE = 8\n",
    "# WINDOW_SIZE = 16\n",
    "WINDOW_SIZE = 32\n",
    "# WINDOW_SIZE = 64\n",
    "\n",
    "MASK_VALUE = 0.\n",
    "\n",
    "# Whether to pad sequences to the same length WINDOW_SIZE \n",
    "# (padding may be needed only at the beginning of recordings)\n",
    "# If False, only the sequences that don't require padding will be generated \n",
    "# (this discards only a small fraction (118/238077) of sequences \n",
    "#  and eliminates the need for padding and masking within PyTorch)\n",
    "pad_sequences = False\n",
    "\n",
    "# Speaker's\n",
    "vision_features_names = [\n",
    "    # Head translations (first-order differences)\n",
    "    'diff_ pose_Tx', \n",
    "    'diff_ pose_Ty', \n",
    "    'diff_ pose_Tz',\n",
    "    # Head rotations (first-order differences)\n",
    "    'diff_ pose_Rx', \n",
    "    'diff_ pose_Ry', \n",
    "    'diff_ pose_Rz',\n",
    "    # Head rotations (raw) as a proxy for gaze - need to be normalized (mean normalization per recording)\n",
    "    'unorm_ pose_Rx', \n",
    "    'unorm_ pose_Ry', \n",
    "    # Gaze angles - need to be normalized (mean normalization per recording)\n",
    "    'unorm_ gaze_angle_x', \n",
    "    'unorm_ gaze_angle_y', \n",
    "    # Smile (binary) \n",
    "#     'smile'\n",
    "    # Smile (raw AU intensities)\n",
    "#     ' AU06_r',\n",
    "#     ' AU12_r'\n",
    "    # All AU intensities\n",
    "    ' AU01_r', ' AU02_r', ' AU04_r', ' AU05_r', ' AU06_r', ' AU07_r', ' AU09_r', ' AU10_r', \n",
    "    ' AU12_r', ' AU14_r', ' AU15_r', ' AU17_r', ' AU20_r', ' AU23_r', ' AU25_r', ' AU26_r', ' AU45_r'\n",
    "]\n",
    "\n",
    "# Speaker's\n",
    "audio_features_names = {\n",
    "    'emobase': [\n",
    "        # Speaking time (in seconds) audio feature is calculated below (after synchronization of features to common data rate)\n",
    "        'speaking_time', \n",
    "        # 52 emobase audio features\n",
    "        'pcm_intensity_sma', 'pcm_loudness_sma', 'mfcc_sma[1]',\n",
    "        'mfcc_sma[2]', 'mfcc_sma[3]', 'mfcc_sma[4]', 'mfcc_sma[5]',\n",
    "        'mfcc_sma[6]', 'mfcc_sma[7]', 'mfcc_sma[8]', 'mfcc_sma[9]',\n",
    "        'mfcc_sma[10]', 'mfcc_sma[11]', 'mfcc_sma[12]', 'lspFreq_sma[0]',\n",
    "        'lspFreq_sma[1]', 'lspFreq_sma[2]', 'lspFreq_sma[3]', 'lspFreq_sma[4]',\n",
    "        'lspFreq_sma[5]', 'lspFreq_sma[6]', 'lspFreq_sma[7]', 'pcm_zcr_sma',\n",
    "        'voiceProb_sma', 'F0_sma', 'F0env_sma', 'pcm_intensity_sma_de',\n",
    "        'pcm_loudness_sma_de', 'mfcc_sma_de[1]', 'mfcc_sma_de[2]',\n",
    "        'mfcc_sma_de[3]', 'mfcc_sma_de[4]', 'mfcc_sma_de[5]', 'mfcc_sma_de[6]',\n",
    "        'mfcc_sma_de[7]', 'mfcc_sma_de[8]', 'mfcc_sma_de[9]', 'mfcc_sma_de[10]',\n",
    "        'mfcc_sma_de[11]', 'mfcc_sma_de[12]', 'lspFreq_sma_de[0]',\n",
    "        'lspFreq_sma_de[1]', 'lspFreq_sma_de[2]', 'lspFreq_sma_de[3]',\n",
    "        'lspFreq_sma_de[4]', 'lspFreq_sma_de[5]', 'lspFreq_sma_de[6]',\n",
    "        'lspFreq_sma_de[7]', 'pcm_zcr_sma_de', 'voiceProb_sma_de', 'F0_sma_de',\n",
    "        'F0env_sma_de'\n",
    "    ], \n",
    "    'mfcc': [\n",
    "        # Speaking time (in seconds) audio feature is calculated below (after synchronization of features to common data rate)\n",
    "        'speaking_time', \n",
    "        # 57 mfcc (extended) features\n",
    "        'voiceProb', 'F0', 'F0env', 'pcm_intensity', 'pcm_loudness', 'pcm_LOGenergy', \n",
    "        'pcm_fftMag_mfcc[0]', 'pcm_fftMag_mfcc[1]', 'pcm_fftMag_mfcc[2]', 'pcm_fftMag_mfcc[3]', 'pcm_fftMag_mfcc[4]', \n",
    "        'pcm_fftMag_mfcc[5]', 'pcm_fftMag_mfcc[6]', 'pcm_fftMag_mfcc[7]', 'pcm_fftMag_mfcc[8]', 'pcm_fftMag_mfcc[9]', \n",
    "        'pcm_fftMag_mfcc[10]', 'pcm_fftMag_mfcc[11]', 'pcm_fftMag_mfcc[12]', \n",
    "        'voiceProb_de', 'F0_de', 'F0env_de', 'pcm_intensity_de', 'pcm_loudness_de', 'pcm_LOGenergy_de', \n",
    "        'pcm_fftMag_mfcc_de[0]', 'pcm_fftMag_mfcc_de[1]', 'pcm_fftMag_mfcc_de[2]', 'pcm_fftMag_mfcc_de[3]', 'pcm_fftMag_mfcc_de[4]', \n",
    "        'pcm_fftMag_mfcc_de[5]', 'pcm_fftMag_mfcc_de[6]', 'pcm_fftMag_mfcc_de[7]', 'pcm_fftMag_mfcc_de[8]', 'pcm_fftMag_mfcc_de[9]', \n",
    "        'pcm_fftMag_mfcc_de[10]', 'pcm_fftMag_mfcc_de[11]', 'pcm_fftMag_mfcc_de[12]', \n",
    "        'voiceProb_de_de', 'F0_de_de', 'F0env_de_de', 'pcm_intensity_de_de', 'pcm_loudness_de_de', 'pcm_LOGenergy_de_de', \n",
    "        'pcm_fftMag_mfcc_de_de[0]', 'pcm_fftMag_mfcc_de_de[1]', 'pcm_fftMag_mfcc_de_de[2]', 'pcm_fftMag_mfcc_de_de[3]', 'pcm_fftMag_mfcc_de_de[4]', \n",
    "        'pcm_fftMag_mfcc_de_de[5]', 'pcm_fftMag_mfcc_de_de[6]', 'pcm_fftMag_mfcc_de_de[7]', 'pcm_fftMag_mfcc_de_de[8]', 'pcm_fftMag_mfcc_de_de[9]', \n",
    "        'pcm_fftMag_mfcc_de_de[10]', 'pcm_fftMag_mfcc_de_de[11]', 'pcm_fftMag_mfcc_de_de[12]'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Listener's\n",
    "labels_names = [\n",
    "    'nod', \n",
    "    'shake', \n",
    "    'tilt', \n",
    "    'smile', \n",
    "    'gaze_away', \n",
    "    # Can be used for paraverbal prediction\n",
    "    # (at inference time, can randomly choose one from a predefined set of paraverbals)\n",
    "    'voice_active'\n",
    "]\n",
    "# and also 'take_turn' labels from speaker vision data\n",
    "\n",
    "vision_features_dir = f'/home/ICT2000/jondras/dvra_datasets/mimicry/vision_features/annotated_features'\n",
    "audio_features_dir = f'/home/ICT2000/jondras/dvra_datasets/mimicry/audio_features/opensmile_{AUDIO_FEATURE_TYPE}'\n",
    "\n",
    "dataset_output_dir_prefix = f'/home/ICT2000/jondras/dvra_datasets/mimicry/segmented_datasets/segmented_datasets_{OUTPUT_DATASET_VERSION}'\n",
    "vision_features_output_dir = f'{dataset_output_dir_prefix}/vision_features/{WINDOW_SIZE}ws'\n",
    "audio_features_output_dir = f'{dataset_output_dir_prefix}/audio_features/{AUDIO_FEATURE_TYPE}/{WINDOW_SIZE}ws'\n",
    "metadata_labels_output_file = f'{dataset_output_dir_prefix}/metadata_labels_{WINDOW_SIZE}ws.csv'\n",
    "\n",
    "for dir_path in [vision_features_output_dir, audio_features_output_dir]:\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        \n",
    "print(f'Dimensionality of vision features: {len(vision_features_names)}')\n",
    "print(f'Dimensionality of audio features: {len(audio_features_names[AUDIO_FEATURE_TYPE])}')\n",
    "print(f'Dimensionality of target labels: {len(labels_names) + 1}\\n')\n",
    "print(f'Window size (sequence length): {WINDOW_SIZE}')\n",
    "print(f'Audio feature type: {AUDIO_FEATURE_TYPE}\\n')\n",
    "\n",
    "# Load vision feature files and annotations\n",
    "file_cnt = 0\n",
    "total_sequence_cnt = 0\n",
    "metadata_labels = []\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate over sessions, considering each person as a speaker and as a listener \n",
    "for vision_features_file_1, vision_features_file_2 in np.reshape(sorted(glob.glob(f'{vision_features_dir}/*.csv')), \n",
    "                                                                 (-1, 2)):\n",
    "    for speaker_vision_features_file, listener_vision_features_file in [[vision_features_file_1, vision_features_file_2], \n",
    "                                                                        [vision_features_file_2, vision_features_file_1]]:\n",
    "        # print(speaker_vision_features_file, listener_vision_features_file)\n",
    "        \n",
    "        speaker_id = speaker_vision_features_file.split('/')[-1][:-4]\n",
    "        speaker_sid = speaker_id[17:19]\n",
    "        listener_id = listener_vision_features_file.split('/')[-1][:-4]\n",
    "        listener_sid = listener_id[17:19]\n",
    "        # Keep track whose speaker features and whose listener labels are used \n",
    "        # (speaker is the first, listener is the second)\n",
    "        sequence_id_prefix = f'{speaker_id}_{listener_id[10:]}'\n",
    "        session_id = sequence_id_prefix[7:9]\n",
    "#         if int(sequence_id_prefix[7:9]) < 15:\n",
    "#             continue\n",
    "        print(sequence_id_prefix)\n",
    "\n",
    "        ############################################################################################################\n",
    "        # Load speaker's and listener's vision data\n",
    "        speaker_vision_df = pd.read_csv(speaker_vision_features_file)\n",
    "        listener_vision_df = pd.read_csv(listener_vision_features_file)\n",
    "        \n",
    "        # Mean-normalize speaker's rotation angles and gaze angles\n",
    "        speaker_vision_df['unorm_ pose_Rx'] = speaker_vision_df[' pose_Rx'] - speaker_vision_df[' pose_Rx'].mean()\n",
    "        speaker_vision_df['unorm_ pose_Ry'] = speaker_vision_df[' pose_Ry'] - speaker_vision_df[' pose_Ry'].mean()\n",
    "        speaker_vision_df['unorm_ gaze_angle_x'] = speaker_vision_df[' gaze_angle_x'] - speaker_vision_df[' gaze_angle_x'].mean()\n",
    "        speaker_vision_df['unorm_ gaze_angle_y'] = speaker_vision_df[' gaze_angle_y'] - speaker_vision_df[' gaze_angle_y'].mean()\n",
    "        \n",
    "        # Set dataframe index to frame timestamp \n",
    "        speaker_vision_df = speaker_vision_df.set_index(pd.DatetimeIndex(pd.to_datetime(speaker_vision_df[' timestamp'], unit='s')))\n",
    "        listener_vision_df = listener_vision_df.set_index(pd.DatetimeIndex(pd.to_datetime(listener_vision_df[' timestamp'], unit='s')))\n",
    "\n",
    "        # First, upsample vision features using padding (duplicating) the frames\n",
    "        # (this avoids creating empty groups below)\n",
    "        speaker_vision_df = speaker_vision_df.resample(vision_upsample_period).pad()\n",
    "        listener_vision_df = listener_vision_df.resample(vision_upsample_period).pad()\n",
    "        \n",
    "        # Average features to COMMON_DATA_RATE (over intervals of length common_data_period)\n",
    "        speaker_vision_groups = speaker_vision_df.groupby(pd.Grouper(freq=common_data_period))\n",
    "        assert len(speaker_vision_groups.size().nonzero()[0]) == len(speaker_vision_groups)\n",
    "        speaker_vision_df = speaker_vision_groups.mean()\n",
    "        \n",
    "        listener_vision_groups = listener_vision_df.groupby(pd.Grouper(freq=common_data_period))\n",
    "        assert len(listener_vision_groups.size().nonzero()[0]) == len(listener_vision_groups)\n",
    "        listener_vision_df = listener_vision_groups.mean()\n",
    "        \n",
    "        # Restore index to default integers (needed for assignments in 'Fix take_turn annotations' below)\n",
    "        speaker_vision_df = speaker_vision_df.reset_index(drop=True)\n",
    "        listener_vision_df = listener_vision_df.reset_index(drop=True)\n",
    "        \n",
    "        # Fix binary annotations\n",
    "        for annotation_column in ['nod', 'shake', 'tilt', 'smile', 'gaze_away', 'voice_active', 'take_turn']:\n",
    "            speaker_vision_df[annotation_column] = np.where((speaker_vision_df[annotation_column] >= 0.5), 1, 0)\n",
    "            listener_vision_df[annotation_column] = np.where((listener_vision_df[annotation_column] >= 0.5), 1, 0)\n",
    "        \n",
    "        # Need to regenerate take-turn annotations even if no averaging was applied, since the original take-turn annotations annotate just one frame following a voice-active interval\n",
    "#         speaker_vision_df['take_turn'] = 0\n",
    "    \n",
    "        # Fix take_turn annotations (became all zero in the above step)\n",
    "        j = 0\n",
    "        while j < len(speaker_vision_df) - 1:\n",
    "            # End of VA interval \n",
    "            # => set take_turn in the first (WINDOW_SIZE - 1) frames after the voice active interval\n",
    "            #    or fewer if another voice-active region starts there (or end of recording)\n",
    "            if (speaker_vision_df.iloc[j]['voice_active'] == 1) and (speaker_vision_df.iloc[j + 1]['voice_active'] == 0):\n",
    "                last_va_idx = j\n",
    "                while ((j + 1 < len(speaker_vision_df)) \n",
    "                       and (j + 1 - last_va_idx < WINDOW_SIZE) \n",
    "                       and (speaker_vision_df.iloc[j + 1]['voice_active'] == 0)):\n",
    "                    speaker_vision_df.at[j + 1, 'take_turn'] = 1\n",
    "                    j += 1\n",
    "            j += 1\n",
    "\n",
    "        # Listener's turn-taking is not needed (kept just for consistency)\n",
    "#         j = 0\n",
    "#         while j < len(listener_vision_df) - 1:\n",
    "#             # End of VA interval \n",
    "#             # => set take_turn in the first (WINDOW_SIZE - 1) frames after the voice active interval\n",
    "#             #    or fewer if another voice-active region starts there (or end of recording)\n",
    "#             if (listener_vision_df.iloc[j]['voice_active'] == 1) and (listener_vision_df.iloc[j + 1]['voice_active'] == 0):\n",
    "#                 last_va_idx = j\n",
    "#                 while ((j + 1 < len(listener_vision_df)) \n",
    "#                        and (j + 1 - last_va_idx < WINDOW_SIZE) \n",
    "#                        and (listener_vision_df.iloc[j + 1]['voice_active'] == 0)):\n",
    "#                     listener_vision_df.at[j + 1, 'take_turn'] = 1\n",
    "#                     j += 1\n",
    "#             j += 1\n",
    "        # OLD way: just one frame following the VA interval is annotated as take-turn\n",
    "#         for j in range(len(listener_vision_df) - 1):\n",
    "#             # End of VA interval => set take_turn in the first frame after the voice active interval\n",
    "#             if (listener_vision_df.iloc[j]['voice_active'] == 1) and (listener_vision_df.iloc[j + 1]['voice_active'] == 0):\n",
    "#                 listener_vision_df.iloc[j + 1]['take_turn'] = 1\n",
    "        assert len(speaker_vision_df) == len(listener_vision_df)\n",
    "        \n",
    "        ############################################################################################################\n",
    "        # Load speaker's audio data\n",
    "        speaker_audio_df = pd.read_csv(f'{audio_features_dir}/{speaker_id}.csv', delimiter=';')\n",
    "        \n",
    "        # Set dataframe index to frame timestamp \n",
    "        speaker_audio_df = speaker_audio_df.set_index(pd.DatetimeIndex(pd.to_datetime(speaker_audio_df['frameTime'], unit='s')))\n",
    "        \n",
    "        # Average features to COMMON_DATA_RATE (over intervals of length common_data_period)\n",
    "        speaker_audio_groups = speaker_audio_df.groupby(pd.Grouper(freq=common_data_period))\n",
    "        assert len(speaker_audio_groups.size().nonzero()[0]) == len(speaker_audio_groups)\n",
    "        speaker_audio_df = speaker_audio_groups.mean()\n",
    "        \n",
    "        # Restore index to default integers\n",
    "        speaker_audio_df = speaker_audio_df.reset_index(drop=True)\n",
    "        \n",
    "        # Consolidate (match) the numbers of vision frames and audio frames (skipping the last excessive frame)\n",
    "        if len(speaker_vision_df) == len(speaker_audio_df) - 1:\n",
    "            print(f'\\t Skipping last audio frame (# video_frames: {len(speaker_vision_df)}, # audio_frames: {len(speaker_audio_df)})')\n",
    "            speaker_audio_df = speaker_audio_df[:len(speaker_vision_df)]\n",
    "        elif len(speaker_vision_df) - 1 == len(speaker_audio_df):\n",
    "            print(f'\\t Skipping last video frame (# video_frames: {len(speaker_vision_df)}, # audio_frames: {len(speaker_audio_df)})')\n",
    "            speaker_vision_df = speaker_vision_df[:len(speaker_audio_df)]\n",
    "            listener_vision_df = listener_vision_df[:len(speaker_audio_df)]\n",
    "        assert len(speaker_vision_df) == len(speaker_audio_df), f'Vision: {len(speaker_vision_df)}\\t Audio: {len(speaker_audio_df)}'\n",
    "        \n",
    "        # Add 'speaking_time' audio feature (in seconds), after synchronization of audio and visual features to common data rate\n",
    "        speaker_audio_df['speaking_time'] = np.zeros(len(speaker_audio_df), dtype=float)\n",
    "        j = 0\n",
    "        while j < len(speaker_vision_df) - 1:\n",
    "            # Start of VA interval \n",
    "            # => set speaking_time based on the number of contiguous voice-active frames\n",
    "            if (speaker_vision_df.iloc[j]['voice_active'] == 0) and (speaker_vision_df.iloc[j + 1]['voice_active'] == 1):\n",
    "                first_va_idx = j\n",
    "                while ((j + 1 < len(speaker_vision_df)) and (speaker_vision_df.iloc[j + 1]['voice_active'] == 1)):\n",
    "                    speaker_audio_df.at[j + 1, 'speaking_time'] = (j + 1 - first_va_idx) / COMMON_DATA_RATE\n",
    "                    j += 1\n",
    "            j += 1\n",
    "        \n",
    "        ############################################################################################################\n",
    "        # Generate sequences only from regions where speaker talks or just stopped talking. \n",
    "        # In particular, we extracted a sequence of audio and video features only if the target prediction frame \n",
    "        # of the sequence was within the voice-active region, or at least one frame of the sequence was within the \n",
    "        # voice-active region and the prediction frame of the sequence was not within the next voice-active region. \n",
    "        # Also, record labels and associated metadata.\n",
    "        \n",
    "        bool_extract = (speaker_vision_df['voice_active'] == 1) | (speaker_vision_df['take_turn'] == 1)\n",
    "        # print(extract_idxs)\n",
    "        # print(len(bool_extract), len(speaker_vision_df), len(listener_vision_df))\n",
    "\n",
    "        sequence_cnt = 0\n",
    "        for i, do_extract in enumerate(bool_extract):\n",
    "            # print(i, do_extract)\n",
    "            if do_extract:\n",
    "\n",
    "                pad_len = WINDOW_SIZE - 1 - i\n",
    "                pad_needed = (0 < pad_len)\n",
    "                # Don't save sequences if padding is not desired and is needed\n",
    "                if pad_sequences or (not pad_needed):\n",
    "                    \n",
    "                    # One training sample: 2D array (timesteps x features)\n",
    "                    # Last (and also the prediction) frame is at index i\n",
    "                    speaker_vision_sequence = speaker_vision_df[i + 1 - WINDOW_SIZE:i + 1][vision_features_names].values\n",
    "                    speaker_audio_sequence = speaker_audio_df[i + 1 - WINDOW_SIZE:i + 1][audio_features_names[AUDIO_FEATURE_TYPE]].values\n",
    "                    \n",
    "                    if pad_needed:\n",
    "                        speaker_vision_sequence = np.pad(speaker_vision_sequence, ((pad_len, 0), (0, 0)), \n",
    "                                                         mode='constant', constant_values=(MASK_VALUE, MASK_VALUE))\n",
    "                        speaker_audio_sequence = np.pad(speaker_audio_sequence, ((pad_len, 0), (0, 0)), \n",
    "                                                         mode='constant', constant_values=(MASK_VALUE, MASK_VALUE))\n",
    "                        print(f'\\t Padded with {pad_len} mask values {MASK_VALUE}')\n",
    "\n",
    "                    # Save speaker vision features sequence and speaker audio features sequence \n",
    "                    sequence_id = f'{sequence_id_prefix}_seq_{i:08}'\n",
    "                    np.save(f'{vision_features_output_dir}/{sequence_id}.npy', \n",
    "                            speaker_vision_sequence)\n",
    "                    np.save(f'{audio_features_output_dir}/{sequence_id}.npy', \n",
    "                            speaker_audio_sequence)\n",
    "\n",
    "                    # Add row to metadata+labels file\n",
    "                    # Besides listener's annotations, also add speaker's 'take_turn' labels\n",
    "                    metadata_labels.append([sequence_id, session_id, speaker_sid, listener_sid] \n",
    "                                           + listener_vision_df.iloc[i][labels_names].tolist() \n",
    "                                           + [speaker_vision_df.iloc[i]['take_turn']])\n",
    "                    sequence_cnt += 1\n",
    "        total_sequence_cnt += sequence_cnt\n",
    "        file_cnt += 1\n",
    "        print(f'\\t Sequence count: {sequence_cnt}')\n",
    "        print(f'\\t Time taken: {time.time() - start_time} s\\n')  \n",
    "        \n",
    "#         break\n",
    "#     break\n",
    "\n",
    "############################################################################################################\n",
    "# Save metadata and all the labels (in the dataset file)\n",
    "assert len(metadata_labels) == total_sequence_cnt, f'Labels count: {len(metadata_labels)}\\t Total sequence count: {total_sequence_cnt}'\n",
    "metadata_labels_df = pd.DataFrame(metadata_labels, \n",
    "                                  columns=['sequence_id', 'session_id', 'speaker_sid', 'listener_sid'] \n",
    "                                          + labels_names + ['take_turn'])\n",
    "metadata_labels_df.to_csv(metadata_labels_output_file, index=False)\n",
    "\n",
    "print(f'\\n Processed {file_cnt} feature files.')\n",
    "print(f'\\n Total sequence count: {total_sequence_cnt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
