### Scripts to preprocess the mimicry dataset

	Project: Deep Virtual Rapport Agent
	Jan Ondras (jo951030@gmail.com)
	Institute for Creative Technologies, University of Southern California
	April-October 2019
------------


The mimicry dataset was used only for the development of the Rapport Model. 


Preprocessing pipeline: 

	extract_vision_features.sh --> rename_featuresfiles.ipynb --> (0)

	(0) --> Annotate vision features with nod, shake, and tilt head gestures using the developed Head Gesture Detector (deep-virtual-rapport-agent/head_gesture_detector/hgd_annotate_frames.ipynb) --> (1)

	(0) --> separate_audio_channels.ipynb --> downsample_to_*kHz.sh --> extract_audio_features.sh --> (2)
	                                                                --> voice_activity_detection --> (3)

	(1) & (3) --> smile_gaze_va_turn_annotate_frames.ipynb --> (4) 
	                                                       --> fully_annotate_videos.py

	(2) & (3) & (4) -->  generate_dataset.ipynb


Overview of the files in this folder:
	
	./access
		Login details to access the Mimicry DB and EULA. 

	./audio_features_extraction_vggish
		Extract audio features from the Mimicry dataset using VGGish. 
		Finally, not used since for the real-time embedding into the Multisense system OpenSMILE might be better. 

	./opensmile_configs
		OpenSMILE configuration files used for audio feature extraction. 
		Please, refer to deep-virtual-rapport-agent/rapport_model/Report_rapport_model.odt for more details. 

	./voice_activity_detection
		Scripts for voice activity detection by OpenSMILE, WebRTC, Google STT, IBM Watson STT, and (Amazon STT). 
		Please, refer to deep-virtual-rapport-agent/rapport_model/Report_rapport_model.odt for more details. 

	./downsample_to_8kHz.sh
		Downsample mono audio to 8 kHz for Voice Activity Detection (VAD). 

	./downsample_to_16kHz.sh
		Downsample mono audio to 16 kHz for audio feature extraction and Google ASR (requires 16kHz audio sampling rate). 

	./extract_audio_features.sh
		Extract audio/speech features from the Mimicry dataset using OpenSMILE. 
		Extracts 2 sets of features: emobase and extended MFCC.

	./extract_vision_features.sh
		Extract vision features from the Mimicry dataset using OpenFace. 
		Optionally, also generate tracked videos by OpenFace. 

	./fully_annotate_videos.py
		Given original video file from Mimicry database and annotated vision features (csv), generate a new video file with color annotations of head gestures (nod, shake, tilt), smile, gaze away, voice activity, and turn-taking. 

	./generate_dataset.ipynb
		Generate sequences/segments of speaker's audio and vision features and listener's labels ready for PyTorch. 
		Run after the voice activity. 
		The generated dataset was used for the development of the Rappport Model.

	./rename_featuresfiles.ipynb
		Rename the extracted vision features csvs to follow the naming convention: sessid_{session id}P{subject position either 1 or 2}sid_{subject id}.csv 
		It uses the session.xml files from the Mimicry dataset and also the vision_feature_extraction_time_log.txt.
		Run only once (after OpenFace feature extraction)! 

	./separate_audio_channels.ipynb
		Separate audio channels of the two subjects from each stereo audio recording. 

	./smile_gaze_va_turn_annotate_frames.ipynb
		Annotate OpenFace vision features from the mimicry dataset with smile, gaze away, voice activity, and take turn binary labels. 
		Run after the vision features are annotated with nod, shake, and tilt head gestures using the developed Head Gesture Detector (HGD) (deep-virtual-rapport-agent/head_gesture_detector/hgd_annotate_frames.ipynb). 
		Run after the voice activity is detected. 
		Note: it changes the annotated vision featurefiles generated by HGD in-place (!) adding smile, gaze_away, voice_active and take_turn columns. 

	./vision_feature_extraction_time_log.txt
		Log of (filepath, session id, filename, and vision features extraction time by OpenFace (in seconds)) for each video in the mimicry dataset. 
		For the first few videos the full filepath is missing but it was never needed. 
