{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# Project: Deep Virtual Rapport Agent (data preprocessing)\n",
    "#\n",
    "#     Jan Ondras (jo951030@gmail.com)\n",
    "#     Institute for Creative Technologies, University of Southern California\n",
    "#     April-October 2019\n",
    "#\n",
    "#######################################################################################################################\n",
    "# Generate segmented/sequenced test dataset from ccdb dataset\n",
    "#\n",
    "#     For head gestures nod, shake, and tilt.\n",
    "#\n",
    "#     So far only for the basic set of the first 8 sessions.\n",
    "#\n",
    "#     Run after the annotate_features.ipynb script was run.\n",
    "#\n",
    "#     Input features: dvra_datasets/ccdb/annotated_features\n",
    "#     Output dataset: dvra_datasets/ccdb/segmented_datasets\n",
    "#\n",
    "#     The generated dataset was used for cross-dataset testing of the developed Head Gesture Detector.\n",
    "#     In future, the ccdb dataset can be included in the training data, extending the 4comb dataset to 5comb.\n",
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "import numpy as np\n",
    "random_seed = 37\n",
    "np.random.seed(random_seed)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(random_seed)\n",
    "###########################################################\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Mask value (if all features for a given sample timestep are equal to MASK_VALUE, \n",
    "# then the sample timestep will be masked (skipped))\n",
    "# Cannot use MASK_VALUE=0.0, as it corresponds to no movement (derivatives are zero)\n",
    "# Cannot use MASK_VALUE=np.inf, as RandomUnderSampler cannot handle NaNs and Inf values\n",
    "MASK_VALUE = 7777777.7777777\n",
    "    \n",
    "    \n",
    "def generate_dataset(selected_features, window_size, val_size, head_gesture):\n",
    "    '''\n",
    "    Split dataset (csv files of recordings) into train/val paritions (to train final model to be used for cross-dataset prediction).\n",
    "    Also prepare test partition that contains all the data for cross-dataset testing. \n",
    "    Segment both partitions of the dataset.\n",
    "        Dataset is segmented into same-length (window_size) sequences.\n",
    "        Feature segments are pre-padded with MASK_VALUE-s and label segments with 0 (not a nod/shake/tilt).\n",
    "    For other datasets need to modify to include all available annotations (for nod, shake, tilt).\n",
    "    One output file is saved. \n",
    "    ''' \n",
    "    \n",
    "    dataset_output_filename_prefix = f'/home/ICT2000/jondras/dvra_datasets/ccdb/segmented_datasets/'\n",
    "    \n",
    "    dataset_type = f'{window_size}ws_{len(selected_features)}f'\n",
    "    if not os.path.exists(dataset_output_filename_prefix):\n",
    "        os.makedirs(dataset_output_filename_prefix)\n",
    "    \n",
    "    print(f'Head gesture: {head_gesture}')\n",
    "    print(f'Window size: {window_size}')\n",
    "    n_features = len(selected_features)\n",
    "    print(f'Selected features: \\n\\t{selected_features}')\n",
    "        \n",
    "    \n",
    "    def get_segments(df):\n",
    "        '''\n",
    "        Generate segments (X (features) and Y (labels)) from the dataframe. \n",
    "        \n",
    "        Returns 2 lists of 2D arrays.\n",
    "        '''\n",
    "        \n",
    "        X = []\n",
    "        Y = []\n",
    "\n",
    "        # Pre-pad all features and labels with (window_size - 1) MASK_VALUE-s \n",
    "        padded_features = np.pad(df.values[:, :-1], ((window_size - 1, 0), (0, 0)), \n",
    "                                 mode='constant', constant_values=(MASK_VALUE, MASK_VALUE))\n",
    "        # Labels are padded with 0 mask value (indicating not a nod)\n",
    "        padded_labels   = np.pad(df.values[:, -1],  (window_size - 1, 0), \n",
    "                                 mode='constant', constant_values=(0, 0))\n",
    "        \n",
    "        assert padded_features.shape[1] == n_features\n",
    "        assert padded_labels.shape[0] == padded_features.shape[0]\n",
    "        assert len(padded_features) - window_size + 1 == len(df), 'Padding failed!'\n",
    "\n",
    "        # Slide window of length window_size over the padded features/labels\n",
    "        for i in range(len(df)):       \n",
    "            X.append( padded_features[i:i + window_size] )\n",
    "            Y.append( padded_labels[i:i + window_size] )\n",
    "\n",
    "        return X, Y\n",
    "    \n",
    "    \n",
    "    # Load the annotated feature files\n",
    "    input_annotated_features_dir = '/home/ICT2000/jondras/dvra_datasets/ccdb/annotated_features'\n",
    "    input_filenames = np.array(sorted(glob.glob(input_annotated_features_dir + '/*.csv')))\n",
    "    n_subjects = len(input_filenames)\n",
    "    print(f'\\t {n_subjects} subjects/sessions')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    segments = defaultdict(list)\n",
    "    for annotated_features_file in input_filenames:\n",
    "        # Take only selected features and annotation columns.\n",
    "        df = pd.read_csv(annotated_features_file)[selected_features + [head_gesture]]\n",
    "        \n",
    "        # Get all segments for cross-dataset testing\n",
    "        X_test, Y_test = get_segments(df=df)\n",
    "        assert len(X_test) == len(df)\n",
    "        segments['X_test'].extend(X_test)\n",
    "        segments['Y_test'].extend(Y_test)\n",
    "        segments['test_len'].append(len(X_test))\n",
    "\n",
    "    # Convert lists to numpy arrays and reshape Y to be 3D (as needed for training)\n",
    "    for key in segments.keys():\n",
    "        segments[key] = np.array(segments[key])\n",
    "        if key[0] == 'Y':\n",
    "            segments[key] = np.expand_dims(segments[key], axis=-1)               \n",
    "        print(key, segments[key].shape)\n",
    "\n",
    "    # Save test segmented data for this fold\n",
    "    segments['selected_features'] = selected_features\n",
    "    segments['WINDOW_SIZE'] = window_size\n",
    "    segments['MASK_VALUE'] = MASK_VALUE\n",
    "    np.savez(dataset_output_filename_prefix + f'ccdb_{head_gesture}_{dataset_type}', **segments)\n",
    "\n",
    "    print(f'\\t\\t Number of test examples per class: \\t{np.unique(segments[f\"Y_test\"][:, -1], return_counts=True)}')        \n",
    "    print(f'\\t\\t Total time taken: {time.time() - start_time} s')\n",
    "    print('====================================================================================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head gesture: nod\n",
      "Window size: 32\n",
      "Selected features: \n",
      "\t['diff_ pose_Tx', 'diff_ pose_Ty', 'diff_ pose_Tz', 'diff2_ pose_Tx', 'diff2_ pose_Ty', 'diff2_ pose_Tz', 'diff_ pose_Rx', 'diff_ pose_Ry', 'diff_ pose_Rz', 'diff2_ pose_Rx', 'diff2_ pose_Ry', 'diff2_ pose_Rz']\n",
      "\t 16 subjects/sessions\n",
      "X_test (142442, 32, 12)\n",
      "Y_test (142442, 32, 1)\n",
      "test_len (16,)\n",
      "\t\t Number of test examples per class: \t(array([0., 1.]), array([135983,   6459]))\n",
      "\t\t Total time taken: 24.609424829483032 s\n",
      "====================================================================================================\n",
      "Head gesture: shake\n",
      "Window size: 32\n",
      "Selected features: \n",
      "\t['diff_ pose_Tx', 'diff_ pose_Ty', 'diff_ pose_Tz', 'diff2_ pose_Tx', 'diff2_ pose_Ty', 'diff2_ pose_Tz', 'diff_ pose_Rx', 'diff_ pose_Ry', 'diff_ pose_Rz', 'diff2_ pose_Rx', 'diff2_ pose_Ry', 'diff2_ pose_Rz']\n",
      "\t 16 subjects/sessions\n",
      "X_test (142442, 32, 12)\n",
      "Y_test (142442, 32, 1)\n",
      "test_len (16,)\n",
      "\t\t Number of test examples per class: \t(array([0., 1.]), array([140519,   1923]))\n",
      "\t\t Total time taken: 24.8432035446167 s\n",
      "====================================================================================================\n",
      "Head gesture: tilt\n",
      "Window size: 32\n",
      "Selected features: \n",
      "\t['diff_ pose_Tx', 'diff_ pose_Ty', 'diff_ pose_Tz', 'diff2_ pose_Tx', 'diff2_ pose_Ty', 'diff2_ pose_Tz', 'diff_ pose_Rx', 'diff_ pose_Ry', 'diff_ pose_Rz', 'diff2_ pose_Rx', 'diff2_ pose_Ry', 'diff2_ pose_Rz']\n",
      "\t 16 subjects/sessions\n",
      "X_test (142442, 32, 12)\n",
      "Y_test (142442, 32, 1)\n",
      "test_len (16,)\n",
      "\t\t Number of test examples per class: \t(array([0., 1.]), array([141365,   1077]))\n",
      "\t\t Total time taken: 24.2677059173584 s\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# selected_features_1 = [\n",
    "#     'diff_ pose_Tx', \n",
    "#     'diff_ pose_Ty', \n",
    "#     'diff_ pose_Tz',\n",
    "\n",
    "#     'diff_ pose_Rx', \n",
    "#     'diff_ pose_Ry', \n",
    "#     'diff_ pose_Rz',\n",
    "# ]\n",
    "selected_features_2 = [\n",
    "    'diff_ pose_Tx', \n",
    "    'diff_ pose_Ty', \n",
    "    'diff_ pose_Tz',\n",
    "    \n",
    "    'diff2_ pose_Tx', \n",
    "    'diff2_ pose_Ty', \n",
    "    'diff2_ pose_Tz',\n",
    "\n",
    "    'diff_ pose_Rx', \n",
    "    'diff_ pose_Ry', \n",
    "    'diff_ pose_Rz',\n",
    "    \n",
    "    'diff2_ pose_Rx', \n",
    "    'diff2_ pose_Ry', \n",
    "    'diff2_ pose_Rz',\n",
    "]\n",
    "\n",
    "for sf in [selected_features_2]:\n",
    "    for ws in [32]:\n",
    "        for hg in ['nod', 'shake', 'tilt']:\n",
    "            generate_dataset(selected_features=sf, window_size=ws, val_size=0.15, head_gesture=hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
